{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # to ignore all future warinings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the dataset\n",
    "\n",
    "### 1.1 Scraping news articles from the web\n",
    "\n",
    "This process takes on average between 2 and 15min, depending on how many website links are to be scraped, how many articles in these links are found and how much computing ressources the machine has on which the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2019-11-12\n",
      "Article `download()` failed with HTTPConnectionPool(host='rss.cnn.com', port=80): Max retries exceeded with url: /~r/rss/cnn_topstories/~3/lEBZaEYAn7Q/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001D199D68E10>, 'Connection to rss.cnn.com timed out. (connect timeout=7)')) on URL http://rss.cnn.com/~r/rss/cnn_topstories/~3/lEBZaEYAn7Q/index.html\n",
      "continuing...\n"
     ]
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import time\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "import dateutil\n",
    "\n",
    "#### 1 Website data ####\n",
    "\n",
    "## 1A ##  From JSON file - for final version\n",
    "\n",
    "with open('NewsPapers.json') as data_file: #Loads the JSON files with news URLs\n",
    "    companies = json.load(data_file)\n",
    "\n",
    "## 1B ## From variable - this is for testing, makes it way faster\n",
    "website = {\"cnn\": {\"rss\": \"http://rss.cnn.com/rss/cnn_topstories.rss\"},\n",
    "          \"cnbc\":{\"rss\": \"https://www.cnbc.com/id/10000664/device/rss/rss.html\"}}\n",
    "\n",
    "\n",
    "#### 2 Todays date - for filtering the articles by todays date ####\n",
    "today = str(date.today()) \n",
    "print(\"Today's date:\", today)\n",
    "\n",
    "\n",
    "#### 3 Scraping the news articles ####\n",
    "\n",
    "text_list = []\n",
    "source_list = []\n",
    "article_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "title_list = []\n",
    "\n",
    "for source, value in website.items(): # if website is changed to companies, it scrapes from JSON file ! takes time !!\n",
    "    d = fp.parse(value['rss'])\n",
    "    article={}\n",
    "    for entry in d.entries:\n",
    "        if hasattr(entry, 'published'):\n",
    "            article['source'] = source\n",
    "            source_list.append(article['source'])\n",
    "\n",
    "            #getting the article URLs\n",
    "            article['link'] = entry.link\n",
    "            article_list.append(article['link'])\n",
    "\n",
    "            #getting the article published dates\n",
    "            date = (getattr(entry, 'published'))\n",
    "            date = dateutil.parser.parse(date)\n",
    "            date_formated = date.strftime(\"%Y-%m-%d\")\n",
    "            time_formated = date.strftime(\"%H:%M:%S %Z\") #hour, minute, timezone (converted)\n",
    "            date_list.append(date_formated)\n",
    "            time_list.append(time_formated)\n",
    "\n",
    "            #getting the titles\n",
    "            content = Article(entry.link)\n",
    "            try:\n",
    "                content.download() #downloading article content\n",
    "                #downloading takes approx. 3min to load\n",
    "                content.parse()                    \n",
    "            except Exception as e: \n",
    "                #in case the download fails, it prints the error and immediatly continues with downloading the next article\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "            title = content.title #extract article titles\n",
    "            title_list.append(title)\n",
    "            text = content.text\n",
    "            text_list.append(text)\n",
    "                \n",
    "#creating dicts for formatting and inserting to pandas df\n",
    "source_dict = {'source':source_list}\n",
    "link_dict = {'link':article_list}\n",
    "date_dict = {'published_date':date_list}\n",
    "time_dict = {'published_time':time_list}\n",
    "title_dict = {'title':title_list}\n",
    "text_dict = {'text':text_list}\n",
    "\n",
    "#creating separate pandas dfs for each feature\n",
    "source_df = pd.DataFrame(source_dict, index=None)\n",
    "link_df = pd.DataFrame(link_dict, index=None)\n",
    "date_df = pd.DataFrame(date_dict, index=None)\n",
    "time_df = pd.DataFrame(time_dict, index=None)\n",
    "title_df = pd.DataFrame(title_dict, index=None)\n",
    "text_df = pd.DataFrame(text_dict, index=None)\n",
    "\n",
    "#join all pandas dfs together\n",
    "news_df = source_df.join(link_df)\n",
    "news_df = news_df.join(date_df)\n",
    "news_df = news_df.join(time_df)\n",
    "news_df = news_df.join(title_df)\n",
    "news_df = news_df.join(text_df)\n",
    "\n",
    "# after running, pandas DF sould be created with link, published_date, published_time, title and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Filtering and cleaning the dataset\n",
    "\n",
    "In order to run some analysis on the titles and text content of the articles, we need to clean them.\n",
    "We first filter all the articles we scraped by todays date. \n",
    "For cleaning the titles and article content text, we go through the following steps:\n",
    "\n",
    "*  remove stopwords (i.e. \"a\", \"for\", \"when\", \"you\", \"if\",... etc. that would impact the accuracy of our similarity analysis)\n",
    "*  remove punctuation\n",
    "*  remove numbers\n",
    "*  remove names of the source website in the article text (we noticed, that f.e. CNN often mentions \"CNN\" in their articles, which would impact on the accuracy of our similarty analysis)\n",
    "*  make the sentences lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/7e...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>22:46:01 UTC</td>\n",
       "      <td>Pentagon official testifies that she was told ...</td>\n",
       "      <td>(CNN) A key Pentagon official told House impea...</td>\n",
       "      <td>pentagon official testifies told ukrainians al...</td>\n",
       "      <td>key pentagon official told house impeachment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/Co...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>22:45:11 UTC</td>\n",
       "      <td>READ: Impeachment testimony of former Ukraine ...</td>\n",
       "      <td>(CNN) The House committees running the impeach...</td>\n",
       "      <td>read impeachment testimony former ukraine aide...</td>\n",
       "      <td>house committees running impeachment inquiry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/F5...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:37:12 UTC</td>\n",
       "      <td>Impeachment witness says in court filing Mulva...</td>\n",
       "      <td>(CNN) Impeachment witness Charles Kupperman di...</td>\n",
       "      <td>impeachment witness says court filing mulvaney...</td>\n",
       "      <td>impeachment witness charles kupperman distanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/uO...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>19:36:50 UTC</td>\n",
       "      <td>Donald Trump Jr. leaves stage after protests a...</td>\n",
       "      <td>Life beyond Netflix: What you should know abou...</td>\n",
       "      <td>donald trump jr leaves stage protests ucla event</td>\n",
       "      <td>life beyond netflix know new wave streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/hH...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>23:00:26 UTC</td>\n",
       "      <td>5 times Capitol Hill testimony left its mark o...</td>\n",
       "      <td>(CNN) A news public phase of the impeachment i...</td>\n",
       "      <td>times capitol hill testimony left mark trump ...</td>\n",
       "      <td>news public phase impeachment inquiry kicks w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               link published_date  \\\n",
       "0    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/7e...     2019-11-11   \n",
       "1    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Co...     2019-11-11   \n",
       "2    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/F5...     2019-11-11   \n",
       "3    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/uO...     2019-11-11   \n",
       "4    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/hH...     2019-11-11   \n",
       "\n",
       "  published_time                                              title  \\\n",
       "0   22:46:01 UTC  Pentagon official testifies that she was told ...   \n",
       "1   22:45:11 UTC  READ: Impeachment testimony of former Ukraine ...   \n",
       "2   21:37:12 UTC  Impeachment witness says in court filing Mulva...   \n",
       "3   19:36:50 UTC  Donald Trump Jr. leaves stage after protests a...   \n",
       "4   23:00:26 UTC  5 times Capitol Hill testimony left its mark o...   \n",
       "\n",
       "                                                text  \\\n",
       "0  (CNN) A key Pentagon official told House impea...   \n",
       "1  (CNN) The House committees running the impeach...   \n",
       "2  (CNN) Impeachment witness Charles Kupperman di...   \n",
       "3  Life beyond Netflix: What you should know abou...   \n",
       "4  (CNN) A news public phase of the impeachment i...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  pentagon official testifies told ukrainians al...   \n",
       "1  read impeachment testimony former ukraine aide...   \n",
       "2  impeachment witness says court filing mulvaney...   \n",
       "3   donald trump jr leaves stage protests ucla event   \n",
       "4   times capitol hill testimony left mark trump ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0   key pentagon official told house impeachment ...  \n",
       "1   house committees running impeachment inquiry ...  \n",
       "2   impeachment witness charles kupperman distanc...  \n",
       "3        life beyond netflix know new wave streaming  \n",
       "4   news public phase impeachment inquiry kicks w...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of english stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Creating a dictionary for removing the names of the source websites\n",
    "sources_list = (list(source_dict.values()))\n",
    "for i in sources_list:\n",
    "    sources_set = set(i)\n",
    "sources_to_replace = dict.fromkeys(sources_set, \"\") # replace every source with \"\" nothing\n",
    "\n",
    "# Cleaning the dataframe\n",
    "news_df_daily = news_df[news_df.published_date == today] # filter by todays date\n",
    "news_df_daily = news_df_daily.reset_index(drop=True) # reseting the index\n",
    "\n",
    "news_df_daily[\"clean_title\"] = news_df_daily[\"title\"].str.lower()\n",
    "news_df_daily[\"clean_text\"] = news_df_daily[\"text\"].str.lower()\n",
    "\n",
    "# Filter out the stopwords\n",
    "news_df_daily['clean_title'] = news_df_daily['clean_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "news_df_daily['clean_text'] = news_df_daily['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "news_df_daily[\"clean_title\"] = ((news_df_daily[\"clean_title\"].str.replace('[^\\w\\s]','')) # remove punctuation from titles\n",
    "                                .str.replace('\\d+', '')) # remove numbers from titles\n",
    "\n",
    "news_df_daily[\"clean_text\"] = (((news_df_daily[\"clean_text\"].str.replace('[^\\w\\s]','')) #remove punctuation from texts\n",
    "                                .str.replace('\\d+', '')) # remove numbers from texts\n",
    "                               .replace(sources_to_replace, regex=True)) # remove source website names in text\n",
    "\n",
    "news_df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing the dataset\n",
    "\n",
    "In this step, we apply several different analysis methods, in order to define which articles out of those we scraped are **most relevant** for portfolio trading customers and **cover trending financial topics**.\n",
    "\n",
    "### 2.1. Cosine similarity\n",
    "\n",
    "Cosine similarity is a metric for measuring the similarity between two sentences. It creates numbered vectors out of sentences and measures the **cosine of the angle between them**.\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d\" alt=\"Cosine similarity formula\" title=\"Cosine similarity formula\" />\n",
    "\n",
    "where\n",
    "* A ........... vector A\n",
    "* A • B ..... dot product between vector A and B\n",
    "* | A | ....... length of vector A\n",
    "\n",
    "\n",
    "We apply this measure for both the title and the texts.\n",
    "\n",
    "#### 2.1.A. Cosine similarity: titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.172133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2      3      4    5      6         7    8         9   \\\n",
       "0  1.0  0.000000  0.000000  0.000  0.000  0.0  0.000  0.000000  0.0  0.000000   \n",
       "1  0.0  1.000000  0.117851  0.000  0.125  0.0  0.000  0.000000  0.0  0.000000   \n",
       "2  0.0  0.117851  1.000000  0.000  0.000  0.0  0.000  0.172133  0.0  0.000000   \n",
       "3  0.0  0.000000  0.000000  1.000  0.125  0.0  0.125  0.091287  0.0  0.133631   \n",
       "4  0.0  0.125000  0.000000  0.125  1.000  0.0  0.125  0.091287  0.0  0.133631   \n",
       "\n",
       "   ...   57   58   59   60   61   62   63     64   65   66  \n",
       "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000  0.0  0.0  \n",
       "1  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000  0.0  0.0  \n",
       "2  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000  0.0  0.0  \n",
       "3  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.125  0.0  0.0  \n",
       "4  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000  0.0  0.0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer #for creating count vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity #cosine similarity calculator\n",
    "\n",
    "# for analysis, we need a list of all the titles\n",
    "clean_titles_list = list(news_df_daily['clean_title'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix_title = count_vectorizer.fit_transform(clean_titles_list) # creates the count vector\n",
    "count_matrix_title = count_matrix_title.todense() # creates numpy matrix out from all count vectors\n",
    "count_matrix_title = pd.DataFrame(count_matrix_title, columns=count_vectorizer.get_feature_names()) # creates pandas dataframe from count vectors\n",
    "\n",
    "# apply consine smilarity on count vector dataframe\n",
    "df_cosim_title = pd.DataFrame(cosine_similarity(count_matrix_title, count_matrix_title))\n",
    "df_cosim_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.B. Cosine similarity: texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333087</td>\n",
       "      <td>0.123590</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.184121</td>\n",
       "      <td>0.026965</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>0.104579</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095805</td>\n",
       "      <td>0.057268</td>\n",
       "      <td>0.076074</td>\n",
       "      <td>0.061210</td>\n",
       "      <td>0.075524</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>0.050307</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>0.089991</td>\n",
       "      <td>0.095805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149822</td>\n",
       "      <td>0.040032</td>\n",
       "      <td>0.101154</td>\n",
       "      <td>0.175960</td>\n",
       "      <td>0.080603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030753</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>0.008095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123590</td>\n",
       "      <td>0.157732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086521</td>\n",
       "      <td>0.023453</td>\n",
       "      <td>0.087835</td>\n",
       "      <td>0.257721</td>\n",
       "      <td>0.108611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047423</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.032997</td>\n",
       "      <td>0.037083</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>0.018017</td>\n",
       "      <td>0.034701</td>\n",
       "      <td>0.035974</td>\n",
       "      <td>0.045052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>0.128586</td>\n",
       "      <td>0.048737</td>\n",
       "      <td>0.119689</td>\n",
       "      <td>0.031068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.017379</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.038311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.031201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.184121</td>\n",
       "      <td>0.149822</td>\n",
       "      <td>0.086521</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049010</td>\n",
       "      <td>0.209882</td>\n",
       "      <td>0.281084</td>\n",
       "      <td>0.093297</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124324</td>\n",
       "      <td>0.056202</td>\n",
       "      <td>0.080657</td>\n",
       "      <td>0.053098</td>\n",
       "      <td>0.049782</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.049059</td>\n",
       "      <td>0.045413</td>\n",
       "      <td>0.094623</td>\n",
       "      <td>0.083783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.000000  0.333087  0.123590  0.021398  0.184121  0.026965  0.147046   \n",
       "1  0.333087  1.000000  0.157732  0.000000  0.149822  0.040032  0.101154   \n",
       "2  0.123590  0.157732  1.000000  0.000000  0.086521  0.023453  0.087835   \n",
       "3  0.021398  0.000000  0.000000  1.000000  0.024749  0.128586  0.048737   \n",
       "4  0.184121  0.149822  0.086521  0.024749  1.000000  0.049010  0.209882   \n",
       "\n",
       "         7         8         9   ...        57        58        59        60  \\\n",
       "0  0.104579  0.093073  0.021398  ...  0.095805  0.057268  0.076074  0.061210   \n",
       "1  0.175960  0.080603  0.000000  ...  0.032378  0.009017  0.000000  0.000000   \n",
       "2  0.257721  0.108611  0.000000  ...  0.047423  0.026415  0.022515  0.032997   \n",
       "3  0.119689  0.031068  1.000000  ...  0.015600  0.017379  0.021162  0.038311   \n",
       "4  0.281084  0.093297  0.024749  ...  0.124324  0.056202  0.080657  0.053098   \n",
       "\n",
       "         61        62        63        64        65        66  \n",
       "0  0.075524  0.033434  0.050307  0.056995  0.089991  0.095805  \n",
       "1  0.000000  0.000000  0.030753  0.013162  0.018893  0.008095  \n",
       "2  0.037083  0.014540  0.018017  0.034701  0.035974  0.045052  \n",
       "3  0.000000  0.000000  0.059270  0.000000  0.018206  0.031201  \n",
       "4  0.049782  0.024859  0.049059  0.045413  0.094623  0.083783  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for analysis, we need a list of all the texts\n",
    "clean_texts_list = list(news_df_daily['clean_text'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix_text = count_vectorizer.fit_transform(clean_texts_list) # creates the count vector\n",
    "count_matrix_text = count_matrix_text.todense() # creates numpy matrix out from all count vectors\n",
    "#count_matrix_text.shape\n",
    "\n",
    "count_matrix_text = pd.DataFrame(count_matrix_text, columns=count_vectorizer.get_feature_names()) # creates pandas dataframe from count vectors\n",
    "\n",
    "# apply consine smilarity on count vector dataframe\n",
    "df_cosim_texts = pd.DataFrame(cosine_similarity(count_matrix_text, count_matrix_text))\n",
    "df_cosim_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Soft cosine similarity measure\n",
    "\n",
    "Metric for measuring the similarity between two sentences, but gives **higher scores for words with similar meaning**. For Example, ‘President’ vs ‘Prime minister’, ‘Food’ vs ‘Dish’, ‘Hi’ vs ‘Hello’ are considered similar. \n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9743aceb346ccb501ceaef15a46570d1ba8a6a1b\" alt=\"Soft cosine formula\" title=\"Soft cosine formula\" />\n",
    "\n",
    "where\n",
    "* sij .... similarity (feature i, feature j)\n",
    "\n",
    "**Difference to cosine similarity**: the traditional cosine similarity considers the vector space model (VSM i.e. features, unique words) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which help generalize the concept of cosine (and soft cosine) as well as the idea of (soft) similarity. https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "This implies that we need some vector defining the similarity between words i.e. vectors of words that are similar. \n",
    "In our case we are going to use the pretrained `fasttext-wiki-news-subwords-300` vector dataset containing 1 million word vectors trained on Wikipedia 2017. More info here: https://github.com/RaRe-Technologies/gensim-data/releases/tag/fasttext-wiki-news-subwords-300\n",
    "\n",
    "_**Side note:** other pre-trained models to be found here: https://github.com/RaRe-Technologies/gensim-data/releases_\n",
    "\n",
    "#### 2.2.A. Soft cosine measure: titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ! ### this will download a file to your harddrive ### ! ###\n",
    "\n",
    "# first we need to download the FastText model - about 960MB\n",
    "# if already downloaded on machine it will only load it, this is a little faster - around 2-5min\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democrats', 0.7794002294540405),\n",
       " ('democrate', 0.7524039149284363),\n",
       " ('republican', 0.7467405200004578),\n",
       " ('anti-democrat', 0.7122665047645569),\n",
       " ('social-democrat', 0.7080994844436646),\n",
       " ('Democrat', 0.7080677151679993),\n",
       " ('socalist', 0.6955678462982178),\n",
       " ('democratic', 0.6946688890457153),\n",
       " ('liberalist', 0.6911271810531616),\n",
       " ('democratic-socialist', 0.688860297203064)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the word vectors from the model\n",
    "fasttext_model300.most_similar(positive=\"democrat\") # outputs words similar to this one\n",
    "#fasttext_model300.similarity(\"democrat\", \"republican\") # outputs the computed smilarity between the two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary, a map of word to unique id from the title list\n",
    "dictionary_titles = corpora.Dictionary([simple_preprocess(word) for word in clean_titles_list])\n",
    "\n",
    "# generate a similarity sparse matrix from the words in the dictionary\n",
    "# this process takes a bit due to calculation time\n",
    "similarity_matrix_titles = fasttext_model300.similarity_matrix(dictionary_titles, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the titles into bag-of-words vectors through function\n",
    "# appends the bag-of-words from all sentences into the sent list\n",
    "def convert_bow(sentences):\n",
    "    global sent_bow\n",
    "    sent_bow = []\n",
    "    for i in sentences:\n",
    "        bow = dictionary_titles.doc2bow(simple_preprocess(i))\n",
    "        sent_bow.append(bow)\n",
    "        \n",
    "convert_bow(clean_titles_list) \n",
    "\n",
    "#create soft cosine measure matrix thourgh function \n",
    "\"\"\" creates a matrix with the results of soft cosine measure calculation.\n",
    "Takes into account the previously created similarity sparse matrix was created from the similar word meanings \n",
    "(we extracted from the FastText model) from the unique words that were in our unique dictionary.\"\"\"\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array) # creates a grid with dimensions (nr of articles x nr of articles)\n",
    "    soft_cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix_titles) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return soft_cossim_mat\n",
    "\n",
    "soft_cossim_mat_titles = create_soft_cossim_matrix(sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4    5     6     7     8     9   ...    57    58  \\\n",
       "0  1.00  0.18  0.08  0.00  0.05  0.0  0.00  0.03  0.00  0.00  ...  0.07  0.00   \n",
       "1  0.18  1.00  0.17  0.00  0.16  0.0  0.00  0.00  0.06  0.05  ...  0.00  0.00   \n",
       "2  0.08  0.17  1.00  0.00  0.09  0.0  0.00  0.31  0.02  0.00  ...  0.06  0.00   \n",
       "3  0.00  0.00  0.00  1.00  0.17  0.0  0.12  0.12  0.03  0.12  ...  0.05  0.03   \n",
       "4  0.05  0.16  0.09  0.17  1.00  0.0  0.12  0.11  0.07  0.13  ...  0.00  0.03   \n",
       "\n",
       "    59   60    61   62   63    64    65    66  \n",
       "0  0.0  0.0  0.00  0.0  0.0  0.04  0.00  0.04  \n",
       "1  0.0  0.0  0.00  0.0  0.0  0.00  0.00  0.00  \n",
       "2  0.0  0.0  0.00  0.0  0.0  0.05  0.04  0.00  \n",
       "3  0.0  0.0  0.00  0.0  0.0  0.11  0.06  0.00  \n",
       "4  0.0  0.0  0.04  0.0  0.0  0.00  0.03  0.00  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_cossim_mat_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.B. Soft cosine measure: texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**! Be aware !** \n",
    "\n",
    "When you run the cell below - even when having only around 50 articles - the creation of a unique word dictionary and especially the corresponding similarity matrix for article texts takes at least 2 to 5min. \n",
    "\n",
    "This waiting time cannot be skipped for text soft cosine measure similarity comparison, since it just takes a lot of ressources to compute. If you want to time how long it exacly takes, look below for paragraph _X. Other stuff that could be helpful in the future_ - there is a code for timing the run time of a code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary, a map of word to unique id from the text list\n",
    "dictionary_texts = corpora.Dictionary([simple_preprocess(word) for word in clean_texts_list])\n",
    "\n",
    "# generate a similarity sparse matrix from the words in the dictionary\n",
    "# this process takes a bit due to calculation time\n",
    "similarity_matrix_texts = fasttext_model300.similarity_matrix(dictionary_texts, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the texts into bag-of-words vectors through function\n",
    "# appends the bag-of-words from all sentences into the sent list\n",
    "def convert_bow(sentences):\n",
    "    global sent_bow\n",
    "    sent_bow = []\n",
    "    for i in sentences:\n",
    "        bow = dictionary_texts.doc2bow(simple_preprocess(i))\n",
    "        sent_bow.append(bow)\n",
    "        \n",
    "convert_bow(clean_texts_list) \n",
    "\n",
    "#create soft cosine measure matrix thourgh function \n",
    "\"\"\" creates a matrix with the results of soft cosine measure calculation.\n",
    "Takes into account the previously created similarity sparse matrix was created from the similar word meanings \n",
    "(we extracted from the FastText model) from the unique words that were in our unique dictionary.\"\"\"\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array) # creates a grid with dimensions (nr of articles x nr of articles)\n",
    "    soft_cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix_texts) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return soft_cossim_mat\n",
    "\n",
    "soft_cossim_mat_texts = create_soft_cossim_matrix(sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9   ...    57  \\\n",
       "0  1.00  0.41  0.22  0.08  0.41  0.18  0.40  0.24  0.22  0.08  ...  0.31   \n",
       "1  0.41  1.00  0.22  0.05  0.27  0.09  0.15  0.20  0.19  0.05  ...  0.11   \n",
       "2  0.22  0.22  1.00  0.03  0.27  0.12  0.24  0.42  0.16  0.03  ...  0.19   \n",
       "3  0.08  0.05  0.03  1.00  0.07  0.15  0.15  0.11  0.05  1.00  ...  0.06   \n",
       "4  0.41  0.27  0.27  0.07  1.00  0.20  0.44  0.47  0.28  0.07  ...  0.32   \n",
       "\n",
       "     58    59    60    61    62    63    64    65    66  \n",
       "0  0.22  0.18  0.27  0.25  0.12  0.20  0.17  0.29  0.28  \n",
       "1  0.09  0.03  0.04  0.07  0.02  0.08  0.03  0.05  0.04  \n",
       "2  0.16  0.10  0.12  0.13  0.06  0.10  0.07  0.15  0.14  \n",
       "3  0.06  0.06  0.07  0.10  0.03  0.09  0.04  0.09  0.13  \n",
       "4  0.25  0.18  0.21  0.21  0.13  0.19  0.15  0.27  0.25  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_cossim_mat_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results: extracting most similar articles\n",
    "\n",
    "After finding some results for the similarity in our scraped articles, we have to **filter the similar articles out of our initial** `news_df_daily` **dataframe**, in order to find out the title and article text.\n",
    "\n",
    "We want to extract only articles that have some predefined minimum value for similarity f.e. we only want **articles that have a similarity of at least 0.7** (this number could vary depending on our choice). Since the row indexes and the column numbers in the `soft_cossim_mat` matrix are equal to the indexes of the articles in our initial `news_df_daily` dataframe, we need to filter `news_df_daily` by exactly these indexes which contain the minimum similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function to find the row and column index in a dataframe for a specific value\n",
    "def get_indexes(dataframe, value):\n",
    "    pos_list = list()\n",
    "    for i in value:\n",
    "        result = dataframe.isin([value]) # crete bool dataframe with True at positions where the given value exists\n",
    "        series = result.any()\n",
    "        column_names = list(series[series == True].index) # create list of columns that contain the value\n",
    "        for col in column_names: # iterate over list of columns and fetch the rows indexes where value exists\n",
    "            rows = list(result[col][result[col] == True].index)\n",
    "            for row in rows:\n",
    "                if row != col: # since matrix diagonal is always == 1, we exclude these results here\n",
    "                    pos_list.append((row, col)) #creates a list of row, col position\n",
    "        return pos_list # Return a list of tuples indicating the positions of value in the dataframe\n",
    "    \n",
    "# choosing the range of similarity values for which the sentences should be filtered\n",
    "simval = np.arange(0.4, 1.01, 0.01) # choose similarity values between first number and 1.0, by steps of 0.01\n",
    "simval = np.around(simval, decimals=2)\n",
    "simval = (simval.astype(str))\n",
    " \n",
    "# use dict comprehension and 'get_indexes' function to get index positions of elements in df with predefined similarity values\n",
    "dict_pos_titles = {elem: get_indexes(soft_cossim_mat_titles, elem) for elem in simval}\n",
    "dict_pos_texts = {elem: get_indexes(soft_cossim_mat_texts, elem) for elem in simval}\n",
    "\n",
    "# function for creating a list of the row indexes\n",
    "def find_indexes(dict_pos, index_list):\n",
    "    for key, value in dict_pos.items():\n",
    "    #print(key, ' : ', value) # this prints the similarity values and its corresponding row and col indexes in the df\n",
    "        for num in value:\n",
    "            for firstnum in num:\n",
    "                index_list.append(firstnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Most similar articles: by similarity of article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/dm...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>13:19:31 UTC</td>\n",
       "      <td>Veteran: Trump family tone deaf to military co...</td>\n",
       "      <td>Life beyond Netflix: What you should know abou...</td>\n",
       "      <td>veteran trump family tone deaf military community</td>\n",
       "      <td>life beyond netflix know new wave streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/VP...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>23:07:48 UTC</td>\n",
       "      <td>Man set on fire in Hong Kong hours after prote...</td>\n",
       "      <td>Anderson Cooper speaks with CNN's Paula Hancoc...</td>\n",
       "      <td>man set fire hong kong hours protester shot</td>\n",
       "      <td>anderson cooper speaks s paula hancocks protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/TT...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>23:19:47 UTC</td>\n",
       "      <td>Family of WWII veteran surprised to learn new ...</td>\n",
       "      <td>The family of World War II veteran Lucian Bask...</td>\n",
       "      <td>family wwii veteran surprised learn new detail...</td>\n",
       "      <td>family world war ii veteran lucian baskin surp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/E9...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:24:38 UTC</td>\n",
       "      <td>A World War II submarine that was missing for ...</td>\n",
       "      <td>(CNN) It's been 75 years since the USS Graybac...</td>\n",
       "      <td>world war ii submarine missing  years found ok...</td>\n",
       "      <td>years since uss grayback went missing  sailo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/ON...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:14:43 UTC</td>\n",
       "      <td>WWII submarine found after being missing for 7...</td>\n",
       "      <td>The Lost 52 Project discovered a WWII US Navy ...</td>\n",
       "      <td>wwii submarine found missing  years</td>\n",
       "      <td>lost  project discovered wwii us navy submarin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/HN...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:28:55 UTC</td>\n",
       "      <td>Hong Kong man set alight hours after protester...</td>\n",
       "      <td>Hong Kong (CNN) A man has been set on fire in ...</td>\n",
       "      <td>hong kong man set alight hours protester shot ...</td>\n",
       "      <td>hong kong  man set fire hong kong hours protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/saudi-aramco-s...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:44:00 UTC</td>\n",
       "      <td>Saudi Aramco stock could price at volatile tim...</td>\n",
       "      <td>The initial public offering of Saudi Arabia's ...</td>\n",
       "      <td>saudi aramco stock could price volatile time o...</td>\n",
       "      <td>initial public offering saudi arabias big oil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/what-happened-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:07:00 UTC</td>\n",
       "      <td>Here's what happened to the stock market on Mo...</td>\n",
       "      <td>The Dow rose 10.25 points, or 0.04%, to close ...</td>\n",
       "      <td>heres happened stock market monday</td>\n",
       "      <td>dow rose  points  close  sp  dipped   nasdaq c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/regulator-prob...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>19:32:00 UTC</td>\n",
       "      <td>Regulator probing Goldman over Apple Card: Gen...</td>\n",
       "      <td>Companies that deploy biased algorithms — even...</td>\n",
       "      <td>regulator probing goldman apple card gender bi...</td>\n",
       "      <td>companies deploy biased algorithms  even unkno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/goldman-wants-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>17:08:00 UTC</td>\n",
       "      <td>Goldman is looking to fix the flaw that has Ap...</td>\n",
       "      <td>Goldman Sachs is looking into ways that family...</td>\n",
       "      <td>goldman looking fix flaw apple card users clai...</td>\n",
       "      <td>goldman sachs looking ways family members shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/stocks-making-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>16:51:00 UTC</td>\n",
       "      <td>Stocks making the biggest moves midday: Walgre...</td>\n",
       "      <td>Check out the companies making headlines midda...</td>\n",
       "      <td>stocks making biggest moves midday walgreens t...</td>\n",
       "      <td>check companies making headlines midday walgre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/stocks-making-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>12:43:00 UTC</td>\n",
       "      <td>Stocks making the biggest moves premarket: Boe...</td>\n",
       "      <td>Check out the companies making headlines befor...</td>\n",
       "      <td>stocks making biggest moves premarket boeing a...</td>\n",
       "      <td>check companies making headlines bell boeing b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/stanchart-dbs-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>07:16:00 UTC</td>\n",
       "      <td>Hong Kong protests haven't hurt our profitabil...</td>\n",
       "      <td>Pro-democracy protests have hurt the Hong Kong...</td>\n",
       "      <td>hong kong protests hurt profitability say bank...</td>\n",
       "      <td>prodemocracy protests hurt hong kong economy c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                               link published_date  \\\n",
       "9     cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/dm...     2019-11-11   \n",
       "15    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/VP...     2019-11-11   \n",
       "25    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/TT...     2019-11-11   \n",
       "28    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/E9...     2019-11-11   \n",
       "29    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/ON...     2019-11-11   \n",
       "41    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/HN...     2019-11-11   \n",
       "51   cnbc  https://www.cnbc.com/2019/11/11/saudi-aramco-s...     2019-11-11   \n",
       "52   cnbc  https://www.cnbc.com/2019/11/11/what-happened-...     2019-11-11   \n",
       "54   cnbc  https://www.cnbc.com/2019/11/11/regulator-prob...     2019-11-11   \n",
       "58   cnbc  https://www.cnbc.com/2019/11/11/goldman-wants-...     2019-11-11   \n",
       "59   cnbc  https://www.cnbc.com/2019/11/11/stocks-making-...     2019-11-11   \n",
       "63   cnbc  https://www.cnbc.com/2019/11/11/stocks-making-...     2019-11-11   \n",
       "64   cnbc  https://www.cnbc.com/2019/11/11/stanchart-dbs-...     2019-11-11   \n",
       "\n",
       "   published_time                                              title  \\\n",
       "9    13:19:31 UTC  Veteran: Trump family tone deaf to military co...   \n",
       "15   23:07:48 UTC  Man set on fire in Hong Kong hours after prote...   \n",
       "25   23:19:47 UTC  Family of WWII veteran surprised to learn new ...   \n",
       "28   21:24:38 UTC  A World War II submarine that was missing for ...   \n",
       "29   21:14:43 UTC  WWII submarine found after being missing for 7...   \n",
       "41   21:28:55 UTC  Hong Kong man set alight hours after protester...   \n",
       "51   21:44:00 UTC  Saudi Aramco stock could price at volatile tim...   \n",
       "52   21:07:00 UTC  Here's what happened to the stock market on Mo...   \n",
       "54   19:32:00 UTC  Regulator probing Goldman over Apple Card: Gen...   \n",
       "58   17:08:00 UTC  Goldman is looking to fix the flaw that has Ap...   \n",
       "59   16:51:00 UTC  Stocks making the biggest moves midday: Walgre...   \n",
       "63   12:43:00 UTC  Stocks making the biggest moves premarket: Boe...   \n",
       "64   07:16:00 UTC  Hong Kong protests haven't hurt our profitabil...   \n",
       "\n",
       "                                                 text  \\\n",
       "9   Life beyond Netflix: What you should know abou...   \n",
       "15  Anderson Cooper speaks with CNN's Paula Hancoc...   \n",
       "25  The family of World War II veteran Lucian Bask...   \n",
       "28  (CNN) It's been 75 years since the USS Graybac...   \n",
       "29  The Lost 52 Project discovered a WWII US Navy ...   \n",
       "41  Hong Kong (CNN) A man has been set on fire in ...   \n",
       "51  The initial public offering of Saudi Arabia's ...   \n",
       "52  The Dow rose 10.25 points, or 0.04%, to close ...   \n",
       "54  Companies that deploy biased algorithms — even...   \n",
       "58  Goldman Sachs is looking into ways that family...   \n",
       "59  Check out the companies making headlines midda...   \n",
       "63  Check out the companies making headlines befor...   \n",
       "64  Pro-democracy protests have hurt the Hong Kong...   \n",
       "\n",
       "                                          clean_title  \\\n",
       "9   veteran trump family tone deaf military community   \n",
       "15        man set fire hong kong hours protester shot   \n",
       "25  family wwii veteran surprised learn new detail...   \n",
       "28  world war ii submarine missing  years found ok...   \n",
       "29                wwii submarine found missing  years   \n",
       "41  hong kong man set alight hours protester shot ...   \n",
       "51  saudi aramco stock could price volatile time o...   \n",
       "52                 heres happened stock market monday   \n",
       "54  regulator probing goldman apple card gender bi...   \n",
       "58  goldman looking fix flaw apple card users clai...   \n",
       "59  stocks making biggest moves midday walgreens t...   \n",
       "63  stocks making biggest moves premarket boeing a...   \n",
       "64  hong kong protests hurt profitability say bank...   \n",
       "\n",
       "                                           clean_text  \n",
       "9         life beyond netflix know new wave streaming  \n",
       "15  anderson cooper speaks s paula hancocks protes...  \n",
       "25  family world war ii veteran lucian baskin surp...  \n",
       "28    years since uss grayback went missing  sailo...  \n",
       "29  lost  project discovered wwii us navy submarin...  \n",
       "41  hong kong  man set fire hong kong hours protes...  \n",
       "51  initial public offering saudi arabias big oil ...  \n",
       "52  dow rose  points  close  sp  dipped   nasdaq c...  \n",
       "54  companies deploy biased algorithms  even unkno...  \n",
       "58  goldman sachs looking ways family members shar...  \n",
       "59  check companies making headlines midday walgre...  \n",
       "63  check companies making headlines bell boeing b...  \n",
       "64  prodemocracy protests hurt hong kong economy c...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list_titles = []\n",
    "find_indexes(dict_pos_titles, index_list_titles)\n",
    "index_list_titles = list(set(index_list_titles))\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_list_titles, :]).drop_duplicates()).sort_index()\n",
    "select_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Most similar articles: by similarity of article texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_indexes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-84df8f04ee4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mindex_list_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfind_indexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_pos_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_list_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mindex_list_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_list_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mselect_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_df_daily\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_list_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'find_indexes' is not defined"
     ]
    }
   ],
   "source": [
    "index_list_texts = []\n",
    "find_indexes(dict_pos_texts, index_list_texts)\n",
    "index_list_texts = list(set(index_list_texts))\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_list_texts, :]).drop_duplicates()).sort_index()\n",
    "select_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Most similar articles: by similarity of article titles and articles\n",
    "\n",
    "This filters the sentences by the `simval` defined before and keeps only the titles and the texts that BOTH match the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/dm...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>13:19:31 UTC</td>\n",
       "      <td>Veteran: Trump family tone deaf to military co...</td>\n",
       "      <td>Life beyond Netflix: What you should know abou...</td>\n",
       "      <td>veteran trump family tone deaf military community</td>\n",
       "      <td>life beyond netflix know new wave streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/VP...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>23:07:48 UTC</td>\n",
       "      <td>Man set on fire in Hong Kong hours after prote...</td>\n",
       "      <td>Anderson Cooper speaks with CNN's Paula Hancoc...</td>\n",
       "      <td>man set fire hong kong hours protester shot</td>\n",
       "      <td>anderson cooper speaks s paula hancocks protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/E9...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:24:38 UTC</td>\n",
       "      <td>A World War II submarine that was missing for ...</td>\n",
       "      <td>(CNN) It's been 75 years since the USS Graybac...</td>\n",
       "      <td>world war ii submarine missing  years found ok...</td>\n",
       "      <td>years since uss grayback went missing  sailo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/ON...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:14:43 UTC</td>\n",
       "      <td>WWII submarine found after being missing for 7...</td>\n",
       "      <td>The Lost 52 Project discovered a WWII US Navy ...</td>\n",
       "      <td>wwii submarine found missing  years</td>\n",
       "      <td>lost  project discovered wwii us navy submarin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topstories/~3/HN...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:28:55 UTC</td>\n",
       "      <td>Hong Kong man set alight hours after protester...</td>\n",
       "      <td>Hong Kong (CNN) A man has been set on fire in ...</td>\n",
       "      <td>hong kong man set alight hours protester shot ...</td>\n",
       "      <td>hong kong  man set fire hong kong hours protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/saudi-aramco-s...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:44:00 UTC</td>\n",
       "      <td>Saudi Aramco stock could price at volatile tim...</td>\n",
       "      <td>The initial public offering of Saudi Arabia's ...</td>\n",
       "      <td>saudi aramco stock could price volatile time o...</td>\n",
       "      <td>initial public offering saudi arabias big oil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/what-happened-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>21:07:00 UTC</td>\n",
       "      <td>Here's what happened to the stock market on Mo...</td>\n",
       "      <td>The Dow rose 10.25 points, or 0.04%, to close ...</td>\n",
       "      <td>heres happened stock market monday</td>\n",
       "      <td>dow rose  points  close  sp  dipped   nasdaq c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/regulator-prob...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>19:32:00 UTC</td>\n",
       "      <td>Regulator probing Goldman over Apple Card: Gen...</td>\n",
       "      <td>Companies that deploy biased algorithms — even...</td>\n",
       "      <td>regulator probing goldman apple card gender bi...</td>\n",
       "      <td>companies deploy biased algorithms  even unkno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/goldman-wants-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>17:08:00 UTC</td>\n",
       "      <td>Goldman is looking to fix the flaw that has Ap...</td>\n",
       "      <td>Goldman Sachs is looking into ways that family...</td>\n",
       "      <td>goldman looking fix flaw apple card users clai...</td>\n",
       "      <td>goldman sachs looking ways family members shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/stocks-making-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>16:51:00 UTC</td>\n",
       "      <td>Stocks making the biggest moves midday: Walgre...</td>\n",
       "      <td>Check out the companies making headlines midda...</td>\n",
       "      <td>stocks making biggest moves midday walgreens t...</td>\n",
       "      <td>check companies making headlines midday walgre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/stocks-making-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>12:43:00 UTC</td>\n",
       "      <td>Stocks making the biggest moves premarket: Boe...</td>\n",
       "      <td>Check out the companies making headlines befor...</td>\n",
       "      <td>stocks making biggest moves premarket boeing a...</td>\n",
       "      <td>check companies making headlines bell boeing b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/2019/11/11/stanchart-dbs-...</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>07:16:00 UTC</td>\n",
       "      <td>Hong Kong protests haven't hurt our profitabil...</td>\n",
       "      <td>Pro-democracy protests have hurt the Hong Kong...</td>\n",
       "      <td>hong kong protests hurt profitability say bank...</td>\n",
       "      <td>prodemocracy protests hurt hong kong economy c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                               link published_date  \\\n",
       "9     cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/dm...     2019-11-11   \n",
       "15    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/VP...     2019-11-11   \n",
       "28    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/E9...     2019-11-11   \n",
       "29    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/ON...     2019-11-11   \n",
       "41    cnn  http://rss.cnn.com/~r/rss/cnn_topstories/~3/HN...     2019-11-11   \n",
       "51   cnbc  https://www.cnbc.com/2019/11/11/saudi-aramco-s...     2019-11-11   \n",
       "52   cnbc  https://www.cnbc.com/2019/11/11/what-happened-...     2019-11-11   \n",
       "54   cnbc  https://www.cnbc.com/2019/11/11/regulator-prob...     2019-11-11   \n",
       "58   cnbc  https://www.cnbc.com/2019/11/11/goldman-wants-...     2019-11-11   \n",
       "59   cnbc  https://www.cnbc.com/2019/11/11/stocks-making-...     2019-11-11   \n",
       "63   cnbc  https://www.cnbc.com/2019/11/11/stocks-making-...     2019-11-11   \n",
       "64   cnbc  https://www.cnbc.com/2019/11/11/stanchart-dbs-...     2019-11-11   \n",
       "\n",
       "   published_time                                              title  \\\n",
       "9    13:19:31 UTC  Veteran: Trump family tone deaf to military co...   \n",
       "15   23:07:48 UTC  Man set on fire in Hong Kong hours after prote...   \n",
       "28   21:24:38 UTC  A World War II submarine that was missing for ...   \n",
       "29   21:14:43 UTC  WWII submarine found after being missing for 7...   \n",
       "41   21:28:55 UTC  Hong Kong man set alight hours after protester...   \n",
       "51   21:44:00 UTC  Saudi Aramco stock could price at volatile tim...   \n",
       "52   21:07:00 UTC  Here's what happened to the stock market on Mo...   \n",
       "54   19:32:00 UTC  Regulator probing Goldman over Apple Card: Gen...   \n",
       "58   17:08:00 UTC  Goldman is looking to fix the flaw that has Ap...   \n",
       "59   16:51:00 UTC  Stocks making the biggest moves midday: Walgre...   \n",
       "63   12:43:00 UTC  Stocks making the biggest moves premarket: Boe...   \n",
       "64   07:16:00 UTC  Hong Kong protests haven't hurt our profitabil...   \n",
       "\n",
       "                                                 text  \\\n",
       "9   Life beyond Netflix: What you should know abou...   \n",
       "15  Anderson Cooper speaks with CNN's Paula Hancoc...   \n",
       "28  (CNN) It's been 75 years since the USS Graybac...   \n",
       "29  The Lost 52 Project discovered a WWII US Navy ...   \n",
       "41  Hong Kong (CNN) A man has been set on fire in ...   \n",
       "51  The initial public offering of Saudi Arabia's ...   \n",
       "52  The Dow rose 10.25 points, or 0.04%, to close ...   \n",
       "54  Companies that deploy biased algorithms — even...   \n",
       "58  Goldman Sachs is looking into ways that family...   \n",
       "59  Check out the companies making headlines midda...   \n",
       "63  Check out the companies making headlines befor...   \n",
       "64  Pro-democracy protests have hurt the Hong Kong...   \n",
       "\n",
       "                                          clean_title  \\\n",
       "9   veteran trump family tone deaf military community   \n",
       "15        man set fire hong kong hours protester shot   \n",
       "28  world war ii submarine missing  years found ok...   \n",
       "29                wwii submarine found missing  years   \n",
       "41  hong kong man set alight hours protester shot ...   \n",
       "51  saudi aramco stock could price volatile time o...   \n",
       "52                 heres happened stock market monday   \n",
       "54  regulator probing goldman apple card gender bi...   \n",
       "58  goldman looking fix flaw apple card users clai...   \n",
       "59  stocks making biggest moves midday walgreens t...   \n",
       "63  stocks making biggest moves premarket boeing a...   \n",
       "64  hong kong protests hurt profitability say bank...   \n",
       "\n",
       "                                           clean_text  \n",
       "9         life beyond netflix know new wave streaming  \n",
       "15  anderson cooper speaks s paula hancocks protes...  \n",
       "28    years since uss grayback went missing  sailo...  \n",
       "29  lost  project discovered wwii us navy submarin...  \n",
       "41  hong kong  man set fire hong kong hours protes...  \n",
       "51  initial public offering saudi arabias big oil ...  \n",
       "52  dow rose  points  close  sp  dipped   nasdaq c...  \n",
       "54  companies deploy biased algorithms  even unkno...  \n",
       "58  goldman sachs looking ways family members shar...  \n",
       "59  check companies making headlines midday walgre...  \n",
       "63  check companies making headlines bell boeing b...  \n",
       "64  prodemocracy protests hurt hong kong economy c...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_intersection = (set(index_list_titles).intersection(set(index_list_texts)))\n",
    "index_intersection = list(index_intersection)\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_intersection, :]).drop_duplicates()).sort_index()\n",
    "select_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Other stuff that could be helpful in the future\n",
    "\n",
    "## Time how long a code takes to execute\n",
    "\n",
    "Could be used for speed comparison of two similarity methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "code_to_test = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "elapsed_time = timeit.timeit(code_to_test, number=100)/100\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google word meaning vector, pre-trained\n",
    "\n",
    "Maybe useful, some time?\n",
    "\n",
    "Other pre-trained models to be found here: https://github.com/RaRe-Technologies/gensim-data/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\") #1.6GB to download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting each word in title/text in pandas df to a separate column\n",
    "\n",
    "Maybe useful, some time?\n",
    "\n",
    "Code was hard to find via google haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = news_df_daily.str.split(expand=True)\n",
    "title_splitted = pd.DataFrame(split)\n",
    "title_splitted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
