{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "* Test is title is shorter than... filter out\n",
    "* Check google news... not so cool stuff\n",
    "* Implement Images\n",
    "* Continue categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # to ignore all future warinings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the dataset\n",
    "\n",
    "### 1.1 Scraping news articles from the web\n",
    "\n",
    "This process takes on average between 2 and 15min, depending on how many website links are to be scraped, how many articles in these links are found and how much computing ressources the machine has on which the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2019-12-03\n",
      "Article `download()` failed with HTTPSConnectionPool(host='news.yahoo.com', port=443): Read timed out. on URL https://news.yahoo.com/egypt-ethiopia-sudan-meet-us-183714485.html\n",
      "continuing...\n",
      "Article `download()` failed with HTTPSConnectionPool(host='news.yahoo.com', port=443): Read timed out. on URL https://news.yahoo.com/brexit-explained-us-shippers-know-141907158.html\n",
      "continuing...\n",
      "Article `download()` failed with HTTPSConnectionPool(host='finance.yahoo.com', port=443): Read timed out. on URL https://finance.yahoo.com/news/faa-calls-lufthansa-skirting-operating-140408913.html\n",
      "continuing...\n",
      "Article `download()` failed with HTTPSConnectionPool(host='finance.yahoo.com', port=443): Read timed out. on URL https://finance.yahoo.com/news/david-tepper-trims-unitedhealth-exits-200143393.html\n",
      "continuing...\n"
     ]
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import time\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "import dateutil\n",
    "\n",
    "#### 1 Website data ####\n",
    "\n",
    "with open('NewsPapers_new.json') as data_file: #Loads the JSON files with news URLs\n",
    "    companies = json.load(data_file)\n",
    "\n",
    "#### 2 Todays date - for filtering the articles by todays date ####\n",
    "today = str(date.today()) \n",
    "print(\"Today's date:\", today)\n",
    "\n",
    "\n",
    "#### 3 Scraping the news articles ####\n",
    "\n",
    "text_list, source_list, article_list, date_list, time_list, title_list, image_list, keywords_list, summaries_list = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for source, value in companies.items(): \n",
    "    d = fp.parse(value['rss'])\n",
    "    article={}\n",
    "    for entry in d.entries:\n",
    "        if hasattr(entry, 'published') and ((dateutil.parser.parse(getattr(entry, 'published'))).strftime(\"%Y-%m-%d\") == today):\n",
    "            article['source'] = source\n",
    "            source_list.append(article['source'])\n",
    "\n",
    "            # getting the article URLs\n",
    "            article['link'] = entry.link\n",
    "            article_list.append(article['link'])\n",
    "\n",
    "            # getting the article published dates\n",
    "            date = (getattr(entry, 'published'))\n",
    "            date = dateutil.parser.parse(date)\n",
    "            date_formated = date.strftime(\"%Y-%m-%d\")\n",
    "            time_formated = date.strftime(\"%H:%M:%S %Z\") # hour, minute, timezone (converted)\n",
    "            date_list.append(date_formated)\n",
    "            time_list.append(time_formated)\n",
    "\n",
    "            # \"downloading\" the articles\n",
    "            content = Article(entry.link)\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()  \n",
    "                content.nlp()\n",
    "            except Exception as e: \n",
    "                # in case the download fails, it prints the error and immediatly continues with downloading the next article\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "            \n",
    "            # save the \"downloaded\" content\n",
    "            title = content.title #extract article titles\n",
    "            image = content.top_image #extract article images\n",
    "            image_list.append(image)\n",
    "            keywords = content.keywords\n",
    "            keywords_list.append(keywords)\n",
    "            title_list.append(title)\n",
    "            text = content.text\n",
    "            text_list.append(text)\n",
    "            summaries = content.summary\n",
    "            summaries_list.append(summaries)\n",
    "                \n",
    "#creating dicts for formatting and inserting to pandas df\n",
    "source_dict = {'source':source_list}\n",
    "link_dict = {'link':article_list}\n",
    "date_dict = {'published_date':date_list}\n",
    "time_dict = {'published_time':time_list}\n",
    "title_dict = {'title':title_list}\n",
    "text_dict = {'text':text_list}\n",
    "keyword_dict = {'keywords':keywords_list}\n",
    "image_dict = {'image':image_list}\n",
    "summary_dict = {'summary':summaries_list}\n",
    "\n",
    "#creating separate pandas dfs for each feature\n",
    "source_df = pd.DataFrame(source_dict, index=None)\n",
    "link_df = pd.DataFrame(link_dict, index=None)\n",
    "date_df = pd.DataFrame(date_dict, index=None)\n",
    "time_df = pd.DataFrame(time_dict, index=None)\n",
    "title_df = pd.DataFrame(title_dict, index=None)\n",
    "text_df = pd.DataFrame(text_dict, index=None)\n",
    "keyword_df = pd.DataFrame(keyword_dict, index=None)\n",
    "image_df = pd.DataFrame(image_dict, index=None)\n",
    "summary_df = pd.DataFrame(summary_dict, index=None)\n",
    "\n",
    "#join all pandas dfs together\n",
    "news_df = source_df.join(link_df).join(date_df).join(time_df).join(title_df).join(text_df).join(keyword_df).join(image_df).join(summary_df)\n",
    "\n",
    "\n",
    "# after running, pandas DF sould be created with link, published_date, published_time, title and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Filtering and cleaning the dataset\n",
    "\n",
    "In order to run some analysis on the titles and text content of the articles, we need to clean them.\n",
    "We first filter all the articles we scraped by todays date. \n",
    "For cleaning the titles and article content text, we go through the following steps:\n",
    "\n",
    "*  remove stopwords (i.e. \"a\", \"for\", \"when\", \"you\", \"if\",... etc. that would impact the accuracy of our similarity analysis)\n",
    "*  remove punctuation\n",
    "*  remove numbers\n",
    "*  remove names of the source website in the article text (we noticed, that f.e. CNN often mentions \"CNN\" in their articles, which would impact on the accuracy of our similarty analysis)\n",
    "*  make the sentences lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elandman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "#from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>image</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:07:56 UTC</td>\n",
       "      <td>Impeachment repo...</td>\n",
       "      <td>(CNN) House Demo...</td>\n",
       "      <td>vote misconduct ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) House Demo...</td>\n",
       "      <td>impeachment repo...</td>\n",
       "      <td>house democrat s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:26:30 UTC</td>\n",
       "      <td>Read: Democrats'...</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>whats facebook r...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>read democrat tr...</td>\n",
       "      <td>chat u facebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:54:11 UTC</td>\n",
       "      <td>Sophia Nelson: A...</td>\n",
       "      <td>Sophia Nelson, f...</td>\n",
       "      <td>nelson embarrass...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Sophia Nelson, f...</td>\n",
       "      <td>sophia nelson fo...</td>\n",
       "      <td>sophia nelson fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:24:49 UTC</td>\n",
       "      <td>Here's why the i...</td>\n",
       "      <td>(CNN) The impeac...</td>\n",
       "      <td>polling trumps m...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) The impeac...</td>\n",
       "      <td>impeachment poll...</td>\n",
       "      <td>impeachment inqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:24:36 UTC</td>\n",
       "      <td>Senators grill S...</td>\n",
       "      <td>(CNN) A Senate h...</td>\n",
       "      <td>hale grill state...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Sen. Robert Mene...</td>\n",
       "      <td>senator grill st...</td>\n",
       "      <td>senate hearing r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:53:00 UTC</td>\n",
       "      <td>1 sentence that ...</td>\n",
       "      <td>(CNN) In late Ju...</td>\n",
       "      <td>russians trumps ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Hicks warned tha...</td>\n",
       "      <td>sentence perfect...</td>\n",
       "      <td>late june presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:10:00 UTC</td>\n",
       "      <td>Kamala Harris en...</td>\n",
       "      <td>(CNN) Sen. Kamal...</td>\n",
       "      <td>35 ends caption ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Photos: Former p...</td>\n",
       "      <td>kamala harris en...</td>\n",
       "      <td>sen kamala harri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:09:51 UTC</td>\n",
       "      <td>Rep. Duncan Hunt...</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>misuse misusing ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>rep duncan hunte...</td>\n",
       "      <td>washington repub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:10:38 UTC</td>\n",
       "      <td>Graham says he's...</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>hes dnc russians...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>graham say confi...</td>\n",
       "      <td>washington repub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>15:10:59 UTC</td>\n",
       "      <td>Macron corrects ...</td>\n",
       "      <td>French President...</td>\n",
       "      <td>fighters french ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>French President...</td>\n",
       "      <td>macron corrects ...</td>\n",
       "      <td>french president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:10:14 UTC</td>\n",
       "      <td>Trump says he do...</td>\n",
       "      <td>London (CNN) US ...</td>\n",
       "      <td>know tell donald...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>London (CNN) US ...</td>\n",
       "      <td>trump say know p...</td>\n",
       "      <td>london u preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:43:04 UTC</td>\n",
       "      <td>The key demograp...</td>\n",
       "      <td>One reason the r...</td>\n",
       "      <td>biden whites oba...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Instead, only tw...</td>\n",
       "      <td>key demographic ...</td>\n",
       "      <td>one reason race ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:34:54 UTC</td>\n",
       "      <td>Appeals court sa...</td>\n",
       "      <td>New York (CNN) T...</td>\n",
       "      <td>trumps tax turn ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>New York (CNN) T...</td>\n",
       "      <td>appeal court say...</td>\n",
       "      <td>new york two ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:48:05 UTC</td>\n",
       "      <td>Macron refuses t...</td>\n",
       "      <td>London (CNN) Fre...</td>\n",
       "      <td>turkey leaders c...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>London (CNN) Fre...</td>\n",
       "      <td>macron refuse ba...</td>\n",
       "      <td>london french pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:14:28 UTC</td>\n",
       "      <td>A Pennsylvania m...</td>\n",
       "      <td>(CNN) When first...</td>\n",
       "      <td>witness pennsylv...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Snyder told inve...</td>\n",
       "      <td>pennsylvania mot...</td>\n",
       "      <td>first responder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>08:26:23 UTC</td>\n",
       "      <td>Keeping up with ...</td>\n",
       "      <td>Hong Kong (CNN) ...</td>\n",
       "      <td>navys navy power...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Hong Kong (CNN) ...</td>\n",
       "      <td>keeping china u ...</td>\n",
       "      <td>hong kong u navy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:27:27 UTC</td>\n",
       "      <td>Prince Andrew ac...</td>\n",
       "      <td>London (CNN) An ...</td>\n",
       "      <td>jeffrey virginia...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Virginia Giuffre...</td>\n",
       "      <td>prince andrew ac...</td>\n",
       "      <td>london american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>13:27:25 UTC</td>\n",
       "      <td>FBI offers $5 mi...</td>\n",
       "      <td>(CNN) The Federa...</td>\n",
       "      <td>mostafa terroris...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) The Federa...</td>\n",
       "      <td>fbi offer millio...</td>\n",
       "      <td>federal bureau i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>05:03:22 UTC</td>\n",
       "      <td>Don Lemon: Barr ...</td>\n",
       "      <td>CNN's Don Lemon ...</td>\n",
       "      <td>initiate don lem...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>CNN's Don Lemon ...</td>\n",
       "      <td>lemon barr seems...</td>\n",
       "      <td>lemon responds w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:42:20 UTC</td>\n",
       "      <td>T-Mobile has lau...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>highband means n...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>tmobile launched...</td>\n",
       "      <td>new york busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:06:18 UTC</td>\n",
       "      <td>Yankees GM: I sl...</td>\n",
       "      <td>Brian Cashman is...</td>\n",
       "      <td>cold covenant ou...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>I've slept more ...</td>\n",
       "      <td>yankee gm slept ...</td>\n",
       "      <td>brian cashman se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:01:31 UTC</td>\n",
       "      <td>Eddie Van Halen'...</td>\n",
       "      <td>(CNN) Don't sham...</td>\n",
       "      <td>plays recently b...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Don't sham...</td>\n",
       "      <td>eddie van halen ...</td>\n",
       "      <td>shame people kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:16:05 UTC</td>\n",
       "      <td>Peloton's perple...</td>\n",
       "      <td>(CNN) Peloton, t...</td>\n",
       "      <td>internet fitness...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Peloton, t...</td>\n",
       "      <td>peloton perplexi...</td>\n",
       "      <td>peloton indoor b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:28:11 UTC</td>\n",
       "      <td>Dog starts house...</td>\n",
       "      <td>(CNN) Every now ...</td>\n",
       "      <td>kitchen starts i...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Every now ...</td>\n",
       "      <td>dog start house ...</td>\n",
       "      <td>every incredible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:38:13 UTC</td>\n",
       "      <td>$4.8 million spi...</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>hangs whats face...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>million spinning...</td>\n",
       "      <td>chat u facebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:31:52 UTC</td>\n",
       "      <td>Brad Pitt cries ...</td>\n",
       "      <td>(CNN) Brad Pitt ...</td>\n",
       "      <td>pitt thing brad ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Brad Pitt ...</td>\n",
       "      <td>brad pitt cry used</td>\n",
       "      <td>brad pitt change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>15:20:44 UTC</td>\n",
       "      <td>That smart TV yo...</td>\n",
       "      <td>CNN (CNN) Those ...</td>\n",
       "      <td>internet feature...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>In a pre-holiday...</td>\n",
       "      <td>smart tv bought ...</td>\n",
       "      <td>black friday cyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:31:42 UTC</td>\n",
       "      <td>Watch Marvel's '...</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>widow whats face...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>watch marvel bla...</td>\n",
       "      <td>chat u facebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:57:28 UTC</td>\n",
       "      <td>'Queer Eye' star...</td>\n",
       "      <td>(CNN) Jonathan V...</td>\n",
       "      <td>wears queer make...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Jonathan V...</td>\n",
       "      <td>queer eye star j...</td>\n",
       "      <td>jonathan van nes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:23:10 UTC</td>\n",
       "      <td>Shark knocks 7-y...</td>\n",
       "      <td>A 7-year-old boy...</td>\n",
       "      <td>boy appeared boa...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>A 7-year-old boy...</td>\n",
       "      <td>shark knock year...</td>\n",
       "      <td>yearold boy surf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>06:00:19</td>\n",
       "      <td>Think fake news ...</td>\n",
       "      <td>Senator John Ken...</td>\n",
       "      <td>fake problem isn...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Senator John Ken...</td>\n",
       "      <td>think fake news ...</td>\n",
       "      <td>senator john ken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>08:18:24</td>\n",
       "      <td>UPDATE 2-Ardelyx...</td>\n",
       "      <td>(Adds comments f...</td>\n",
       "      <td>latestage phosph...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>However, 52.5% p...</td>\n",
       "      <td>update ardelyx d...</td>\n",
       "      <td>add comment comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:54:38</td>\n",
       "      <td>2020 market outl...</td>\n",
       "      <td>Pollster: By the...</td>\n",
       "      <td>happens outlook ...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Pollster: By the...</td>\n",
       "      <td>market outlook r...</td>\n",
       "      <td>pollster time ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>10:25:32</td>\n",
       "      <td>CN Cuts Earnings...</td>\n",
       "      <td>Canadian Nationa...</td>\n",
       "      <td>canadian outlook...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Canadian Nationa...</td>\n",
       "      <td>cn cut earnings ...</td>\n",
       "      <td>canadian nationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:11:11</td>\n",
       "      <td>U.S. Supreme Cou...</td>\n",
       "      <td>By Andrew Chung\\...</td>\n",
       "      <td>richfield landow...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>The case hinges ...</td>\n",
       "      <td>u supreme court ...</td>\n",
       "      <td>andrew chung was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>11:26:38</td>\n",
       "      <td>Peloton under fi...</td>\n",
       "      <td>Peloton faced ba...</td>\n",
       "      <td>ad gamm peloton ...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Peloton faced ba...</td>\n",
       "      <td>peloton fire vir...</td>\n",
       "      <td>peloton faced ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:10:36</td>\n",
       "      <td>Wall Street Ride...</td>\n",
       "      <td>Wait hold on.\\n\\...</td>\n",
       "      <td>rides wait ruben...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Wait hold on.\\nT...</td>\n",
       "      <td>wall street ride...</td>\n",
       "      <td>wait hold on pel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:25:35</td>\n",
       "      <td>Chile eyes state...</td>\n",
       "      <td>By Fabian Camber...</td>\n",
       "      <td>flats lithium ch...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Chile has the wo...</td>\n",
       "      <td>chile eye stateb...</td>\n",
       "      <td>fabian cambero s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>13:53:06</td>\n",
       "      <td>Xiaomi, Oppo to ...</td>\n",
       "      <td>By Stephen Nelli...</td>\n",
       "      <td>networks phone x...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>By Stephen Nelli...</td>\n",
       "      <td>xiaomi oppo use ...</td>\n",
       "      <td>stephen nellis r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>13:05:25</td>\n",
       "      <td>My Family &amp; I Do...</td>\n",
       "      <td>This holiday sea...</td>\n",
       "      <td>fish seven dont ...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>My husband and I...</td>\n",
       "      <td>family dont holi...</td>\n",
       "      <td>holiday season r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>09:07:07</td>\n",
       "      <td>Repsol Announces...</td>\n",
       "      <td>As the annual U....</td>\n",
       "      <td>emissions compan...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Climate Conferen...</td>\n",
       "      <td>repsol announces...</td>\n",
       "      <td>annual un climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:21:11</td>\n",
       "      <td>Trump uses flame...</td>\n",
       "      <td>And today’s inve...</td>\n",
       "      <td>sp uses trade va...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>For starters, Tr...</td>\n",
       "      <td>trump us flameth...</td>\n",
       "      <td>today lesson com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:52:57</td>\n",
       "      <td>Amazon announces...</td>\n",
       "      <td>Yahoo Finance’s ...</td>\n",
       "      <td>cloud julie disc...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Yahoo Finance’s ...</td>\n",
       "      <td>amazon announces...</td>\n",
       "      <td>yahoo finance ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:02:13</td>\n",
       "      <td>Were Hedge Funds...</td>\n",
       "      <td>With the first-q...</td>\n",
       "      <td>piling stock sto...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Video: Click the...</td>\n",
       "      <td>hedge fund right...</td>\n",
       "      <td>firstquarter rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>05:30:00</td>\n",
       "      <td>Nintendo Faces F...</td>\n",
       "      <td>(Bloomberg) -- N...</td>\n",
       "      <td>nintendos famili...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>(Bloomberg) -- N...</td>\n",
       "      <td>nintendo face fa...</td>\n",
       "      <td>bloomberg ninten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>06:34:34</td>\n",
       "      <td>Pharmacy chains ...</td>\n",
       "      <td>Dec 3 (Reuters) ...</td>\n",
       "      <td>pharmacy health ...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Dec 3 (Reuters) ...</td>\n",
       "      <td>pharmacy chain s...</td>\n",
       "      <td>dec reuters thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>13:56:49</td>\n",
       "      <td>Hedge Funds Have...</td>\n",
       "      <td>Hedge funds are ...</td>\n",
       "      <td>stock stocks pos...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Facebook, a stoc...</td>\n",
       "      <td>hedge fund never...</td>\n",
       "      <td>hedge fund perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>09:37:54</td>\n",
       "      <td>Hedge Funds Are ...</td>\n",
       "      <td>\"The global econ...</td>\n",
       "      <td>holdings limited...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>So let’s take a ...</td>\n",
       "      <td>hedge fund buyin...</td>\n",
       "      <td>the global econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>10:23:52</td>\n",
       "      <td>Norfolk Southern...</td>\n",
       "      <td>Norfolk Southern...</td>\n",
       "      <td>shippers makes n...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Norfolk Southern...</td>\n",
       "      <td>norfolk southern...</td>\n",
       "      <td>norfolk southern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>11:42:51</td>\n",
       "      <td>Charlie Munger: ...</td>\n",
       "      <td>In 1996, Charlie...</td>\n",
       "      <td>trillion problem...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>In 1996, Charlie...</td>\n",
       "      <td>charlie munger c...</td>\n",
       "      <td>charlie munger t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:28:00</td>\n",
       "      <td>Glencore CEO Hin...</td>\n",
       "      <td>(Bloomberg) -- G...</td>\n",
       "      <td>leave suggested ...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>(Bloomberg) -- G...</td>\n",
       "      <td>glencore ceo hin...</td>\n",
       "      <td>bloomberg glenco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:27:52</td>\n",
       "      <td>Poloz Likely to ...</td>\n",
       "      <td>© Reuters. Poloz...</td>\n",
       "      <td>hold guide outli...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Poloz Likely to ...</td>\n",
       "      <td>poloz likely hol...</td>\n",
       "      <td>reuters poloz li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:26:00</td>\n",
       "      <td>U.S. senators ca...</td>\n",
       "      <td>By M.B. Pell\\n\\n...</td>\n",
       "      <td>beatty private f...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>The top civilian...</td>\n",
       "      <td>u senator call b...</td>\n",
       "      <td>mb pell new york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:22:34</td>\n",
       "      <td>France fights ba...</td>\n",
       "      <td>© Reuters. The l...</td>\n",
       "      <td>champagne compan...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>An investigation...</td>\n",
       "      <td>france fight bac...</td>\n",
       "      <td>reuters logo mob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:22:30</td>\n",
       "      <td>Institutional in...</td>\n",
       "      <td>Institutional in...</td>\n",
       "      <td>aramco bank inve...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Institutional in...</td>\n",
       "      <td>institutional in...</td>\n",
       "      <td>institutional in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:17:09</td>\n",
       "      <td>Wall Street’s Bi...</td>\n",
       "      <td>© Reuters. Wall ...</td>\n",
       "      <td>beker equity ban...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Wall Street’s Bi...</td>\n",
       "      <td>wall street bigg...</td>\n",
       "      <td>reuters wall str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:16:47</td>\n",
       "      <td>California congr...</td>\n",
       "      <td>© Reuters. U.S. ...</td>\n",
       "      <td>case pleads corr...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>U.S. Representat...</td>\n",
       "      <td>california congr...</td>\n",
       "      <td>reuters u repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:15:08</td>\n",
       "      <td>Alibaba’s Hong K...</td>\n",
       "      <td>© Reuters. Aliba...</td>\n",
       "      <td>alibabas misconc...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Alibaba’s Hong K...</td>\n",
       "      <td>alibaba hong kon...</td>\n",
       "      <td>reuters alibaba ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:13:22</td>\n",
       "      <td>Mexico resists U...</td>\n",
       "      <td>© Reuters. Mexic...</td>\n",
       "      <td>tweaks mexico la...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Mexico approved ...</td>\n",
       "      <td>mexico resists u...</td>\n",
       "      <td>reuters mexican ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>20:12:31</td>\n",
       "      <td>When Trump Tweet...</td>\n",
       "      <td>(Bloomberg) -- T...</td>\n",
       "      <td>tweets talks can...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>(Bloomberg) -- T...</td>\n",
       "      <td>trump tweet chin...</td>\n",
       "      <td>bloomberg questi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                 link published_date published_time  \\\n",
       "0             cnn  http://rss.cnn.c...     2019-12-03   20:07:56 UTC   \n",
       "1             cnn  http://rss.cnn.c...     2019-12-03   19:26:30 UTC   \n",
       "2             cnn  http://rss.cnn.c...     2019-12-03   19:54:11 UTC   \n",
       "3             cnn  http://rss.cnn.c...     2019-12-03   17:24:49 UTC   \n",
       "4             cnn  http://rss.cnn.c...     2019-12-03   19:24:36 UTC   \n",
       "5             cnn  http://rss.cnn.c...     2019-12-03   19:53:00 UTC   \n",
       "6             cnn  http://rss.cnn.c...     2019-12-03   19:10:00 UTC   \n",
       "7             cnn  http://rss.cnn.c...     2019-12-03   19:09:51 UTC   \n",
       "8             cnn  http://rss.cnn.c...     2019-12-03   20:10:38 UTC   \n",
       "9             cnn  http://rss.cnn.c...     2019-12-03   15:10:59 UTC   \n",
       "10            cnn  http://rss.cnn.c...     2019-12-03   20:10:14 UTC   \n",
       "11            cnn  http://rss.cnn.c...     2019-12-03   19:43:04 UTC   \n",
       "12            cnn  http://rss.cnn.c...     2019-12-03   18:34:54 UTC   \n",
       "13            cnn  http://rss.cnn.c...     2019-12-03   16:48:05 UTC   \n",
       "14            cnn  http://rss.cnn.c...     2019-12-03   17:14:28 UTC   \n",
       "15            cnn  http://rss.cnn.c...     2019-12-03   08:26:23 UTC   \n",
       "16            cnn  http://rss.cnn.c...     2019-12-03   12:27:27 UTC   \n",
       "17            cnn  http://rss.cnn.c...     2019-12-03   13:27:25 UTC   \n",
       "18            cnn  http://rss.cnn.c...     2019-12-03   05:03:22 UTC   \n",
       "19            cnn  http://rss.cnn.c...     2019-12-03   19:42:20 UTC   \n",
       "20            cnn  http://rss.cnn.c...     2019-12-03   20:06:18 UTC   \n",
       "21            cnn  http://rss.cnn.c...     2019-12-03   16:01:31 UTC   \n",
       "22            cnn  http://rss.cnn.c...     2019-12-03   18:16:05 UTC   \n",
       "23            cnn  http://rss.cnn.c...     2019-12-03   17:28:11 UTC   \n",
       "24            cnn  http://rss.cnn.c...     2019-12-03   18:38:13 UTC   \n",
       "25            cnn  http://rss.cnn.c...     2019-12-03   19:31:52 UTC   \n",
       "26            cnn  http://rss.cnn.c...     2019-12-03   15:20:44 UTC   \n",
       "27            cnn  http://rss.cnn.c...     2019-12-03   17:31:42 UTC   \n",
       "28            cnn  http://rss.cnn.c...     2019-12-03   14:57:28 UTC   \n",
       "29            cnn  http://rss.cnn.c...     2019-12-03   14:23:10 UTC   \n",
       "..            ...                  ...            ...            ...   \n",
       "406     yahoonews  https://news.yah...     2019-12-03      06:00:19    \n",
       "407  yahoofinance  https://finance....     2019-12-03      08:18:24    \n",
       "408  yahoofinance  https://finance....     2019-12-03      14:54:38    \n",
       "409  yahoofinance  https://finance....     2019-12-03      10:25:32    \n",
       "410  yahoofinance  https://finance....     2019-12-03      14:11:11    \n",
       "411  yahoofinance  https://finance....     2019-12-03      11:26:38    \n",
       "412  yahoofinance  https://finance....     2019-12-03      12:10:36    \n",
       "413  yahoofinance  https://finance....     2019-12-03      14:25:35    \n",
       "414  yahoofinance  https://finance....     2019-12-03      13:53:06    \n",
       "415  yahoofinance  https://finance....     2019-12-03      13:05:25    \n",
       "416  yahoofinance  https://finance....     2019-12-03      09:07:07    \n",
       "417  yahoofinance  https://finance....     2019-12-03      12:21:11    \n",
       "418  yahoofinance  https://finance....     2019-12-03      12:52:57    \n",
       "419  yahoofinance  https://finance....     2019-12-03      14:02:13    \n",
       "420  yahoofinance  https://finance....     2019-12-03      05:30:00    \n",
       "421  yahoofinance  https://finance....     2019-12-03      06:34:34    \n",
       "422  yahoofinance  https://finance....     2019-12-03      13:56:49    \n",
       "423  yahoofinance  https://finance....     2019-12-03      09:37:54    \n",
       "424  yahoofinance  https://finance....     2019-12-03      10:23:52    \n",
       "425  yahoofinance  https://finance....     2019-12-03      11:42:51    \n",
       "426     investing  https://www.inve...     2019-12-03      20:28:00    \n",
       "427     investing  https://www.inve...     2019-12-03      20:27:52    \n",
       "428     investing  https://www.inve...     2019-12-03      20:26:00    \n",
       "429     investing  https://www.inve...     2019-12-03      20:22:34    \n",
       "430     investing  https://www.inve...     2019-12-03      20:22:30    \n",
       "431     investing  https://www.inve...     2019-12-03      20:17:09    \n",
       "432     investing  https://www.inve...     2019-12-03      20:16:47    \n",
       "433     investing  https://www.inve...     2019-12-03      20:15:08    \n",
       "434     investing  https://www.inve...     2019-12-03      20:13:22    \n",
       "435     investing  https://www.inve...     2019-12-03      20:12:31    \n",
       "\n",
       "                   title                 text             keywords  \\\n",
       "0    Impeachment repo...  (CNN) House Demo...  vote misconduct ...   \n",
       "1    Read: Democrats'...  Chat with us in ...  whats facebook r...   \n",
       "2    Sophia Nelson: A...  Sophia Nelson, f...  nelson embarrass...   \n",
       "3    Here's why the i...  (CNN) The impeac...  polling trumps m...   \n",
       "4    Senators grill S...  (CNN) A Senate h...  hale grill state...   \n",
       "5    1 sentence that ...  (CNN) In late Ju...  russians trumps ...   \n",
       "6    Kamala Harris en...  (CNN) Sen. Kamal...  35 ends caption ...   \n",
       "7    Rep. Duncan Hunt...  Washington (CNN)...  misuse misusing ...   \n",
       "8    Graham says he's...  Washington (CNN)...  hes dnc russians...   \n",
       "9    Macron corrects ...  French President...  fighters french ...   \n",
       "10   Trump says he do...  London (CNN) US ...  know tell donald...   \n",
       "11   The key demograp...  One reason the r...  biden whites oba...   \n",
       "12   Appeals court sa...  New York (CNN) T...  trumps tax turn ...   \n",
       "13   Macron refuses t...  London (CNN) Fre...  turkey leaders c...   \n",
       "14   A Pennsylvania m...  (CNN) When first...  witness pennsylv...   \n",
       "15   Keeping up with ...  Hong Kong (CNN) ...  navys navy power...   \n",
       "16   Prince Andrew ac...  London (CNN) An ...  jeffrey virginia...   \n",
       "17   FBI offers $5 mi...  (CNN) The Federa...  mostafa terroris...   \n",
       "18   Don Lemon: Barr ...  CNN's Don Lemon ...  initiate don lem...   \n",
       "19   T-Mobile has lau...  New York (CNN Bu...  highband means n...   \n",
       "20   Yankees GM: I sl...  Brian Cashman is...  cold covenant ou...   \n",
       "21   Eddie Van Halen'...  (CNN) Don't sham...  plays recently b...   \n",
       "22   Peloton's perple...  (CNN) Peloton, t...  internet fitness...   \n",
       "23   Dog starts house...  (CNN) Every now ...  kitchen starts i...   \n",
       "24   $4.8 million spi...  Chat with us in ...  hangs whats face...   \n",
       "25   Brad Pitt cries ...  (CNN) Brad Pitt ...  pitt thing brad ...   \n",
       "26   That smart TV yo...  CNN (CNN) Those ...  internet feature...   \n",
       "27   Watch Marvel's '...  Chat with us in ...  widow whats face...   \n",
       "28   'Queer Eye' star...  (CNN) Jonathan V...  wears queer make...   \n",
       "29   Shark knocks 7-y...  A 7-year-old boy...  boy appeared boa...   \n",
       "..                   ...                  ...                  ...   \n",
       "406  Think fake news ...  Senator John Ken...  fake problem isn...   \n",
       "407  UPDATE 2-Ardelyx...  (Adds comments f...  latestage phosph...   \n",
       "408  2020 market outl...  Pollster: By the...  happens outlook ...   \n",
       "409  CN Cuts Earnings...  Canadian Nationa...  canadian outlook...   \n",
       "410  U.S. Supreme Cou...  By Andrew Chung\\...  richfield landow...   \n",
       "411  Peloton under fi...  Peloton faced ba...  ad gamm peloton ...   \n",
       "412  Wall Street Ride...  Wait hold on.\\n\\...  rides wait ruben...   \n",
       "413  Chile eyes state...  By Fabian Camber...  flats lithium ch...   \n",
       "414  Xiaomi, Oppo to ...  By Stephen Nelli...  networks phone x...   \n",
       "415  My Family & I Do...  This holiday sea...  fish seven dont ...   \n",
       "416  Repsol Announces...  As the annual U....  emissions compan...   \n",
       "417  Trump uses flame...  And today’s inve...  sp uses trade va...   \n",
       "418  Amazon announces...  Yahoo Finance’s ...  cloud julie disc...   \n",
       "419  Were Hedge Funds...  With the first-q...  piling stock sto...   \n",
       "420  Nintendo Faces F...  (Bloomberg) -- N...  nintendos famili...   \n",
       "421  Pharmacy chains ...  Dec 3 (Reuters) ...  pharmacy health ...   \n",
       "422  Hedge Funds Have...  Hedge funds are ...  stock stocks pos...   \n",
       "423  Hedge Funds Are ...  \"The global econ...  holdings limited...   \n",
       "424  Norfolk Southern...  Norfolk Southern...  shippers makes n...   \n",
       "425  Charlie Munger: ...  In 1996, Charlie...  trillion problem...   \n",
       "426  Glencore CEO Hin...  (Bloomberg) -- G...  leave suggested ...   \n",
       "427  Poloz Likely to ...  © Reuters. Poloz...  hold guide outli...   \n",
       "428  U.S. senators ca...  By M.B. Pell\\n\\n...  beatty private f...   \n",
       "429  France fights ba...  © Reuters. The l...  champagne compan...   \n",
       "430  Institutional in...  Institutional in...  aramco bank inve...   \n",
       "431  Wall Street’s Bi...  © Reuters. Wall ...  beker equity ban...   \n",
       "432  California congr...  © Reuters. U.S. ...  case pleads corr...   \n",
       "433  Alibaba’s Hong K...  © Reuters. Aliba...  alibabas misconc...   \n",
       "434  Mexico resists U...  © Reuters. Mexic...  tweaks mexico la...   \n",
       "435  When Trump Tweet...  (Bloomberg) -- T...  tweets talks can...   \n",
       "\n",
       "                   image              summary          clean_title  \\\n",
       "0    https://cdn.cnn....  (CNN) House Demo...  impeachment repo...   \n",
       "1    https://cdn.cnn....  Chat with us in ...  read democrat tr...   \n",
       "2    https://cdn.cnn....  Sophia Nelson, f...  sophia nelson fo...   \n",
       "3    https://cdn.cnn....  (CNN) The impeac...  impeachment poll...   \n",
       "4    https://cdn.cnn....  Sen. Robert Mene...  senator grill st...   \n",
       "5    https://cdn.cnn....  Hicks warned tha...  sentence perfect...   \n",
       "6    https://cdn.cnn....  Photos: Former p...  kamala harris en...   \n",
       "7    https://cdn.cnn....  Washington (CNN)...  rep duncan hunte...   \n",
       "8    https://cdn.cnn....  Washington (CNN)...  graham say confi...   \n",
       "9    https://cdn.cnn....  French President...  macron corrects ...   \n",
       "10   https://cdn.cnn....  London (CNN) US ...  trump say know p...   \n",
       "11   https://cdn.cnn....  Instead, only tw...  key demographic ...   \n",
       "12   https://cdn.cnn....  New York (CNN) T...  appeal court say...   \n",
       "13   https://cdn.cnn....  London (CNN) Fre...  macron refuse ba...   \n",
       "14   https://cdn.cnn....  Snyder told inve...  pennsylvania mot...   \n",
       "15   https://cdn.cnn....  Hong Kong (CNN) ...  keeping china u ...   \n",
       "16   https://cdn.cnn....  Virginia Giuffre...  prince andrew ac...   \n",
       "17   https://cdn.cnn....  (CNN) The Federa...  fbi offer millio...   \n",
       "18   https://cdn.cnn....  CNN's Don Lemon ...  lemon barr seems...   \n",
       "19   https://cdn.cnn....  New York (CNN Bu...  tmobile launched...   \n",
       "20   https://cdn.cnn....  I've slept more ...  yankee gm slept ...   \n",
       "21   https://cdn.cnn....  (CNN) Don't sham...  eddie van halen ...   \n",
       "22   https://cdn.cnn....  (CNN) Peloton, t...  peloton perplexi...   \n",
       "23   https://cdn.cnn....  (CNN) Every now ...  dog start house ...   \n",
       "24   https://cdn.cnn....  Chat with us in ...  million spinning...   \n",
       "25   https://cdn.cnn....  (CNN) Brad Pitt ...   brad pitt cry used   \n",
       "26   https://cdn.cnn....  In a pre-holiday...  smart tv bought ...   \n",
       "27   https://cdn.cnn....  Chat with us in ...  watch marvel bla...   \n",
       "28   https://cdn.cnn....  (CNN) Jonathan V...  queer eye star j...   \n",
       "29   https://cdn.cnn....  A 7-year-old boy...  shark knock year...   \n",
       "..                   ...                  ...                  ...   \n",
       "406  https://s.yimg.c...  Senator John Ken...  think fake news ...   \n",
       "407  https://s.yimg.c...  However, 52.5% p...  update ardelyx d...   \n",
       "408  https://s.yimg.c...  Pollster: By the...  market outlook r...   \n",
       "409  https://s.yimg.c...  Canadian Nationa...  cn cut earnings ...   \n",
       "410  https://s.yimg.c...  The case hinges ...  u supreme court ...   \n",
       "411  https://s.yimg.c...  Peloton faced ba...  peloton fire vir...   \n",
       "412  https://s.yimg.c...  Wait hold on.\\nT...  wall street ride...   \n",
       "413  https://s.yimg.c...  Chile has the wo...  chile eye stateb...   \n",
       "414  https://s.yimg.c...  By Stephen Nelli...  xiaomi oppo use ...   \n",
       "415  https://s.yimg.c...  My husband and I...  family dont holi...   \n",
       "416  https://s.yimg.c...  Climate Conferen...  repsol announces...   \n",
       "417  https://s.yimg.c...  For starters, Tr...  trump us flameth...   \n",
       "418  https://s.yimg.c...  Yahoo Finance’s ...  amazon announces...   \n",
       "419  https://s.yimg.c...  Video: Click the...  hedge fund right...   \n",
       "420  https://s.yimg.c...  (Bloomberg) -- N...  nintendo face fa...   \n",
       "421  https://s.yimg.c...  Dec 3 (Reuters) ...  pharmacy chain s...   \n",
       "422  https://s.yimg.c...  Facebook, a stoc...  hedge fund never...   \n",
       "423  https://s.yimg.c...  So let’s take a ...  hedge fund buyin...   \n",
       "424  https://s.yimg.c...  Norfolk Southern...  norfolk southern...   \n",
       "425  https://s.yimg.c...  In 1996, Charlie...  charlie munger c...   \n",
       "426  https://i-invdn-...  (Bloomberg) -- G...  glencore ceo hin...   \n",
       "427  https://i-invdn-...  Poloz Likely to ...  poloz likely hol...   \n",
       "428  https://i-invdn-...  The top civilian...  u senator call b...   \n",
       "429  https://i-invdn-...  An investigation...  france fight bac...   \n",
       "430  https://i-invdn-...  Institutional in...  institutional in...   \n",
       "431  https://i-invdn-...  Wall Street’s Bi...  wall street bigg...   \n",
       "432  https://i-invdn-...  U.S. Representat...  california congr...   \n",
       "433  https://i-invdn-...  Alibaba’s Hong K...  alibaba hong kon...   \n",
       "434  https://i-invdn-...  Mexico approved ...  mexico resists u...   \n",
       "435  https://i-invdn-...  (Bloomberg) -- T...  trump tweet chin...   \n",
       "\n",
       "              clean_text  \n",
       "0    house democrat s...  \n",
       "1    chat u facebook ...  \n",
       "2    sophia nelson fo...  \n",
       "3    impeachment inqu...  \n",
       "4    senate hearing r...  \n",
       "5    late june presid...  \n",
       "6    sen kamala harri...  \n",
       "7    washington repub...  \n",
       "8    washington repub...  \n",
       "9    french president...  \n",
       "10   london u preside...  \n",
       "11   one reason race ...  \n",
       "12   new york two ban...  \n",
       "13   london french pr...  \n",
       "14   first responder ...  \n",
       "15   hong kong u navy...  \n",
       "16   london american ...  \n",
       "17   federal bureau i...  \n",
       "18   lemon responds w...  \n",
       "19   new york busines...  \n",
       "20   brian cashman se...  \n",
       "21   shame people kno...  \n",
       "22   peloton indoor b...  \n",
       "23   every incredible...  \n",
       "24   chat u facebook ...  \n",
       "25   brad pitt change...  \n",
       "26   black friday cyb...  \n",
       "27   chat u facebook ...  \n",
       "28   jonathan van nes...  \n",
       "29   yearold boy surf...  \n",
       "..                   ...  \n",
       "406  senator john ken...  \n",
       "407  add comment comp...  \n",
       "408  pollster time ne...  \n",
       "409  canadian nationa...  \n",
       "410  andrew chung was...  \n",
       "411  peloton faced ba...  \n",
       "412  wait hold on pel...  \n",
       "413  fabian cambero s...  \n",
       "414  stephen nellis r...  \n",
       "415  holiday season r...  \n",
       "416  annual un climat...  \n",
       "417  today lesson com...  \n",
       "418  yahoo finance ju...  \n",
       "419  firstquarter rou...  \n",
       "420  bloomberg ninten...  \n",
       "421  dec reuters thre...  \n",
       "422  hedge fund perfe...  \n",
       "423  the global econo...  \n",
       "424  norfolk southern...  \n",
       "425  charlie munger t...  \n",
       "426  bloomberg glenco...  \n",
       "427  reuters poloz li...  \n",
       "428  mb pell new york...  \n",
       "429  reuters logo mob...  \n",
       "430  institutional in...  \n",
       "431  reuters wall str...  \n",
       "432  reuters u repres...  \n",
       "433  reuters alibaba ...  \n",
       "434  reuters mexican ...  \n",
       "435  bloomberg questi...  \n",
       "\n",
       "[433 rows x 11 columns]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Reset the DF index\n",
    "#news_df_daily = news_df_daily[news_df_daily.title != \"\"]\n",
    "#news_df_daily = news_df.reset_index(drop=True)\n",
    "\n",
    "# 2 Removing missing titles i.e. articles extracted without titles or texts\n",
    "\n",
    "news_df_daily = news_df[news_df.title != \"\"]\n",
    "news_df_daily = news_df_daily[news_df_daily.text != \"\"]\n",
    "news_df_daily = news_df_daily.reset_index(drop=True)\n",
    "\n",
    "# 3 Make all letters lower case\n",
    "news_df_daily[\"clean_title\"] = news_df_daily[\"title\"].str.lower()\n",
    "news_df_daily[\"clean_text\"] = news_df_daily[\"text\"].str.lower()\n",
    "\n",
    "# 4 Filter out the stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "news_df_daily[\"clean_title\"] = ((news_df_daily[\"clean_title\"].str.replace(\"'s\",'')).str.replace(\"’s\",''))\n",
    "news_df_daily[\"clean_text\"] = ((news_df_daily[\"clean_text\"].str.replace(\"'s\",'')).str.replace(\"’s\",''))\n",
    "\n",
    "news_df_daily['clean_title'] = news_df_daily['clean_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "news_df_daily['clean_text'] = news_df_daily['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "\n",
    "# 5 Remove sources, punctuation ('[^\\w\\s]','') and numbers ('\\d+', '')\n",
    "sources_list = (list(source_dict.values()))\n",
    "for i in sources_list:\n",
    "    sources_set = set(i)\n",
    "sources_to_replace = dict.fromkeys(sources_set, \"\") # replace every source with \"\" nothing\n",
    "\n",
    "news_df_daily[\"clean_title\"] = (((news_df_daily[\"clean_title\"].str.replace('[^\\w\\s]',''))\n",
    "                                .str.replace('\\d+', '')).replace(sources_to_replace, regex=True))\n",
    "\n",
    "\n",
    "news_df_daily[\"clean_text\"] = (((news_df_daily[\"clean_text\"].str.replace('[^\\w\\s]',''))\n",
    "                                .str.replace('\\d+', ''))\n",
    "                               .replace(sources_to_replace, regex=True))\n",
    "\n",
    "news_df_daily = news_df_daily[~news_df_daily[\"clean_title\"].str.contains(\"washington post\")] \n",
    "# remove washington post articles, since these cannot bet scraped\n",
    "# TODO if test is shorter than... filter out #####################################\n",
    "\n",
    "# 6 Remove non-ascii characters\n",
    "news_df_daily[\"clean_title\"] = news_df_daily[\"clean_title\"].apply(unidecode)\n",
    "news_df_daily[\"clean_text\"] = news_df_daily[\"clean_text\"].apply(unidecode)\n",
    "\n",
    "# 7 Lemmatize words\n",
    "w_tokenizer, lemmatizer = nltk.tokenize.WhitespaceTokenizer() , nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "news_df_daily[\"clean_title\"] = (news_df_daily[\"clean_title\"].apply(lemmatize_text).apply(lambda x: ' '.join([word for word in x])))\n",
    "\n",
    "news_df_daily[\"clean_text\"] = (news_df_daily[\"clean_text\"].apply(lemmatize_text).apply(lambda x: ' '.join([word for word in x])))\n",
    "\n",
    "news_df_daily[\"keywords\"] = news_df_daily[\"keywords\"].apply(lambda x: ' '.join([word for word in x]))\n",
    "\n",
    "######\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 20)\n",
    "\n",
    "news_df_daily\n",
    "##################################################\n",
    "\n",
    "#(news_df_daily[\"title\"] == \"\")\n",
    "\n",
    "#for i in news_df_daily[\"source\"].isna():\n",
    "#    if i == False:\n",
    "#        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing the dataset\n",
    "\n",
    "In this step, we apply several different analysis methods, in order to define which articles out of those we scraped are **most relevant** for portfolio trading customers and **cover trending financial topics**.\n",
    "\n",
    "### 2.1. Cosine similarity\n",
    "\n",
    "Cosine similarity is a metric for measuring the similarity between two sentences. It creates numbered vectors out of sentences and measures the **cosine of the angle between them**.\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d\" alt=\"Cosine similarity formula\" title=\"Cosine similarity formula\" />\n",
    "\n",
    "where\n",
    "* A ........... vector A\n",
    "* A • B ..... dot product between vector A and B\n",
    "* | A | ....... length of vector A\n",
    "\n",
    "\n",
    "We apply this measure for both the title and the texts.\n",
    "\n",
    "#### 2.1.A. Cosine similarity: titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.474342</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113961</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1    2         3    4       5    6    7         8    \\\n",
       "0  1.000000  0.474342  0.0  0.204124  0.0  0.0000  0.0  0.0  0.000000   \n",
       "1  0.474342  1.000000  0.0  0.258199  0.0  0.0000  0.0  0.0  0.000000   \n",
       "2  0.000000  0.000000  1.0  0.000000  0.0  0.0000  0.0  0.0  0.000000   \n",
       "3  0.204124  0.258199  0.0  1.000000  0.0  0.0000  0.0  0.0  0.000000   \n",
       "4  0.000000  0.000000  0.0  0.000000  1.0  0.1066  0.0  0.0  0.113961   \n",
       "\n",
       "        9    ...  426  427       428  429  430  431  432  433  434     435  \n",
       "0  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0000  \n",
       "1  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0000  \n",
       "2  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0000  \n",
       "3  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0000  \n",
       "4  0.123091  ...  0.0  0.0  0.113961  0.0  0.0  0.0  0.0  0.0  0.0  0.1066  \n",
       "\n",
       "[5 rows x 436 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer #for creating count vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity #cosine similarity calculator\n",
    "\n",
    "# for analysis, we need a list of all the titles\n",
    "clean_titles_list = list(news_df_daily['clean_title'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix_title_sparse = count_vectorizer.fit_transform(clean_titles_list) # creates the count vector in sparse matrix\n",
    "count_matrix_title_np = count_matrix_title_sparse.todense() # creates numpy matrix out from all count vectors\n",
    "count_matrix_title_df = pd.DataFrame(count_matrix_title_np, columns=count_vectorizer.get_feature_names()) # creates df from count vectors\n",
    "\n",
    "# apply consine smilarity on count vector dataframe\n",
    "df_cosim_title = pd.DataFrame(cosine_similarity(count_matrix_title_df, count_matrix_title_df))\n",
    "df_cosim_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.B. Cosine similarity: texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433142</td>\n",
       "      <td>0.271713</td>\n",
       "      <td>0.331837</td>\n",
       "      <td>0.199111</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>0.044967</td>\n",
       "      <td>0.325653</td>\n",
       "      <td>0.167705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062660</td>\n",
       "      <td>0.039603</td>\n",
       "      <td>0.161955</td>\n",
       "      <td>0.135668</td>\n",
       "      <td>0.032341</td>\n",
       "      <td>0.034595</td>\n",
       "      <td>0.127323</td>\n",
       "      <td>0.048405</td>\n",
       "      <td>0.182643</td>\n",
       "      <td>0.095097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030261</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.433142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>0.324736</td>\n",
       "      <td>0.162851</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.055279</td>\n",
       "      <td>0.318335</td>\n",
       "      <td>0.151186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068088</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.129821</td>\n",
       "      <td>0.098280</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.029238</td>\n",
       "      <td>0.124964</td>\n",
       "      <td>0.011974</td>\n",
       "      <td>0.110258</td>\n",
       "      <td>0.064297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.271713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129129</td>\n",
       "      <td>0.195019</td>\n",
       "      <td>0.046392</td>\n",
       "      <td>0.028209</td>\n",
       "      <td>0.162450</td>\n",
       "      <td>0.138873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.019130</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>0.104765</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>0.043767</td>\n",
       "      <td>0.053851</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.093026</td>\n",
       "      <td>0.104997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.331837</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.324736</td>\n",
       "      <td>0.129129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.178202</td>\n",
       "      <td>0.113410</td>\n",
       "      <td>0.089755</td>\n",
       "      <td>0.441692</td>\n",
       "      <td>0.093728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116137</td>\n",
       "      <td>0.067191</td>\n",
       "      <td>0.159966</td>\n",
       "      <td>0.156674</td>\n",
       "      <td>0.041601</td>\n",
       "      <td>0.067901</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>0.046188</td>\n",
       "      <td>0.178807</td>\n",
       "      <td>0.078457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.000000  0.000000  0.433142  0.271713  0.331837  0.199111  0.058192   \n",
       "1  0.000000  1.000000  0.000000  0.000000  0.008435  0.000000  0.002444   \n",
       "2  0.433142  0.000000  1.000000  0.116642  0.324736  0.162851  0.080645   \n",
       "3  0.271713  0.000000  0.116642  1.000000  0.129129  0.195019  0.046392   \n",
       "4  0.331837  0.008435  0.324736  0.129129  1.000000  0.178202  0.113410   \n",
       "\n",
       "        7         8         9    ...       426       427       428       429  \\\n",
       "0  0.044967  0.325653  0.167705  ...  0.062660  0.039603  0.161955  0.135668   \n",
       "1  0.000000  0.000000  0.000000  ...  0.030261  0.013388  0.000000  0.032760   \n",
       "2  0.055279  0.318335  0.151186  ...  0.068088  0.013388  0.129821  0.098280   \n",
       "3  0.028209  0.162450  0.138873  ...  0.043240  0.019130  0.027652  0.104765   \n",
       "4  0.089755  0.441692  0.093728  ...  0.116137  0.067191  0.159966  0.156674   \n",
       "\n",
       "        430       431       432       433       434       435  \n",
       "0  0.032341  0.034595  0.127323  0.048405  0.182643  0.095097  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.021432  \n",
       "2  0.024296  0.029238  0.124964  0.011974  0.110258  0.064297  \n",
       "3  0.024797  0.043767  0.053851  0.027700  0.093026  0.104997  \n",
       "4  0.041601  0.067901  0.140186  0.046188  0.178807  0.078457  \n",
       "\n",
       "[5 rows x 436 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for analysis, we need a list of all the texts\n",
    "clean_texts_list = list(news_df_daily['clean_text'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix_text = count_vectorizer.fit_transform(clean_texts_list) # creates the count vector\n",
    "count_matrix_text = count_matrix_text.todense() # creates numpy matrix out from all count vectors\n",
    "\n",
    "count_matrix_text = pd.DataFrame(count_matrix_text, columns=count_vectorizer.get_feature_names()) # creates df from count vectors\n",
    "\n",
    "# apply consine smilarity on count vector dataframe\n",
    "df_cosim_texts = pd.DataFrame(cosine_similarity(count_matrix_text, count_matrix_text))\n",
    "df_cosim_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Soft cosine similarity measure\n",
    "\n",
    "Metric for measuring the similarity between two sentences, but gives **higher scores for words with similar meaning**. For Example, ‘President’ vs ‘Prime minister’, ‘Food’ vs ‘Dish’, ‘Hi’ vs ‘Hello’ are considered similar. \n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9743aceb346ccb501ceaef15a46570d1ba8a6a1b\" alt=\"Soft cosine formula\" title=\"Soft cosine formula\" />\n",
    "\n",
    "where\n",
    "* sij .... similarity (feature i, feature j)\n",
    "\n",
    "**Difference to cosine similarity**: the traditional cosine similarity considers the vector space model (VSM i.e. features, unique words) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which help generalize the concept of cosine (and soft cosine) as well as the idea of (soft) similarity. https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "This implies that we need some vector defining the similarity between words i.e. vectors of words that are similar. \n",
    "In our case we are going to use the pretrained `fasttext-wiki-news-subwords-300` vector dataset containing 1 million word embeddings trained on Wikipedia 2017. More info here: https://github.com/RaRe-Technologies/gensim-data/releases/tag/fasttext-wiki-news-subwords-300\n",
    "\n",
    "_**Side note:** other pre-trained models to be found here: https://github.com/RaRe-Technologies/gensim-data/releases_\n",
    "\n",
    "**Word embeddings**: position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be used with pre-trained models applying transfer learning.\n",
    "\n",
    "#### 2.2.A. Soft cosine measure: titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ! ### this will download a file to your harddrive ### ! ###\n",
    "\n",
    "# first we need to download the FastText model - about 250MB\n",
    "\n",
    "glove_wiki = api.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('students', 0.7942561507225037),\n",
       " ('teacher', 0.7468630075454712),\n",
       " ('graduate', 0.7329548001289368),\n",
       " ('school', 0.689203143119812),\n",
       " ('university', 0.6683695316314697),\n",
       " ('college', 0.6582204103469849),\n",
       " ('faculty', 0.6569023728370667),\n",
       " ('teachers', 0.6506001949310303),\n",
       " ('undergraduate', 0.6371238231658936),\n",
       " ('academic', 0.623526930809021)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki.most_similar(positive=\"student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary, a map of word to unique id from the title list\n",
    "dictionary_titles = corpora.Dictionary([simple_preprocess(word) for word in clean_titles_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a similarity sparse matrix from the words in the dictionary\n",
    "# this process takes a bit due to calculation time\n",
    "similarity_matrix_titles = glove_wiki.similarity_matrix(dictionary_titles, \n",
    "                                                        tfidf=None, \n",
    "                                                        threshold=0.0, \n",
    "                                                        exponent=2.0, \n",
    "                                                        nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the titles into bag-of-words vectors through function\n",
    "# appends the bag-of-words from all sentences into the sent list\n",
    "def convert_bow(sentences):\n",
    "    global sent_bow\n",
    "    sent_bow = []\n",
    "    for i in sentences:\n",
    "        bow = dictionary_titles.doc2bow(simple_preprocess(i))\n",
    "        sent_bow.append(bow)\n",
    "        \n",
    "convert_bow(clean_titles_list) \n",
    "\n",
    "# create soft cosine measure matrix thourgh function \n",
    "\"\"\" creates a matrix with the results of soft cosine measure calculation.\n",
    "Takes into account the previously created similarity sparse matrix was created from the similar word meanings \n",
    "(we extracted from the FastText model) from the unique words that were in our unique dictionary.\"\"\"\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array) # creates a grid with dimensions (nr of articles x nr of articles)\n",
    "    soft_cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix_titles) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return soft_cossim_mat\n",
    "\n",
    "soft_cossim_mat_titles = create_soft_cossim_matrix(sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9    ...   426  427  \\\n",
       "0  1.00  0.50  0.08  0.22  0.15  0.04  0.03  0.05  0.09  0.08  ...  0.04  0.0   \n",
       "1  0.50  1.00  0.11  0.26  0.20  0.06  0.03  0.00  0.07  0.05  ...  0.05  0.0   \n",
       "2  0.08  0.11  1.00  0.00  0.21  0.00  0.15  0.05  0.08  0.00  ...  0.03  0.0   \n",
       "3  0.22  0.26  0.00  1.00  0.00  0.03  0.09  0.00  0.10  0.00  ...  0.00  0.0   \n",
       "4  0.15  0.20  0.21  0.00  1.00  0.14  0.16  0.06  0.20  0.15  ...  0.00  0.0   \n",
       "\n",
       "    428   429   430   431   432  433   434   435  \n",
       "0  0.09  0.00  0.00  0.00  0.24  0.0  0.08  0.00  \n",
       "1  0.10  0.00  0.00  0.04  0.13  0.0  0.14  0.00  \n",
       "2  0.12  0.00  0.00  0.00  0.09  0.0  0.07  0.00  \n",
       "3  0.00  0.09  0.05  0.00  0.07  0.0  0.13  0.00  \n",
       "4  0.17  0.02  0.00  0.00  0.13  0.0  0.11  0.13  \n",
       "\n",
       "[5 rows x 436 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_cossim_mat_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.B. Soft cosine measure: texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**! Be aware !** \n",
    "\n",
    "When you run the cell below - even when having only around 50 articles - the creation of a unique word dictionary and especially the corresponding similarity matrix for article texts takes at least 2 to 5min. \n",
    "\n",
    "This waiting time cannot be skipped for text soft cosine measure similarity comparison, since it just takes a lot of ressources to compute. If you want to time how long it exacly takes, look below for paragraph _X. Other stuff that could be helpful in the future_ - there is a code for timing the run time of a code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary, a map of word to unique id from the text list\n",
    "dictionary_texts = corpora.Dictionary([simple_preprocess(word) for word in clean_texts_list])\n",
    "\n",
    "# generate a similarity sparse matrix from the words in the dictionary\n",
    "# this process takes a bit due to calculation time\n",
    "similarity_matrix_texts = fasttext_model300.similarity_matrix(dictionary_texts, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the texts into bag-of-words vectors through function\n",
    "# appends the bag-of-words from all sentences into the sent list\n",
    "def convert_bow(sentences):\n",
    "    global sent_bow\n",
    "    sent_bow = []\n",
    "    for i in sentences:\n",
    "        bow = dictionary_texts.doc2bow(simple_preprocess(i))\n",
    "        sent_bow.append(bow)\n",
    "        \n",
    "convert_bow(clean_texts_list) \n",
    "\n",
    "#create soft cosine measure matrix thourgh function \n",
    "\"\"\" creates a matrix with the results of soft cosine measure calculation.\n",
    "Takes into account the previously created similarity sparse matrix was created from the similar word meanings \n",
    "(we extracted from the FastText model) from the unique words that were in our unique dictionary.\"\"\"\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array) # creates a grid with dimensions (nr of articles x nr of articles)\n",
    "    soft_cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix_texts) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return soft_cossim_mat\n",
    "\n",
    "soft_cossim_mat_texts = create_soft_cossim_matrix(sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_cossim_mat_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Eudlidean Distance\n",
    "\n",
    "Euclidean Distance is a measure for computing the distance between two vectors while taking into account the magnitude (length) the vectors have. This is different from Cosine Similarity: cossim only calculates the cosine of the angle between two vectors, which automatically doesn't take the magintude into account. \n",
    "\n",
    "Repeated application of the Pythagorian Theorem gives the formula:\n",
    "\n",
    "<img src= \"https://wikimedia.org/api/rest_v1/media/math/render/svg/4efcba672e6df32cc8eb7ce0863591806a6581b5\">\n",
    "\n",
    "The Euclidean Distance measures the ordinary distance between two points in a space. The points represent the points to which the vectors point. Looking at it in 2 dimensions makes it much more visually appealing and understandable: here $p$ and $q$ represent the points for which the distance is measured:\n",
    "\n",
    "<img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Euclidean_distance_2d.svg/1280px-Euclidean_distance_2d.svg.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.74</td>\n",
       "      <td>...</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.32</td>\n",
       "      <td>...</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.74</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.46</td>\n",
       "      <td>...</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.36</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.87</td>\n",
       "      <td>...</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.47</td>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9    ...   426  \\\n",
       "0  0.00  2.65  3.74  3.00  4.36  4.00  3.61  4.00  3.87  3.74  ...  3.87   \n",
       "1  2.65  0.00  3.32  2.45  4.00  3.61  3.16  3.61  3.46  3.32  ...  3.46   \n",
       "2  3.74  3.32  0.00  3.00  4.12  3.74  3.32  3.74  3.61  3.46  ...  3.61   \n",
       "3  3.00  2.45  3.00  0.00  3.74  3.32  2.83  3.32  3.16  3.00  ...  3.16   \n",
       "4  4.36  4.00  4.12  3.74  0.00  4.12  4.00  4.36  4.00  3.87  ...  4.24   \n",
       "\n",
       "    427   428   429   430   431   432   433   434   435  \n",
       "0  4.00  3.87  3.87  4.24  4.00  3.87  3.87  4.12  4.00  \n",
       "1  3.61  3.46  3.46  3.87  3.61  3.46  3.46  3.74  3.61  \n",
       "2  3.74  3.61  3.61  4.00  3.74  3.61  3.61  3.87  3.74  \n",
       "3  3.32  3.16  3.16  3.61  3.32  3.16  3.16  3.46  3.32  \n",
       "4  4.36  4.00  4.24  4.58  4.36  4.24  4.24  4.47  4.12  \n",
       "\n",
       "[5 rows x 436 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "eucl_dist = pairwise_distances(count_matrix_title_sparse, metric='euclidean')\n",
    "eucl_dist_df = pd.DataFrame(eucl_dist)\n",
    "eucl_dist_df = eucl_dist_df.round(2)\n",
    "\n",
    "eucl_dist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results: extracting most similar articles\n",
    "\n",
    "After finding some results for the similarity in our scraped articles, we have to **filter the similar articles out of our initial** `news_df_daily` **dataframe**, in order to find out the title and article text.\n",
    "\n",
    "We want to extract only articles that have some predefined minimum value for similarity f.e. we only want **articles that have a similarity of at least 0.7** (this number could vary depending on our choice). Since the row indexes and the column numbers in the `soft_cossim_mat` matrix are equal to the indexes of the articles in our initial `news_df_daily` dataframe, we need to filter `news_df_daily` by exactly these indexes which contain the minimum similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function to find the row and column index in a dataframe for a specific value\n",
    "def get_indexes(dataframe, value):\n",
    "    pos_list = list()\n",
    "    for i in value:\n",
    "        result = dataframe.isin([value]) # crete bool dataframe with True at positions where the given value exists\n",
    "        series = result.any()\n",
    "        column_names = list(series[series == True].index) # create list of columns that contain the value\n",
    "        for col in column_names: # iterate over list of columns and fetch the rows indexes where value exists\n",
    "            rows = list(result[col][result[col] == True].index)\n",
    "            for row in rows:\n",
    "                if row != col: # since matrix diagonal is always == 1, we exclude these results here\n",
    "                    pos_list.append((row, col)) #creates a list of row, col position\n",
    "        return pos_list # Return a list of tuples indicating the positions of value in the dataframe\n",
    "\n",
    "# function for creating a list of the row indexes\n",
    "def find_indexes(dict_pos, index_list):\n",
    "    for key, value in dict_pos.items():\n",
    "    #print(key, ' : ', value) # this prints the similarity values and its corresponding row and col indexes in the df\n",
    "        for num in value:\n",
    "            for firstnum in num:\n",
    "                index_list.append(firstnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Most similar articles: by Soft Cosine Similarity of article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>image</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topsto...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:10:00 UTC</td>\n",
       "      <td>Kamala Harris ends 2020 presidential...</td>\n",
       "      <td>(CNN) Sen. Kamala Harris ended her 2...</td>\n",
       "      <td>35 ends caption presidential photos ...</td>\n",
       "      <td>https://cdn.cnn.com/cnnnext/dam/asse...</td>\n",
       "      <td>Photos: Former presidential candidat...</td>\n",
       "      <td>kamala harris end presidential campaign</td>\n",
       "      <td>sen kamala harris ended presidential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topsto...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:09:51 UTC</td>\n",
       "      <td>Rep. Duncan Hunter pleads guilty for...</td>\n",
       "      <td>Washington (CNN) Republican Rep. Dun...</td>\n",
       "      <td>misuse misusing rep pleads resign hu...</td>\n",
       "      <td>https://cdn.cnn.com/cnnnext/dam/asse...</td>\n",
       "      <td>Washington (CNN) Republican Rep. Dun...</td>\n",
       "      <td>rep duncan hunter pleads guilty misu...</td>\n",
       "      <td>washington republican rep duncan hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topsto...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:31:42 UTC</td>\n",
       "      <td>Watch Marvel's 'Black Widow' first t...</td>\n",
       "      <td>Chat with us in Facebook Messenger. ...</td>\n",
       "      <td>widow whats facebook marvels unfolds...</td>\n",
       "      <td>https://cdn.cnn.com/cnnnext/dam/asse...</td>\n",
       "      <td>Chat with us in Facebook Messenger.\\...</td>\n",
       "      <td>watch marvel black widow first trailer</td>\n",
       "      <td>chat u facebook messenger find happe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>cbn</td>\n",
       "      <td>https://www1.cbn.com/cbnnews/politic...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:34:22 UTC</td>\n",
       "      <td>Kamala Harris to End Democratic Pres...</td>\n",
       "      <td>WASHINGTON (AP) - Democratic Sen. Ka...</td>\n",
       "      <td>end state presidential democratic se...</td>\n",
       "      <td>https://www1.cbn.com/sites/default/f...</td>\n",
       "      <td>WASHINGTON (AP) - Democratic Sen. Ka...</td>\n",
       "      <td>kamala harris end democratic preside...</td>\n",
       "      <td>washington ap democratic sen kamala ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>wsj_business</td>\n",
       "      <td>https://www.wsj.com/articles/sprint-...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>07:18:00</td>\n",
       "      <td>Sprint Overcounted Subsidized Custom...</td>\n",
       "      <td>Sprint Corp. has for years overestim...</td>\n",
       "      <td>federal customers using sprint tens ...</td>\n",
       "      <td>https://images.wsj.net/im-132404/social</td>\n",
       "      <td>Sprint Corp. has for years overestim...</td>\n",
       "      <td>sprint overcounted subsidized custom...</td>\n",
       "      <td>sprint corp year overestimated many ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                     link published_date  \\\n",
       "6            cnn  http://rss.cnn.com/~r/rss/cnn_topsto...     2019-12-03   \n",
       "7            cnn  http://rss.cnn.com/~r/rss/cnn_topsto...     2019-12-03   \n",
       "27           cnn  http://rss.cnn.com/~r/rss/cnn_topsto...     2019-12-03   \n",
       "55           cbn  https://www1.cbn.com/cbnnews/politic...     2019-12-03   \n",
       "66  wsj_business  https://www.wsj.com/articles/sprint-...     2019-12-03   \n",
       "\n",
       "   published_time                                    title  \\\n",
       "6    19:10:00 UTC  Kamala Harris ends 2020 presidential...   \n",
       "7    19:09:51 UTC  Rep. Duncan Hunter pleads guilty for...   \n",
       "27   17:31:42 UTC  Watch Marvel's 'Black Widow' first t...   \n",
       "55   18:34:22 UTC  Kamala Harris to End Democratic Pres...   \n",
       "66      07:18:00   Sprint Overcounted Subsidized Custom...   \n",
       "\n",
       "                                       text  \\\n",
       "6   (CNN) Sen. Kamala Harris ended her 2...   \n",
       "7   Washington (CNN) Republican Rep. Dun...   \n",
       "27  Chat with us in Facebook Messenger. ...   \n",
       "55  WASHINGTON (AP) - Democratic Sen. Ka...   \n",
       "66  Sprint Corp. has for years overestim...   \n",
       "\n",
       "                                   keywords  \\\n",
       "6   35 ends caption presidential photos ...   \n",
       "7   misuse misusing rep pleads resign hu...   \n",
       "27  widow whats facebook marvels unfolds...   \n",
       "55  end state presidential democratic se...   \n",
       "66  federal customers using sprint tens ...   \n",
       "\n",
       "                                      image  \\\n",
       "6   https://cdn.cnn.com/cnnnext/dam/asse...   \n",
       "7   https://cdn.cnn.com/cnnnext/dam/asse...   \n",
       "27  https://cdn.cnn.com/cnnnext/dam/asse...   \n",
       "55  https://www1.cbn.com/sites/default/f...   \n",
       "66  https://images.wsj.net/im-132404/social   \n",
       "\n",
       "                                    summary  \\\n",
       "6   Photos: Former presidential candidat...   \n",
       "7   Washington (CNN) Republican Rep. Dun...   \n",
       "27  Chat with us in Facebook Messenger.\\...   \n",
       "55  WASHINGTON (AP) - Democratic Sen. Ka...   \n",
       "66  Sprint Corp. has for years overestim...   \n",
       "\n",
       "                                clean_title  \\\n",
       "6   kamala harris end presidential campaign   \n",
       "7   rep duncan hunter pleads guilty misu...   \n",
       "27   watch marvel black widow first trailer   \n",
       "55  kamala harris end democratic preside...   \n",
       "66  sprint overcounted subsidized custom...   \n",
       "\n",
       "                                 clean_text  \n",
       "6   sen kamala harris ended presidential...  \n",
       "7   washington republican rep duncan hun...  \n",
       "27  chat u facebook messenger find happe...  \n",
       "55  washington ap democratic sen kamala ...  \n",
       "66  sprint corp year overestimated many ...  "
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing the range of similarity values for which the sentences should be filtered\n",
    "simval = np.arange(0.9, 1.01, 0.01) # choose similarity values between first number and 1.0, by steps of 0.01\n",
    "simval = (np.around(simval, decimals=2)).astype(str)\n",
    "#simval = (simval.astype(str))\n",
    " \n",
    "# use dict comprehension and 'get_indexes' function to get index positions of elements in df with predefined similarity values\n",
    "dict_pos_titles = {elem: get_indexes(soft_cossim_mat_titles, elem) for elem in simval}\n",
    "#dict_pos_texts = {elem: get_indexes(soft_cossim_mat_texts, elem) for elem in simval}\n",
    "\n",
    "# applying the functions for finding the similarity values in the dataframes\n",
    "\n",
    "index_list_titles = []\n",
    "find_indexes(dict_pos_titles, index_list_titles)\n",
    "index_list_titles = list(set(index_list_titles))\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_list_titles, :]).drop_duplicates((\"title\"))).sort_index()\n",
    "print(select_articles.shape)\n",
    "select_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Most similar articles: by Euclidean Similarity of article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.0' '2.24']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cols = list(eucl_dist_df.columns)    \n",
    "#unique_list = []\n",
    "unique_simvals = sorted(list(pd.unique(eucl_dist_df[cols].values.ravel())))\n",
    "unique_simvals.remove(float(0))\n",
    "filter_criteria = round((len(unique_simvals))*0.18)\n",
    "\n",
    "unique_simvals_filtered = (np.array(unique_simvals[3:filter_criteria])).astype(str)\n",
    "\n",
    "print(unique_simvals_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>image</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topsto...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:24:49 UTC</td>\n",
       "      <td>Here's why the impeachment polling i...</td>\n",
       "      <td>(CNN) The impeachment inquiry into P...</td>\n",
       "      <td>polling trumps moving wanted isnt op...</td>\n",
       "      <td>https://cdn.cnn.com/cnnnext/dam/asse...</td>\n",
       "      <td>(CNN) The impeachment inquiry into P...</td>\n",
       "      <td>impeachment polling moving</td>\n",
       "      <td>impeachment inquiry president donald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topsto...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:10:00 UTC</td>\n",
       "      <td>Kamala Harris ends 2020 presidential...</td>\n",
       "      <td>(CNN) Sen. Kamala Harris ended her 2...</td>\n",
       "      <td>35 ends caption presidential photos ...</td>\n",
       "      <td>https://cdn.cnn.com/cnnnext/dam/asse...</td>\n",
       "      <td>Photos: Former presidential candidat...</td>\n",
       "      <td>kamala harris end presidential campaign</td>\n",
       "      <td>sen kamala harris ended presidential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.com/~r/rss/cnn_topsto...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:48:05 UTC</td>\n",
       "      <td>Macron refuses to back down after Tr...</td>\n",
       "      <td>London (CNN) French President Emmanu...</td>\n",
       "      <td>turkey leaders companies meeting ref...</td>\n",
       "      <td>https://cdn.cnn.com/cnnnext/dam/asse...</td>\n",
       "      <td>London (CNN) French President Emmanu...</td>\n",
       "      <td>macron refuse back trump attack</td>\n",
       "      <td>london french president emmanuel mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>cbn</td>\n",
       "      <td>https://www1.cbn.com/cbnnews/politic...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:34:22 UTC</td>\n",
       "      <td>Kamala Harris to End Democratic Pres...</td>\n",
       "      <td>WASHINGTON (AP) - Democratic Sen. Ka...</td>\n",
       "      <td>end state presidential democratic se...</td>\n",
       "      <td>https://www1.cbn.com/sites/default/f...</td>\n",
       "      <td>WASHINGTON (AP) - Democratic Sen. Ka...</td>\n",
       "      <td>kamala harris end democratic preside...</td>\n",
       "      <td>washington ap democratic sen kamala ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cbn</td>\n",
       "      <td>https://www1.cbn.com/cbnnews/us/2019...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>13:24:11 UTC</td>\n",
       "      <td>Washington DC Synagogue Vandalized W...</td>\n",
       "      <td>JERUSALEM, Israel - Swastikas and an...</td>\n",
       "      <td>united means antisemitic states post...</td>\n",
       "      <td>https://www1.cbn.com/sites/default/f...</td>\n",
       "      <td>JERUSALEM, Israel - Swastikas and an...</td>\n",
       "      <td>washington dc synagogue vandalized s...</td>\n",
       "      <td>jerusalem israel swastika antisemiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>wsj_business</td>\n",
       "      <td>https://www.wsj.com/articles/thanksg...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:51:00</td>\n",
       "      <td>Thanksgiving Weekend Shoppers Booste...</td>\n",
       "      <td>American shoppers increased their sp...</td>\n",
       "      <td>stores wavered period signaling boos...</td>\n",
       "      <td>https://images.wsj.net/im-132607/social</td>\n",
       "      <td>American shoppers increased their sp...</td>\n",
       "      <td>thanksgiving weekend shopper boosted...</td>\n",
       "      <td>american shopper increased spending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>wsj_market</td>\n",
       "      <td>https://www.wsj.com/articles/amazon-...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:26:00</td>\n",
       "      <td>Amazon Dots the Landscape</td>\n",
       "      <td>Consumers apparently love having Ama...</td>\n",
       "      <td>ubercheap speaker smart seven sales ...</td>\n",
       "      <td>https://images.wsj.net/im-132609/social</td>\n",
       "      <td>Consumers apparently love having Ama...</td>\n",
       "      <td>amazon dot landscape</td>\n",
       "      <td>consumer apparently love amazoncom m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>theguardian</td>\n",
       "      <td>https://www.theguardian.com/us-news/...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:22:23 UTC</td>\n",
       "      <td>Kamala Harris drops out of Democrati...</td>\n",
       "      <td>California senator had started stron...</td>\n",
       "      <td>iowa state drops candidates debate r...</td>\n",
       "      <td>https://i.guim.co.uk/img/media/349ae...</td>\n",
       "      <td>California senator had started stron...</td>\n",
       "      <td>kamala harris drop democratic presid...</td>\n",
       "      <td>california senator started strongly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>theguardian</td>\n",
       "      <td>https://www.theguardian.com/food/sho...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:36:19 UTC</td>\n",
       "      <td>Why is a 2017 bottle of Irn-Bru sell...</td>\n",
       "      <td>An out-of-date Irn-Bru bottle is bei...</td>\n",
       "      <td>irnbru 2017 recipe version tax origi...</td>\n",
       "      <td>https://i.guim.co.uk/img/media/0d20d...</td>\n",
       "      <td>An out-of-date Irn-Bru bottle is bei...</td>\n",
       "      <td>bottle irnbru selling</td>\n",
       "      <td>outofdate irnbru bottle sold ebay is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>skynews</td>\n",
       "      <td>http://news.sky.com/story/kamala-har...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:29:00 UTC</td>\n",
       "      <td>Kamala Harris drops out of race for ...</td>\n",
       "      <td>Kamala Harris said she will 'do ever...</td>\n",
       "      <td>resources drops regret deep states r...</td>\n",
       "      <td>https://e3.365dm.com/19/12/1600x900/...</td>\n",
       "      <td>Kamala Harris said she will 'do ever...</td>\n",
       "      <td>kamala harris drop race democratic p...</td>\n",
       "      <td>kamala harris said do everything pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>fortune</td>\n",
       "      <td>https://fortune.com/2019/12/03/kamal...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:00:38 UTC</td>\n",
       "      <td>Kamala Harris Drops Out of President...</td>\n",
       "      <td>Why IBM Is Joining the Corporate Cho...</td>\n",
       "      <td>drops calling race carbon presidenti...</td>\n",
       "      <td>https://content.fortune.com/wp-conte...</td>\n",
       "      <td>Why IBM Is Joining the Corporate Cho...</td>\n",
       "      <td>kamala harris drop presidential race</td>\n",
       "      <td>ibm joining corporate chorus calling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>fortune</td>\n",
       "      <td>https://fortune.com/2019/12/03/its-g...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:47:06 UTC</td>\n",
       "      <td>It’s Giving Tuesday and You Have a C...</td>\n",
       "      <td>Kamala Harris Drops Out of President...</td>\n",
       "      <td>drops bloomberg dig race im choice p...</td>\n",
       "      <td>https://content.fortune.com/wp-conte...</td>\n",
       "      <td>Kamala Harris Drops Out of President...</td>\n",
       "      <td>giving tuesday choice make</td>\n",
       "      <td>kamala harris drop presidential race...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>fortune</td>\n",
       "      <td>https://fortune.com/2019/12/03/trump...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>12:06:51 UTC</td>\n",
       "      <td>Trump's New Trade Offensives</td>\n",
       "      <td>Kamala Harris Drops Out of President...</td>\n",
       "      <td>offensives trumps drops bloomberg tr...</td>\n",
       "      <td>https://content.fortune.com/wp-conte...</td>\n",
       "      <td>Kamala Harris Drops Out of President...</td>\n",
       "      <td>trump new trade offensive</td>\n",
       "      <td>kamala harris drop presidential race...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>google news</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/a...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:35:00 UTC</td>\n",
       "      <td>NPR Choice page</td>\n",
       "      <td>By choosing “I agree” below, you agr...</td>\n",
       "      <td>social information npr choice sites ...</td>\n",
       "      <td></td>\n",
       "      <td>By choosing “I agree” below, you agr...</td>\n",
       "      <td>npr choice page</td>\n",
       "      <td>choosing i agree below agree npr sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>google news</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/a...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>17:54:00 UTC</td>\n",
       "      <td>Appeals Court Orders Trump’s Banks t...</td>\n",
       "      <td>President Donald Trump in London on ...</td>\n",
       "      <td>records setting trumps stage subpoen...</td>\n",
       "      <td>https://compote.slate.com/images/861...</td>\n",
       "      <td>The judge refused, so Trump appealed...</td>\n",
       "      <td>appeal court order trump bank turn r...</td>\n",
       "      <td>president donald trump london tuesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>google news</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/a...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:22:18 UTC</td>\n",
       "      <td>Oshkosh West school shooting: Studen...</td>\n",
       "      <td>Wisconsin officer, student wounded a...</td>\n",
       "      <td>shooting lockdown wisconsin officer ...</td>\n",
       "      <td>https://www.gannett-cdn.com/presto/2...</td>\n",
       "      <td>Wisconsin officer, student wounded a...</td>\n",
       "      <td>oshkosh west school shooting student...</td>\n",
       "      <td>wisconsin officer student wounded os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>google news</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/a...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:09:55 UTC</td>\n",
       "      <td>Greta Thunberg: People underestimate...</td>\n",
       "      <td>Image copyright AFP Image caption Ms...</td>\n",
       "      <td>health kids angry greta climate call...</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/b...</td>\n",
       "      <td>Image copyright AFP Image caption Ms...</td>\n",
       "      <td>greta thunberg people underestimate ...</td>\n",
       "      <td>image copyright afp image caption m ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>google news</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/a...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>14:44:00 UTC</td>\n",
       "      <td>Elon Musk defamation trial begins ov...</td>\n",
       "      <td>Elon Musk is going on trial Tuesday ...</td>\n",
       "      <td>tweets case trial elon cave begins r...</td>\n",
       "      <td>https://cbsnews3.cbsistatic.com/hub/...</td>\n",
       "      <td>Elon Musk is going on trial Tuesday ...</td>\n",
       "      <td>elon musk defamation trial begin ped...</td>\n",
       "      <td>elon musk going trial tuesday troubl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>nytimes_world</td>\n",
       "      <td>https://www.nytimes.com/2019/12/03/w...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>18:23:42 UTC</td>\n",
       "      <td>North Korea Touts New Resort, Seekin...</td>\n",
       "      <td>SEOUL, South Korea — North Korea sai...</td>\n",
       "      <td>touts north norths kim agency resort...</td>\n",
       "      <td>https://static01.nyt.com/images/2019...</td>\n",
       "      <td>SEOUL, South Korea — North Korea sai...</td>\n",
       "      <td>north korea tout new resort seeking ...</td>\n",
       "      <td>seoul south korea north korea said t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>nytimes_business</td>\n",
       "      <td>https://www.nytimes.com/2019/12/03/b...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>08:00:11 UTC</td>\n",
       "      <td>How Far Can Cities Go to Police the ...</td>\n",
       "      <td>Dozens of cities have filed briefs b...</td>\n",
       "      <td>moving boise legitimate los far ange...</td>\n",
       "      <td>https://static01.nyt.com/images/2019...</td>\n",
       "      <td>Dozens of cities have filed briefs b...</td>\n",
       "      <td>far city go police homeless boise te...</td>\n",
       "      <td>dozen city filed brief backing boise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>nytimes_business</td>\n",
       "      <td>https://www.nytimes.com/2019/12/02/b...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>00:19:38 UTC</td>\n",
       "      <td>Trump Bars Bloomberg News Journalist...</td>\n",
       "      <td>President Trump’s re-election campai...</td>\n",
       "      <td>trumps bloomberg events mr democrati...</td>\n",
       "      <td>https://static01.nyt.com/images/2019...</td>\n",
       "      <td>President Trump’s re-election campai...</td>\n",
       "      <td>trump bar bloomberg news journalist ...</td>\n",
       "      <td>president trump reelection campaign ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yahoo.com/trump-foreign...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>09:00:26</td>\n",
       "      <td>Trump’s Foreign Policy Won’t Distrac...</td>\n",
       "      <td>(Bloomberg Opinion) -- If patriotism...</td>\n",
       "      <td>watergate trumps state foreign polic...</td>\n",
       "      <td>https://s.yimg.com/cv/apiv2/social/i...</td>\n",
       "      <td>(Bloomberg Opinion) -- If patriotism...</td>\n",
       "      <td>trump foreign policy wont distract a...</td>\n",
       "      <td>bloomberg opinion patriotism last re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                     link published_date  \\\n",
       "3                 cnn  http://rss.cnn.com/~r/rss/cnn_topsto...     2019-12-03   \n",
       "6                 cnn  http://rss.cnn.com/~r/rss/cnn_topsto...     2019-12-03   \n",
       "13                cnn  http://rss.cnn.com/~r/rss/cnn_topsto...     2019-12-03   \n",
       "55                cbn  https://www1.cbn.com/cbnnews/politic...     2019-12-03   \n",
       "63                cbn  https://www1.cbn.com/cbnnews/us/2019...     2019-12-03   \n",
       "78       wsj_business  https://www.wsj.com/articles/thanksg...     2019-12-03   \n",
       "89         wsj_market  https://www.wsj.com/articles/amazon-...     2019-12-03   \n",
       "179       theguardian  https://www.theguardian.com/us-news/...     2019-12-03   \n",
       "242       theguardian  https://www.theguardian.com/food/sho...     2019-12-03   \n",
       "262           skynews  http://news.sky.com/story/kamala-har...     2019-12-03   \n",
       "271           fortune  https://fortune.com/2019/12/03/kamal...     2019-12-03   \n",
       "274           fortune  https://fortune.com/2019/12/03/its-g...     2019-12-03   \n",
       "289           fortune  https://fortune.com/2019/12/03/trump...     2019-12-03   \n",
       "298       google news  https://news.google.com/__i/rss/rd/a...     2019-12-03   \n",
       "299       google news  https://news.google.com/__i/rss/rd/a...     2019-12-03   \n",
       "304       google news  https://news.google.com/__i/rss/rd/a...     2019-12-03   \n",
       "308       google news  https://news.google.com/__i/rss/rd/a...     2019-12-03   \n",
       "312       google news  https://news.google.com/__i/rss/rd/a...     2019-12-03   \n",
       "329     nytimes_world  https://www.nytimes.com/2019/12/03/w...     2019-12-03   \n",
       "344  nytimes_business  https://www.nytimes.com/2019/12/03/b...     2019-12-03   \n",
       "354  nytimes_business  https://www.nytimes.com/2019/12/02/b...     2019-12-03   \n",
       "389         yahoonews  https://news.yahoo.com/trump-foreign...     2019-12-03   \n",
       "\n",
       "    published_time                                    title  \\\n",
       "3     17:24:49 UTC  Here's why the impeachment polling i...   \n",
       "6     19:10:00 UTC  Kamala Harris ends 2020 presidential...   \n",
       "13    16:48:05 UTC  Macron refuses to back down after Tr...   \n",
       "55    18:34:22 UTC  Kamala Harris to End Democratic Pres...   \n",
       "63    13:24:11 UTC  Washington DC Synagogue Vandalized W...   \n",
       "78       12:51:00   Thanksgiving Weekend Shoppers Booste...   \n",
       "89       12:26:00                 Amazon Dots the Landscape   \n",
       "179   19:22:23 UTC  Kamala Harris drops out of Democrati...   \n",
       "242   16:36:19 UTC  Why is a 2017 bottle of Irn-Bru sell...   \n",
       "262   18:29:00 UTC  Kamala Harris drops out of race for ...   \n",
       "271   19:00:38 UTC  Kamala Harris Drops Out of President...   \n",
       "274   17:47:06 UTC  It’s Giving Tuesday and You Have a C...   \n",
       "289   12:06:51 UTC             Trump's New Trade Offensives   \n",
       "298   16:35:00 UTC                          NPR Choice page   \n",
       "299   17:54:00 UTC  Appeals Court Orders Trump’s Banks t...   \n",
       "304   18:22:18 UTC  Oshkosh West school shooting: Studen...   \n",
       "308   16:09:55 UTC  Greta Thunberg: People underestimate...   \n",
       "312   14:44:00 UTC  Elon Musk defamation trial begins ov...   \n",
       "329   18:23:42 UTC  North Korea Touts New Resort, Seekin...   \n",
       "344   08:00:11 UTC  How Far Can Cities Go to Police the ...   \n",
       "354   00:19:38 UTC  Trump Bars Bloomberg News Journalist...   \n",
       "389      09:00:26   Trump’s Foreign Policy Won’t Distrac...   \n",
       "\n",
       "                                        text  \\\n",
       "3    (CNN) The impeachment inquiry into P...   \n",
       "6    (CNN) Sen. Kamala Harris ended her 2...   \n",
       "13   London (CNN) French President Emmanu...   \n",
       "55   WASHINGTON (AP) - Democratic Sen. Ka...   \n",
       "63   JERUSALEM, Israel - Swastikas and an...   \n",
       "78   American shoppers increased their sp...   \n",
       "89   Consumers apparently love having Ama...   \n",
       "179  California senator had started stron...   \n",
       "242  An out-of-date Irn-Bru bottle is bei...   \n",
       "262  Kamala Harris said she will 'do ever...   \n",
       "271  Why IBM Is Joining the Corporate Cho...   \n",
       "274  Kamala Harris Drops Out of President...   \n",
       "289  Kamala Harris Drops Out of President...   \n",
       "298  By choosing “I agree” below, you agr...   \n",
       "299  President Donald Trump in London on ...   \n",
       "304  Wisconsin officer, student wounded a...   \n",
       "308  Image copyright AFP Image caption Ms...   \n",
       "312  Elon Musk is going on trial Tuesday ...   \n",
       "329  SEOUL, South Korea — North Korea sai...   \n",
       "344  Dozens of cities have filed briefs b...   \n",
       "354  President Trump’s re-election campai...   \n",
       "389  (Bloomberg Opinion) -- If patriotism...   \n",
       "\n",
       "                                    keywords  \\\n",
       "3    polling trumps moving wanted isnt op...   \n",
       "6    35 ends caption presidential photos ...   \n",
       "13   turkey leaders companies meeting ref...   \n",
       "55   end state presidential democratic se...   \n",
       "63   united means antisemitic states post...   \n",
       "78   stores wavered period signaling boos...   \n",
       "89   ubercheap speaker smart seven sales ...   \n",
       "179  iowa state drops candidates debate r...   \n",
       "242  irnbru 2017 recipe version tax origi...   \n",
       "262  resources drops regret deep states r...   \n",
       "271  drops calling race carbon presidenti...   \n",
       "274  drops bloomberg dig race im choice p...   \n",
       "289  offensives trumps drops bloomberg tr...   \n",
       "298  social information npr choice sites ...   \n",
       "299  records setting trumps stage subpoen...   \n",
       "304  shooting lockdown wisconsin officer ...   \n",
       "308  health kids angry greta climate call...   \n",
       "312  tweets case trial elon cave begins r...   \n",
       "329  touts north norths kim agency resort...   \n",
       "344  moving boise legitimate los far ange...   \n",
       "354  trumps bloomberg events mr democrati...   \n",
       "389  watergate trumps state foreign polic...   \n",
       "\n",
       "                                       image  \\\n",
       "3    https://cdn.cnn.com/cnnnext/dam/asse...   \n",
       "6    https://cdn.cnn.com/cnnnext/dam/asse...   \n",
       "13   https://cdn.cnn.com/cnnnext/dam/asse...   \n",
       "55   https://www1.cbn.com/sites/default/f...   \n",
       "63   https://www1.cbn.com/sites/default/f...   \n",
       "78   https://images.wsj.net/im-132607/social   \n",
       "89   https://images.wsj.net/im-132609/social   \n",
       "179  https://i.guim.co.uk/img/media/349ae...   \n",
       "242  https://i.guim.co.uk/img/media/0d20d...   \n",
       "262  https://e3.365dm.com/19/12/1600x900/...   \n",
       "271  https://content.fortune.com/wp-conte...   \n",
       "274  https://content.fortune.com/wp-conte...   \n",
       "289  https://content.fortune.com/wp-conte...   \n",
       "298                                            \n",
       "299  https://compote.slate.com/images/861...   \n",
       "304  https://www.gannett-cdn.com/presto/2...   \n",
       "308  https://ichef.bbci.co.uk/news/1024/b...   \n",
       "312  https://cbsnews3.cbsistatic.com/hub/...   \n",
       "329  https://static01.nyt.com/images/2019...   \n",
       "344  https://static01.nyt.com/images/2019...   \n",
       "354  https://static01.nyt.com/images/2019...   \n",
       "389  https://s.yimg.com/cv/apiv2/social/i...   \n",
       "\n",
       "                                     summary  \\\n",
       "3    (CNN) The impeachment inquiry into P...   \n",
       "6    Photos: Former presidential candidat...   \n",
       "13   London (CNN) French President Emmanu...   \n",
       "55   WASHINGTON (AP) - Democratic Sen. Ka...   \n",
       "63   JERUSALEM, Israel - Swastikas and an...   \n",
       "78   American shoppers increased their sp...   \n",
       "89   Consumers apparently love having Ama...   \n",
       "179  California senator had started stron...   \n",
       "242  An out-of-date Irn-Bru bottle is bei...   \n",
       "262  Kamala Harris said she will 'do ever...   \n",
       "271  Why IBM Is Joining the Corporate Cho...   \n",
       "274  Kamala Harris Drops Out of President...   \n",
       "289  Kamala Harris Drops Out of President...   \n",
       "298  By choosing “I agree” below, you agr...   \n",
       "299  The judge refused, so Trump appealed...   \n",
       "304  Wisconsin officer, student wounded a...   \n",
       "308  Image copyright AFP Image caption Ms...   \n",
       "312  Elon Musk is going on trial Tuesday ...   \n",
       "329  SEOUL, South Korea — North Korea sai...   \n",
       "344  Dozens of cities have filed briefs b...   \n",
       "354  President Trump’s re-election campai...   \n",
       "389  (Bloomberg Opinion) -- If patriotism...   \n",
       "\n",
       "                                 clean_title  \\\n",
       "3                 impeachment polling moving   \n",
       "6    kamala harris end presidential campaign   \n",
       "13           macron refuse back trump attack   \n",
       "55   kamala harris end democratic preside...   \n",
       "63   washington dc synagogue vandalized s...   \n",
       "78   thanksgiving weekend shopper boosted...   \n",
       "89                      amazon dot landscape   \n",
       "179  kamala harris drop democratic presid...   \n",
       "242                    bottle irnbru selling   \n",
       "262  kamala harris drop race democratic p...   \n",
       "271     kamala harris drop presidential race   \n",
       "274               giving tuesday choice make   \n",
       "289                trump new trade offensive   \n",
       "298                          npr choice page   \n",
       "299  appeal court order trump bank turn r...   \n",
       "304  oshkosh west school shooting student...   \n",
       "308  greta thunberg people underestimate ...   \n",
       "312  elon musk defamation trial begin ped...   \n",
       "329  north korea tout new resort seeking ...   \n",
       "344  far city go police homeless boise te...   \n",
       "354  trump bar bloomberg news journalist ...   \n",
       "389  trump foreign policy wont distract a...   \n",
       "\n",
       "                                  clean_text  \n",
       "3    impeachment inquiry president donald...  \n",
       "6    sen kamala harris ended presidential...  \n",
       "13   london french president emmanuel mac...  \n",
       "55   washington ap democratic sen kamala ...  \n",
       "63   jerusalem israel swastika antisemiti...  \n",
       "78   american shopper increased spending ...  \n",
       "89   consumer apparently love amazoncom m...  \n",
       "179  california senator started strongly ...  \n",
       "242  outofdate irnbru bottle sold ebay is...  \n",
       "262  kamala harris said do everything pow...  \n",
       "271  ibm joining corporate chorus calling...  \n",
       "274  kamala harris drop presidential race...  \n",
       "289  kamala harris drop presidential race...  \n",
       "298  choosing i agree below agree npr sit...  \n",
       "299  president donald trump london tuesda...  \n",
       "304  wisconsin officer student wounded os...  \n",
       "308  image copyright afp image caption m ...  \n",
       "312  elon musk going trial tuesday troubl...  \n",
       "329  seoul south korea north korea said t...  \n",
       "344  dozen city filed brief backing boise...  \n",
       "354  president trump reelection campaign ...  \n",
       "389  bloomberg opinion patriotism last re...  "
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simval = \"2\"\n",
    "dict_pos_titles = {elem: get_indexes(eucl_dist_df, elem) for elem in unique_simvals_filtered}\n",
    "index_list_titles = []\n",
    "find_indexes(dict_pos_titles, index_list_titles)\n",
    "index_list_titles = list(set(index_list_titles))\n",
    "\n",
    "sorted(index_list_titles)\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_list_titles, :]).drop_duplicates((\"title\"))).sort_index()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 40)\n",
    "select_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>image</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>theguardian</td>\n",
       "      <td>https://www.theguardian.com/us-news/...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>19:22:23 UTC</td>\n",
       "      <td>Kamala Harris drops out of Democrati...</td>\n",
       "      <td>California senator had started stron...</td>\n",
       "      <td>iowa state drops candidates debate r...</td>\n",
       "      <td>https://i.guim.co.uk/img/media/349ae...</td>\n",
       "      <td>California senator had started stron...</td>\n",
       "      <td>kamala harris drop democratic presid...</td>\n",
       "      <td>california senator started strongly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>google news</td>\n",
       "      <td>https://news.google.com/__i/rss/rd/a...</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:35:00 UTC</td>\n",
       "      <td>NPR Choice page</td>\n",
       "      <td>By choosing “I agree” below, you agr...</td>\n",
       "      <td>social information npr choice sites ...</td>\n",
       "      <td></td>\n",
       "      <td>By choosing “I agree” below, you agr...</td>\n",
       "      <td>npr choice page</td>\n",
       "      <td>choosing i agree below agree npr sit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                     link published_date  \\\n",
       "179  theguardian  https://www.theguardian.com/us-news/...     2019-12-03   \n",
       "298  google news  https://news.google.com/__i/rss/rd/a...     2019-12-03   \n",
       "\n",
       "    published_time                                    title  \\\n",
       "179   19:22:23 UTC  Kamala Harris drops out of Democrati...   \n",
       "298   16:35:00 UTC                          NPR Choice page   \n",
       "\n",
       "                                        text  \\\n",
       "179  California senator had started stron...   \n",
       "298  By choosing “I agree” below, you agr...   \n",
       "\n",
       "                                    keywords  \\\n",
       "179  iowa state drops candidates debate r...   \n",
       "298  social information npr choice sites ...   \n",
       "\n",
       "                                       image  \\\n",
       "179  https://i.guim.co.uk/img/media/349ae...   \n",
       "298                                            \n",
       "\n",
       "                                     summary  \\\n",
       "179  California senator had started stron...   \n",
       "298  By choosing “I agree” below, you agr...   \n",
       "\n",
       "                                 clean_title  \\\n",
       "179  kamala harris drop democratic presid...   \n",
       "298                          npr choice page   \n",
       "\n",
       "                                  clean_text  \n",
       "179  california senator started strongly ...  \n",
       "298  choosing i agree below agree npr sit...  "
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining categories\n",
    "\n",
    "\n",
    "listtest = [\"model\",\"sales\",\"technology\",\"stocks\",\"stockmarket\",\"finance\",\"model\",\"2020\",\"ends\",\"federal\"]\n",
    "\n",
    "select_articles1 = select_articles[select_articles[\"keywords\"].str.contains('|'.join(listtest))]\n",
    "\n",
    "def categories(topic, topic_list):\n",
    "    for i,v in (glove_wiki.most_similar(positive=topic)):\n",
    "        topic_list.append(i)\n",
    "\n",
    "        \n",
    "politics, economy, finance, tech, business = [], [], [], [], []\n",
    "        \n",
    "categories(\"politics\", politics)\n",
    "categories(\"economy\", economy)\n",
    "categories(\"finance\", finance)\n",
    "categories(\"tech\", tech)\n",
    "categories(\"business\", business)\n",
    "\n",
    "select_politics = select_articles[select_articles[\"keywords\"].str.contains('|'.join(politics))]\n",
    "select_economy = select_articles[select_articles[\"keywords\"].str.contains('|'.join(economy))]\n",
    "select_finance = select_articles[select_articles[\"keywords\"].str.contains('|'.join(finance))]\n",
    "select_tech = select_articles[select_articles[\"keywords\"].str.contains('|'.join(tech))]\n",
    "select_business = select_articles[select_articles[\"keywords\"].str.contains('|'.join(business))]\n",
    "\n",
    "select_politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generating newsletter in HTML\n",
    "\n",
    "After already having designed the HTML body for the newsletter, we need to prepare the extracted article titles and texts for automatically entering intp the HTML body.\n",
    "\n",
    "## 4.1 Importing extracted titles and content into Newsletter\n",
    "\n",
    "Just for testing, we will randomly chose which articles to include in our newsletter body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appeals Court Orders Trump’s Banks to Turn Over Records, Setting the Stage for a SCOTUS Showdown \n",
      " President Donald Trump in London on Tuesday. Ludovic Marin/Getty Images\n",
      "\n",
      "A federal appeals court upheld the House of Representatives’ subpoena of Donald Trump’s financial records on Tuesday, continuing Democrats’ winning streak in their fight for ove... \n",
      " google news \n",
      " https://news.google.com/__i/rss/rd/articles/CBMiZmh0dHBzOi8vc2xhdGUuY29tL25ld3MtYW5kLXBvbGl0aWNzLzIwMTkvMTIvY291cnQtb3JkZXItZGV1dHNjaGUtYmFuay1jYXBpdGFsLW9uZS10cnVtcC1zdWJwb2VuYXMuaHRtbNIBZWh0dHBzOi8vc2xhdGUuY29tL25ld3MtYW5kLXBvbGl0aWNzLzIwMTkvMTIvY291cnQtb3JkZXItZGV1dHNjaGUtYmFuay1jYXBpdGFsLW9uZS10cnVtcC1zdWJwb2VuYXMuYW1w?oc=5\n"
     ]
    }
   ],
   "source": [
    "# creating separate lists of the columns and info we want to include\n",
    "similar_sources_list = list(select_articles['source'])\n",
    "similar_links_list = list(select_articles['link'])\n",
    "similar_titles_list = list(select_articles['title'])\n",
    "similar_texts_list = list(select_articles['text'])\n",
    "\n",
    "# randomly select articles to include\n",
    "random_select = select_articles.reset_index(drop=True) # resetting the index of the df\n",
    "nr_of_art = (list(random_select.shape))[0] # finding max number of rows of the df of the most similar articles\n",
    "\n",
    "random_art_nr = np.random.choice(nr_of_art, 8, replace=False) # randomly chose 7 articles out of the max possible\n",
    "random_art_nr_list = list(random_art_nr)\n",
    "\n",
    "# function to extract the articles by their random number in the index, limits the characters of text by 'max_chars' and adds '...' to the end \n",
    "def rand_info(nr_of_art, max_chars):\n",
    "    global rand_text, rand_source, rand_link, rand_title\n",
    "    rand_text, rand_source, rand_link, rand_title = [], [], [], []\n",
    "    random_art_nr = np.random.choice(nr_of_art, 8, replace=False)  # chosen randomly  \n",
    "    for nr in random_art_nr:\n",
    "        (rand_text.append((similar_texts_list[nr])[:max_chars]))\n",
    "        (rand_source.append(similar_sources_list[nr]))\n",
    "        (rand_link.append(similar_links_list[nr]))\n",
    "        (rand_title.append(similar_titles_list[nr]))\n",
    "    rand_text = [item + '...' for item in rand_text]\n",
    "\n",
    "#random_select = select_articles.reset_index(drop=True) # resetting the index of the df\n",
    "rand_info(random_art_nr_list, 250) # selecting the articles randomly and maximizing texts by 250 chars\n",
    "\n",
    "# now every time the code is excuted, a new randomly chosen article appears\n",
    "print(rand_title[0],\"\\n\" , rand_text[0], \"\\n\" , rand_source[0], \"\\n\", rand_link[0])\n",
    "\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Appeals Court Orders Trump`s Banks to Turn Over Records, Setting the Stage for a SCOTUS Showdown',\n",
       " 'With Brits Used to Surveillance, More Companies Try Tracking Faces',\n",
       " 'Cannabis On The Tickets In The Coming UK Elections',\n",
       " 'Pioneer Distilleries down 5% after fixing swap ratio for merger with USL',\n",
       " \"India's yield curve sees steepest rise in nine years, set to go up further\",\n",
       " 'MARKET WRAP: Sensex dips 127 pts, Nifty below 12,000; PSBs, metals decline',\n",
       " 'North Korea Touts New Resort, Seeking to Blunt U.N. Sanctions',\n",
       " 'Kamala Harris to End Democratic Presidential Campaign']"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting issues\n",
    "# ! are more unsupported characters ! to be edited and added over time\n",
    "\n",
    "# function to replace the wrongly formatted characters (obersed by looking at the html output)\n",
    "def replace_char(list_of_str):\n",
    "    for i in range(len(list_of_str)):\n",
    "        list_of_str[i] = list_of_str[i].replace(\"’\",\"`\")\n",
    "        list_of_str[i] = list_of_str[i].replace(\":\",\":\")\n",
    "        list_of_str[i] = list_of_str[i].replace(\"–\",\"-\")\n",
    "        #print(data)\n",
    "\n",
    "replace_char(rand_text)\n",
    "replace_char(rand_title)\n",
    "\n",
    "rand_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code is way easier to edit in Notepad ++\n",
    "\n",
    "print ()\n",
    "f = open('HTML_with VARS_V1.html','w')\n",
    " \n",
    "message = \"\"\"\n",
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
    "<html xmlns=\"http://www.w3.org/1999/xhtml\"><head>\n",
    "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
    "<title>Demystifying Email Design</title>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<link href=\"NewsletterTemplate_files/css.css\" rel=\"stylesheet\">    \n",
    " \n",
    "</head>\n",
    "<body style=\"margin: 0; padding: 0;\">\n",
    "    <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\"> \n",
    "        <tbody><tr>\n",
    "            <td style=\"padding: 10px 0 10px 0;\">\n",
    "                <table style=\"border: 1px solid #cccccc; border-collapse: collapse;\" width=\"1000\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\" align=\"center\">\n",
    "                    <tbody><tr>\n",
    "                        <td style=\"padding: 20px\" height=\"204\" bgcolor=\"#fbf315\" align=\"top\">\n",
    "                            <img alt=\"Creating Email Magic\" style=\"display: block;\" src=\"NewsletterTemplate_files/Logo-Raiffeisen-Bank-2017.png\" width=\"304\" height=\"304\">\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 20px 30px 40px 30px;\" bgcolor=\"#ffffff\">\n",
    "                            <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                <tbody><tr>\n",
    "                                    <td style=\"color: #153643; \n",
    "    font-family: 'Archivo Black', sans-serif; font-size: 40px;\">\n",
    "                                        <b>Daily Finance Update\n",
    "</b>\n",
    "                                    </td>\n",
    "                                \n",
    "                                        \n",
    "                                    </tr><tr>\n",
    "                                    <td style=\"color: #153643; \n",
    "    font-family: 'Archivo Black', sans-serif; font-size: 20px; padding: 10px 0px 10px 0px;\">\n",
    "                                        <b>Stocks\n",
    "</b>\n",
    "                                    </td>\n",
    "                                \n",
    "                                        \n",
    "                                    </tr>\n",
    "                                \n",
    "                                <tr>\n",
    "                                    <td>\n",
    "                                        <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                            <tbody><tr>\n",
    "                                                <td style=\"box-shadow: 1px 2px 4px rgba(0, 0, 0, .5);\" width=\"160\" valign=\"top\">\n",
    "                                                    <div style=\"padding: 20px 10px 5px 10px; font-family: 'Archivo Black', sans-serif; font-size: 22px\">\n",
    "  <b>TECH</b>\n",
    "</div><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Tech | {rand_source[0]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[0]}\">{rand_title[0]}</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[0]}\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Tech | {rand_source[1]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[1]}\">{rand_title[1]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[1]}\n",
    "                                                   \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Tech | {rand_source[2]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[2]}\">{rand_title[2]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[2]}\n",
    "</div></tdbody><table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                                                        </table>\n",
    "                                                </td><td style=\"font-size: 0; line-height: 0;\" width=\"20\">\n",
    "                                                    &nbsp;\n",
    "                                                </td><td style=\"box-shadow: 1px 2px 4px rgba(0, 0, 0, .5);\" width=\"160\" valign=\"top\">\n",
    "                                                    <div style=\"padding: 20px 10px 5px 10px; font-family: 'Archivo Black', sans-serif; font-size: 22px\">\n",
    "  <b>DEALS AND IPOs\n",
    "</b>\n",
    "</div><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Deals | {rand_source[3]} \n",
    "</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[3]}\">{rand_title[3]}\n",
    "Day record of more than $30 billion in sales and climbing</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">\n",
    "                                                            \n",
    "{rand_text[3]}\n",
    "\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Markets | {rand_source[4]}\n",
    "</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[4]}\">{rand_title[4]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">\n",
    "                                                            \n",
    "{rand_text[4]}\n",
    "\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                                        \n",
    "</table>\n",
    "                                                </td>\n",
    "                                                <td style=\"font-size: 0; line-height: 0;\" width=\"20\">\n",
    "                                                    &nbsp;\n",
    "                                                </td>\n",
    "                                                <td style=\"box-shadow: 1px 2px 4px rgba(0, 0, 0, .5);\" width=\"160\" valign=\"top\">\n",
    "                                                    <div style=\"padding: 20px 10px 5px 10px; font-family: 'Archivo Black', sans-serif; font-size: 22px\">\n",
    "  <b>BANKS\n",
    "</b>\n",
    "</div><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Trading | {rand_source[5]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[5]}\">{rand_title[5]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">\n",
    "                                                            \n",
    "{rand_text[5]}\n",
    "\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Earnings | {rand_source[6]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[6]}\">{rand_title[6]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[6]}\n",
    "</div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">JPMorgan | {rand_source[7]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[7]}\">{rand_title[7]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[7]}\n",
    "</div></tdbody><table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                                                        </table>\n",
    "                                                </td>\n",
    "                                            </tr>\n",
    "                                        </tbody></table>\n",
    "                                    </td>\n",
    "                                </tr>\n",
    "                            </tbody></table>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 30px 30px 30px 30px;\" bgcolor=\"#666666\">\n",
    "                            <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                <tbody><tr>\n",
    "                                    <td style=\"color: #ffffff; font-family: Arial, sans-serif; font-size: 14px;\" width=\"75%\">\n",
    "                                        ® Someone, somewhere 2019<br>\n",
    "                                        <a href=\"#\" style=\"color: #ffffff;\"><font color=\"#ffffff\">Unsubscribe</font></a> to this newsletter instantly\n",
    "                                    </td>\n",
    "                                    <td width=\"25%\" align=\"right\">\n",
    "                                        <table cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                            <tbody><tr>\n",
    "                                                <td style=\"font-family: Arial, sans-serif; font-size: 12px; font-weight: bold;\">\n",
    "                                                    <a href=\"https://twitter.com/raiffeisen_at\" style=\"color: #666666;\">\n",
    "                                                        <img src=\"NewsletterTemplate_files/logo.png\" alt=\"Twitter\" style=\"display: block;\" width=\"38\" height=\"38\" border=\"0\">\n",
    "                                                    </a>\n",
    "                                                </td>\n",
    "                                                <td style=\"font-size: 0; line-height: 0;\" width=\"20\">&nbsp;</td>\n",
    "                                                <td style=\"font-family: Arial, sans-serif; font-size: 12px; font-weight: bold;\">\n",
    "                                                    <a href=\"http://www.facebook.com/raiffeisen/\" style=\"color: #666666;\">\n",
    "                                                        <img alt=\"Facebook\" style=\"display: block;\" src=\"NewsletterTemplate_files/facebook-2.svg\" width=\"38\" height=\"38\" border=\"0\">\n",
    "                                                    </a>\n",
    "                                                </td>\n",
    "                                            </tr>\n",
    "                                        </tbody></table>\n",
    "                                    </td>\n",
    "                                </tr>\n",
    "                            </tbody></table>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </tbody></table>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody></table>\n",
    "\n",
    "\n",
    "\n",
    "</body></html>\n",
    "\n",
    "\"\"\".format(**locals()) #########\n",
    " \n",
    "f.write(message)\n",
    "f.close()\n",
    "\n",
    "#Change path to reflect file location\n",
    "filename = 'file:///'+os.getcwd()+'/' + 'HTML_with VARS_V1.html'\n",
    "webbrowser.open_new_tab(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Other stuff that could be helpful in the future\n",
    "\n",
    "## Time how long a code takes to execute\n",
    "\n",
    "Could be used for speed comparison of two similarity methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "code_to_test = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "elapsed_time = timeit.timeit(code_to_test, number=100)/100\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google word meaning vector, pre-trained\n",
    "\n",
    "Maybe useful, some time?\n",
    "\n",
    "Other pre-trained models to be found here: https://github.com/RaRe-Technologies/gensim-data/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\") #1.6GB to download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting each word in title/text in pandas df to a separate column\n",
    "\n",
    "Maybe useful, some time?\n",
    "\n",
    "Code was hard to find via google haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = news_df_daily.str.split(expand=True)\n",
    "title_splitted = pd.DataFrame(split)\n",
    "title_splitted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
