{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # to ignore all future warinings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the dataset\n",
    "\n",
    "### 1.1 Scraping news articles from the web\n",
    "\n",
    "This process takes on average between 2 and 15min, depending on how many website links are to be scraped, how many articles in these links are found and how much computing ressources the machine has on which the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2019-11-18\n"
     ]
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import time\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "import dateutil\n",
    "\n",
    "#### 1 Website data ####\n",
    "\n",
    "with open('NewsPapers_new.json') as data_file: #Loads the JSON files with news URLs\n",
    "    companies = json.load(data_file)\n",
    "\n",
    "#### 2 Todays date - for filtering the articles by todays date ####\n",
    "today = str(date.today()) \n",
    "print(\"Today's date:\", today)\n",
    "\n",
    "\n",
    "#### 3 Scraping the news articles ####\n",
    "\n",
    "text_list = []\n",
    "source_list = []\n",
    "article_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "title_list = []\n",
    "image_list = []\n",
    "keywords_list = []\n",
    "summaries_list = []\n",
    "\n",
    "for source, value in companies.items(): \n",
    "    d = fp.parse(value['rss'])\n",
    "    article={}\n",
    "    for entry in d.entries:\n",
    "        if hasattr(entry, 'published') and ((dateutil.parser.parse(getattr(entry, 'published'))).strftime(\"%Y-%m-%d\") == today):\n",
    "            article['source'] = source\n",
    "            source_list.append(article['source'])\n",
    "\n",
    "            #getting the article URLs\n",
    "            article['link'] = entry.link\n",
    "            article_list.append(article['link'])\n",
    "\n",
    "            #getting the article published dates\n",
    "            date = (getattr(entry, 'published'))\n",
    "            date = dateutil.parser.parse(date)\n",
    "            date_formated = date.strftime(\"%Y-%m-%d\")\n",
    "            time_formated = date.strftime(\"%H:%M:%S %Z\") #hour, minute, timezone (converted)\n",
    "            date_list.append(date_formated)\n",
    "            time_list.append(time_formated)\n",
    "\n",
    "            #getting the titles\n",
    "            content = Article(entry.link)\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()  \n",
    "                content.nlp()\n",
    "            except Exception as e: \n",
    "                #in case the download fails, it prints the error and immediatly continues with downloading the next article\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "            title = content.title #extract article titles\n",
    "            image = content.top_image\n",
    "            image_list.append(image)\n",
    "            content.nlp()\n",
    "            keywords = content.keywords\n",
    "            keywords_list.append(keywords)\n",
    "            title_list.append(title)\n",
    "            text = content.text\n",
    "            text_list.append(text)\n",
    "            summaries = content.summary\n",
    "            summaries_list.append(summaries)\n",
    "                \n",
    "#creating dicts for formatting and inserting to pandas df\n",
    "source_dict = {'source':source_list}\n",
    "link_dict = {'link':article_list}\n",
    "date_dict = {'published_date':date_list}\n",
    "time_dict = {'published_time':time_list}\n",
    "title_dict = {'title':title_list}\n",
    "text_dict = {'text':text_list}\n",
    "keyword_dict = {'keywords':keywords_list}\n",
    "image_dict = {'image':image_list}\n",
    "summary_dict = {'summary':summaries_list}\n",
    "\n",
    "#creating separate pandas dfs for each feature\n",
    "source_df = pd.DataFrame(source_dict, index=None)\n",
    "link_df = pd.DataFrame(link_dict, index=None)\n",
    "date_df = pd.DataFrame(date_dict, index=None)\n",
    "time_df = pd.DataFrame(time_dict, index=None)\n",
    "title_df = pd.DataFrame(title_dict, index=None)\n",
    "text_df = pd.DataFrame(text_dict, index=None)\n",
    "keyword_df = pd.DataFrame(keyword_dict, index=None)\n",
    "image_df = pd.DataFrame(image_dict, index=None)\n",
    "summary_df = pd.DataFrame(summary_dict, index=None)\n",
    "\n",
    "#join all pandas dfs together\n",
    "news_df = source_df.join(link_df)\n",
    "news_df = news_df.join(date_df)\n",
    "news_df = news_df.join(time_df)\n",
    "news_df = news_df.join(title_df)\n",
    "news_df = news_df.join(text_df)\n",
    "news_df = news_df.join(keyword_df)\n",
    "news_df = news_df.join(image_df)\n",
    "news_df = news_df.join(summary_df)\n",
    "\n",
    "# after running, pandas DF sould be created with link, published_date, published_time, title and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Filtering and cleaning the dataset\n",
    "\n",
    "In order to run some analysis on the titles and text content of the articles, we need to clean them.\n",
    "We first filter all the articles we scraped by todays date. \n",
    "For cleaning the titles and article content text, we go through the following steps:\n",
    "\n",
    "*  remove stopwords (i.e. \"a\", \"for\", \"when\", \"you\", \"if\",... etc. that would impact the accuracy of our similarity analysis)\n",
    "*  remove punctuation\n",
    "*  remove numbers\n",
    "*  remove names of the source website in the article text (we noticed, that f.e. CNN often mentions \"CNN\" in their articles, which would impact on the accuracy of our similarty analysis)\n",
    "*  make the sentences lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>image</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>18:02:00 UTC</td>\n",
       "      <td>House investigat...</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>[mueller, trump,...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>house investigat...</td>\n",
       "      <td>washington  hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:23:02 UTC</td>\n",
       "      <td>The latest on th...</td>\n",
       "      <td>Mark Makela/Gett...</td>\n",
       "      <td>[trump, set, twe...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Mark Makela/Gett...</td>\n",
       "      <td>latest trump imp...</td>\n",
       "      <td>mark makelagetty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>16:58:27 UTC</td>\n",
       "      <td>Trump attacks an...</td>\n",
       "      <td>(CNN) President ...</td>\n",
       "      <td>[witness, trump,...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>But Sondland is ...</td>\n",
       "      <td>trump attacks an...</td>\n",
       "      <td>president donal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>01:00:41 UTC</td>\n",
       "      <td>Tweets can be us...</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>[world, court, m...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Chat with us in ...</td>\n",
       "      <td>tweets used cour...</td>\n",
       "      <td>chat us facebook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:03:00 UTC</td>\n",
       "      <td>House Republican...</td>\n",
       "      <td>(CNN) House Repu...</td>\n",
       "      <td>[republican, ukr...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) House Repu...</td>\n",
       "      <td>house republican...</td>\n",
       "      <td>house republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>18:18:27 UTC</td>\n",
       "      <td>Supreme Court st...</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>[documents, trum...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Washington (CNN)...</td>\n",
       "      <td>supreme court st...</td>\n",
       "      <td>washington  pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:09:03 UTC</td>\n",
       "      <td>Why you just can...</td>\n",
       "      <td>(CNN) President ...</td>\n",
       "      <td>[physical, donal...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) President ...</td>\n",
       "      <td>cant trust white...</td>\n",
       "      <td>president donal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:10:48 UTC</td>\n",
       "      <td>Chick-fil-A will...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>[salvation, orga...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>chickfila longer...</td>\n",
       "      <td>new york  busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>18:40:04 UTC</td>\n",
       "      <td>Gunman kills at ...</td>\n",
       "      <td>(CNN) At least t...</td>\n",
       "      <td>[lot, walmart, s...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) At least t...</td>\n",
       "      <td>gunman kills lea...</td>\n",
       "      <td>least two victi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>15:37:22 UTC</td>\n",
       "      <td>Ryan Costello: M...</td>\n",
       "      <td>(CNN) Ryan Coste...</td>\n",
       "      <td>[organization, m...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Ryan Coste...</td>\n",
       "      <td>ryan costello mi...</td>\n",
       "      <td>ryan costello p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:17:51 UTC</td>\n",
       "      <td>Lindsey Graham f...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>[failure, media,...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>He then repeated...</td>\n",
       "      <td>lindsey graham f...</td>\n",
       "      <td>new york  busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>18:52:17 UTC</td>\n",
       "      <td>10 people were s...</td>\n",
       "      <td>(CNN) A group of...</td>\n",
       "      <td>[reid, shooting,...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) A group of...</td>\n",
       "      <td>people shot  fa...</td>\n",
       "      <td>group family fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>10:49:00 UTC</td>\n",
       "      <td>Hunter in China ...</td>\n",
       "      <td>Hong Kong (CNN) ...</td>\n",
       "      <td>[person, infecte...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Hong Kong (CNN) ...</td>\n",
       "      <td>hunter china cat...</td>\n",
       "      <td>hong kong  twent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:14:24 UTC</td>\n",
       "      <td>WeWork braces fo...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>[according, brac...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>On Monday, WeWor...</td>\n",
       "      <td>wework braces ma...</td>\n",
       "      <td>new york  busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>16:47:41 UTC</td>\n",
       "      <td>Box truck crash ...</td>\n",
       "      <td>A close call was...</td>\n",
       "      <td>[struck, skidded...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>A close call was...</td>\n",
       "      <td>box truck crash ...</td>\n",
       "      <td>close call caugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:26:45 UTC</td>\n",
       "      <td>How to help Saug...</td>\n",
       "      <td>(CNN) While noth...</td>\n",
       "      <td>[school, shootin...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) While noth...</td>\n",
       "      <td>help saugus high...</td>\n",
       "      <td>nothing bring b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>15:36:46 UTC</td>\n",
       "      <td>Kylie Jenner sel...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>[cosmetics, mill...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>kylie jenner sel...</td>\n",
       "      <td>new york  busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>15:53:20 UTC</td>\n",
       "      <td>Will.i.am: Qanta...</td>\n",
       "      <td>(CNN) — Qantas h...</td>\n",
       "      <td>[member, doesnt,...</td>\n",
       "      <td>https://dynaimag...</td>\n",
       "      <td>He said on Twitt...</td>\n",
       "      <td>william qantas s...</td>\n",
       "      <td>qantas vowed h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:08:57 UTC</td>\n",
       "      <td>Kathie Lee Giffo...</td>\n",
       "      <td>(CNN) Multiple w...</td>\n",
       "      <td>[lee, thrilled, ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Multiple w...</td>\n",
       "      <td>kathie lee giffo...</td>\n",
       "      <td>multiple weddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:17:42 UTC</td>\n",
       "      <td>'Jeopardy! The G...</td>\n",
       "      <td>(CNN) The three ...</td>\n",
       "      <td>[receive, trebek...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) The three ...</td>\n",
       "      <td>jeopardy greates...</td>\n",
       "      <td>three top winne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:06:52 UTC</td>\n",
       "      <td>Idris Elba is OK...</td>\n",
       "      <td>(CNN) In case yo...</td>\n",
       "      <td>[legend, alive, ...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) In case yo...</td>\n",
       "      <td>idris elba ok de...</td>\n",
       "      <td>case wondering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:02:35 UTC</td>\n",
       "      <td>'The Real Housew...</td>\n",
       "      <td>\"The Real Housew...</td>\n",
       "      <td>[coming, 2020, u...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>\"The Real Housew...</td>\n",
       "      <td>the real housewi...</td>\n",
       "      <td>the real housewi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:44:43 UTC</td>\n",
       "      <td>Marie Kondo clea...</td>\n",
       "      <td>London (CNN Lond...</td>\n",
       "      <td>[london, product...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>London (CNN Lond...</td>\n",
       "      <td>marie kondo clea...</td>\n",
       "      <td>london  london m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:56:42 UTC</td>\n",
       "      <td>Ford takes aim a...</td>\n",
       "      <td>Los Angeles (CNN...</td>\n",
       "      <td>[model, ford, pe...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Los Angeles (CNN...</td>\n",
       "      <td>ford takes aim t...</td>\n",
       "      <td>los angeles  bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>16:32:01 UTC</td>\n",
       "      <td>Elon Musk tweets...</td>\n",
       "      <td>(CNN) Ford unvei...</td>\n",
       "      <td>[elon, ford, mod...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>(CNN) Ford unvei...</td>\n",
       "      <td>elon musk tweets...</td>\n",
       "      <td>ford unveiled a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:18:26 UTC</td>\n",
       "      <td>Prince Andrew ac...</td>\n",
       "      <td>London (CNN) Pri...</td>\n",
       "      <td>[standard, andre...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>London (CNN) Pri...</td>\n",
       "      <td>prince andrew ac...</td>\n",
       "      <td>london  prince a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>08:05:33 UTC</td>\n",
       "      <td>What a shot! 27 ...</td>\n",
       "      <td></td>\n",
       "      <td>[shot, photos, a...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td></td>\n",
       "      <td>shot  amazing sp...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>15:37:54 UTC</td>\n",
       "      <td>The flat-Earth c...</td>\n",
       "      <td>(CNN) \"I don't w...</td>\n",
       "      <td>[does, earth, wo...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>But remarkably, ...</td>\n",
       "      <td>flatearth conspi...</td>\n",
       "      <td>i want flat ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>16:37:11 UTC</td>\n",
       "      <td>FedEx CEO challe...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>[challenges, fed...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>fedex ceo challe...</td>\n",
       "      <td>new york  busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>18:03:24 UTC</td>\n",
       "      <td>Andrew Yang: As ...</td>\n",
       "      <td>Andrew Yang is f...</td>\n",
       "      <td>[kids, media, an...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Andrew Yang is f...</td>\n",
       "      <td>andrew yang pres...</td>\n",
       "      <td>andrew yang foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>06:17:17</td>\n",
       "      <td>UPDATE 2-UK lawm...</td>\n",
       "      <td>(Adds Standard C...</td>\n",
       "      <td>[2uk, stanchart,...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>HSBC said it ful...</td>\n",
       "      <td>update uk lawmak...</td>\n",
       "      <td>adds standard ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>08:15:56</td>\n",
       "      <td>NVIDIA Corporati...</td>\n",
       "      <td>A week ago, NVID...</td>\n",
       "      <td>[past, eps, esti...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>NVIDIA beat earn...</td>\n",
       "      <td>nvidia corporati...</td>\n",
       "      <td>week ago nvidia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:50:42</td>\n",
       "      <td>Edited Transcrip...</td>\n",
       "      <td>Q3 2019 Hornbeck...</td>\n",
       "      <td>[guidance, offsh...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Now I'd like to ...</td>\n",
       "      <td>edited transcrip...</td>\n",
       "      <td>q  hornbeck offs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:22:16</td>\n",
       "      <td>Trump backs down...</td>\n",
       "      <td>The New York Tim...</td>\n",
       "      <td>[nyt, trump, zac...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>The New York Tim...</td>\n",
       "      <td>trump backs flav...</td>\n",
       "      <td>new york times r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>11:31:54</td>\n",
       "      <td>UPDATE 5-Shootin...</td>\n",
       "      <td>(Adds details on...</td>\n",
       "      <td>[lot, walmart, 5...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Video of the sce...</td>\n",
       "      <td>update shooting ...</td>\n",
       "      <td>adds details vic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:15:03</td>\n",
       "      <td>8 Stocks Donald ...</td>\n",
       "      <td>The president an...</td>\n",
       "      <td>[guru, continues...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>The president an...</td>\n",
       "      <td>stocks donald s...</td>\n",
       "      <td>president cochie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>09:49:03</td>\n",
       "      <td>Millennials 'are...</td>\n",
       "      <td>The declining he...</td>\n",
       "      <td>[health, decline...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>The declining he...</td>\n",
       "      <td>millennials are ...</td>\n",
       "      <td>declining health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:06:00</td>\n",
       "      <td>How Does Investi...</td>\n",
       "      <td>If you're intere...</td>\n",
       "      <td>[does, understan...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>If you're intere...</td>\n",
       "      <td>investing netgea...</td>\n",
       "      <td>interested netge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:05:13</td>\n",
       "      <td>Why Norfolk Sout...</td>\n",
       "      <td>Many investors a...</td>\n",
       "      <td>[high, roe, nyse...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>We'll use ROE to...</td>\n",
       "      <td>norfolk southern...</td>\n",
       "      <td>many investors s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>06:20:09</td>\n",
       "      <td>Fuel for Rockets...</td>\n",
       "      <td>(Bloomberg) -- I...</td>\n",
       "      <td>[green, producti...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>When it’s driven...</td>\n",
       "      <td>fuel rockets zep...</td>\n",
       "      <td>bloomberg  its n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:48:53</td>\n",
       "      <td>HP Doesn’t Need ...</td>\n",
       "      <td>(Bloomberg Opini...</td>\n",
       "      <td>[hp, xerox, gene...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>The problem for ...</td>\n",
       "      <td>hp doesnt need x...</td>\n",
       "      <td>bloomberg opinio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:38:18</td>\n",
       "      <td>FTC chief says h...</td>\n",
       "      <td>WASHINGTON (Reut...</td>\n",
       "      <td>[say, chief, pla...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>WASHINGTON (Reut...</td>\n",
       "      <td>ftc chief says m...</td>\n",
       "      <td>washington reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:08:29</td>\n",
       "      <td>FAA may overhaul...</td>\n",
       "      <td>The Federal Avia...</td>\n",
       "      <td>[keenan, faa, ex...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>The Federal Avia...</td>\n",
       "      <td>faa may overhaul...</td>\n",
       "      <td>federal aviation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>07:00:02</td>\n",
       "      <td>Market Morning: ...</td>\n",
       "      <td>China Edges Into...</td>\n",
       "      <td>[morning, heinz,...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>China Edges Into...</td>\n",
       "      <td>market morning c...</td>\n",
       "      <td>china edges hong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>09:50:51</td>\n",
       "      <td>Canadian Nationa...</td>\n",
       "      <td>Canadian Nationa...</td>\n",
       "      <td>[laying, cns, vo...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Canadian Nationa...</td>\n",
       "      <td>canadian nationa...</td>\n",
       "      <td>canadian nationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:56:43</td>\n",
       "      <td>Warren Calls Out...</td>\n",
       "      <td>(Bloomberg) -- E...</td>\n",
       "      <td>[street, blackst...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>(Bloomberg) -- E...</td>\n",
       "      <td>warren calls bla...</td>\n",
       "      <td>bloomberg  eliza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>00:11:56</td>\n",
       "      <td>Did You Manage T...</td>\n",
       "      <td>The main aim of ...</td>\n",
       "      <td>[nokia, growth, ...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>At this point so...</td>\n",
       "      <td>manage avoid nok...</td>\n",
       "      <td>main aim stock p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:00:33</td>\n",
       "      <td>Are Kinross Gold...</td>\n",
       "      <td>Today we'll look...</td>\n",
       "      <td>[high, roce, ret...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Today we'll look...</td>\n",
       "      <td>kinross gold cor...</td>\n",
       "      <td>today well look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>11:50:47</td>\n",
       "      <td>Kylie Jenner Jus...</td>\n",
       "      <td>Kylie Jenner jus...</td>\n",
       "      <td>[companys, cosme...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Kylie Jenner jus...</td>\n",
       "      <td>kylie jenner got...</td>\n",
       "      <td>kylie jenner got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>yahoofinance</td>\n",
       "      <td>https://finance....</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>11:32:12</td>\n",
       "      <td>10 Most Profitab...</td>\n",
       "      <td>Have you ever wo...</td>\n",
       "      <td>[profitable, wor...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Have you ever wo...</td>\n",
       "      <td>profitable fran...</td>\n",
       "      <td>ever wondered pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:51:00</td>\n",
       "      <td>Food prices push...</td>\n",
       "      <td>By Alexis Akwagy...</td>\n",
       "      <td>[high, push, 17m...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>By Alexis Akwagy...</td>\n",
       "      <td>food prices push...</td>\n",
       "      <td>alexis akwagyira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:48:12</td>\n",
       "      <td>Wall Street nudg...</td>\n",
       "      <td>© Reuters. A tra...</td>\n",
       "      <td>[n, corp, street...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>But also on inve...</td>\n",
       "      <td>wall street nudg...</td>\n",
       "      <td>reuters trader ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:46:21</td>\n",
       "      <td>Canadian ministe...</td>\n",
       "      <td>© Reuters. Canad...</td>\n",
       "      <td>[planned, strike...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>The threatened s...</td>\n",
       "      <td>canadian ministe...</td>\n",
       "      <td>reuters canadas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:28:22</td>\n",
       "      <td>California to st...</td>\n",
       "      <td>© Reuters. FILE ...</td>\n",
       "      <td>[trump, toyota, ...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Between 2016 and...</td>\n",
       "      <td>california stop ...</td>\n",
       "      <td>reuters file ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:27:27</td>\n",
       "      <td>Pompeo expected ...</td>\n",
       "      <td>© Reuters. U.S. ...</td>\n",
       "      <td>[pompeo, expecte...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>U.S. Secretary o...</td>\n",
       "      <td>pompeo expected ...</td>\n",
       "      <td>reuters us secr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:26:33</td>\n",
       "      <td>Brexit Bulletin:...</td>\n",
       "      <td>Days to Brexit d...</td>\n",
       "      <td>[does, johnson, ...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Days to Brexit d...</td>\n",
       "      <td>brexit bulletin ...</td>\n",
       "      <td>days brexit dead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:26:00</td>\n",
       "      <td>Dollar Stifled b...</td>\n",
       "      <td>© Reuters. Dolla...</td>\n",
       "      <td>[yearend, worlds...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Dollar Stifled b...</td>\n",
       "      <td>dollar stifled g...</td>\n",
       "      <td>reuters dollar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:21:42</td>\n",
       "      <td>FCC chairman Pai...</td>\n",
       "      <td>© Reuters. Ajit ...</td>\n",
       "      <td>[commission, pub...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Ajit Pai, Chairm...</td>\n",
       "      <td>fcc chairman pai...</td>\n",
       "      <td>reuters ajit pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:21:37</td>\n",
       "      <td>U.S. grants Huaw...</td>\n",
       "      <td>© Reuters. FILE ...</td>\n",
       "      <td>[security, comme...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>Reuters on Sunda...</td>\n",
       "      <td>us grants huawei...</td>\n",
       "      <td>reuters file ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.inve...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:11:00</td>\n",
       "      <td>Lawmakers urge U...</td>\n",
       "      <td>WASHINGTON (Reut...</td>\n",
       "      <td>[export, tighten...</td>\n",
       "      <td>https://i-invdn-...</td>\n",
       "      <td>The letter, seen...</td>\n",
       "      <td>lawmakers urge u...</td>\n",
       "      <td>washington reute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                 link published_date published_time  \\\n",
       "0             cnn  http://rss.cnn.c...     2019-11-18   18:02:00 UTC   \n",
       "1             cnn  http://rss.cnn.c...     2019-11-18   13:23:02 UTC   \n",
       "2             cnn  http://rss.cnn.c...     2019-11-18   16:58:27 UTC   \n",
       "3             cnn  http://rss.cnn.c...     2019-11-18   01:00:41 UTC   \n",
       "4             cnn  http://rss.cnn.c...     2019-11-18   17:03:00 UTC   \n",
       "5             cnn  http://rss.cnn.c...     2019-11-18   18:18:27 UTC   \n",
       "6             cnn  http://rss.cnn.c...     2019-11-18   19:09:03 UTC   \n",
       "7             cnn  http://rss.cnn.c...     2019-11-18   19:10:48 UTC   \n",
       "8             cnn  http://rss.cnn.c...     2019-11-18   18:40:04 UTC   \n",
       "9             cnn  http://rss.cnn.c...     2019-11-18   15:37:22 UTC   \n",
       "10            cnn  http://rss.cnn.c...     2019-11-18   17:17:51 UTC   \n",
       "11            cnn  http://rss.cnn.c...     2019-11-18   18:52:17 UTC   \n",
       "12            cnn  http://rss.cnn.c...     2019-11-18   10:49:00 UTC   \n",
       "13            cnn  http://rss.cnn.c...     2019-11-18   19:14:24 UTC   \n",
       "14            cnn  http://rss.cnn.c...     2019-11-18   16:47:41 UTC   \n",
       "15            cnn  http://rss.cnn.c...     2019-11-18   19:26:45 UTC   \n",
       "16            cnn  http://rss.cnn.c...     2019-11-18   15:36:46 UTC   \n",
       "17            cnn  http://rss.cnn.c...     2019-11-18   15:53:20 UTC   \n",
       "18            cnn  http://rss.cnn.c...     2019-11-18   14:08:57 UTC   \n",
       "19            cnn  http://rss.cnn.c...     2019-11-18   19:17:42 UTC   \n",
       "20            cnn  http://rss.cnn.c...     2019-11-18   19:06:52 UTC   \n",
       "21            cnn  http://rss.cnn.c...     2019-11-18   17:02:35 UTC   \n",
       "22            cnn  http://rss.cnn.c...     2019-11-18   13:44:43 UTC   \n",
       "23            cnn  http://rss.cnn.c...     2019-11-18   13:56:42 UTC   \n",
       "24            cnn  http://rss.cnn.c...     2019-11-18   16:32:01 UTC   \n",
       "25            cnn  http://rss.cnn.c...     2019-11-18   17:18:26 UTC   \n",
       "26            cnn  http://rss.cnn.c...     2019-11-18   08:05:33 UTC   \n",
       "27            cnn  http://rss.cnn.c...     2019-11-18   15:37:54 UTC   \n",
       "28            cnn  http://rss.cnn.c...     2019-11-18   16:37:11 UTC   \n",
       "29            cnn  http://rss.cnn.c...     2019-11-18   18:03:24 UTC   \n",
       "..            ...                  ...            ...            ...   \n",
       "414  yahoofinance  https://finance....     2019-11-18      06:17:17    \n",
       "415  yahoofinance  https://finance....     2019-11-18      08:15:56    \n",
       "416  yahoofinance  https://finance....     2019-11-18      13:50:42    \n",
       "417  yahoofinance  https://finance....     2019-11-18      14:22:16    \n",
       "418  yahoofinance  https://finance....     2019-11-18      11:31:54    \n",
       "419  yahoofinance  https://finance....     2019-11-18      14:15:03    \n",
       "420  yahoofinance  https://finance....     2019-11-18      09:49:03    \n",
       "421  yahoofinance  https://finance....     2019-11-18      14:06:00    \n",
       "422  yahoofinance  https://finance....     2019-11-18      14:05:13    \n",
       "423  yahoofinance  https://finance....     2019-11-18      06:20:09    \n",
       "424  yahoofinance  https://finance....     2019-11-18      12:48:53    \n",
       "425  yahoofinance  https://finance....     2019-11-18      13:38:18    \n",
       "426  yahoofinance  https://finance....     2019-11-18      12:08:29    \n",
       "427  yahoofinance  https://finance....     2019-11-18      07:00:02    \n",
       "428  yahoofinance  https://finance....     2019-11-18      09:50:51    \n",
       "429  yahoofinance  https://finance....     2019-11-18      13:56:43    \n",
       "430  yahoofinance  https://finance....     2019-11-18      00:11:56    \n",
       "431  yahoofinance  https://finance....     2019-11-18      12:00:33    \n",
       "432  yahoofinance  https://finance....     2019-11-18      11:50:47    \n",
       "433  yahoofinance  https://finance....     2019-11-18      11:32:12    \n",
       "434     investing  https://www.inve...     2019-11-18      19:51:00    \n",
       "435     investing  https://www.inve...     2019-11-18      19:48:12    \n",
       "436     investing  https://www.inve...     2019-11-18      19:46:21    \n",
       "437     investing  https://www.inve...     2019-11-18      19:28:22    \n",
       "438     investing  https://www.inve...     2019-11-18      19:27:27    \n",
       "439     investing  https://www.inve...     2019-11-18      19:26:33    \n",
       "440     investing  https://www.inve...     2019-11-18      19:26:00    \n",
       "441     investing  https://www.inve...     2019-11-18      19:21:42    \n",
       "442     investing  https://www.inve...     2019-11-18      19:21:37    \n",
       "443     investing  https://www.inve...     2019-11-18      19:11:00    \n",
       "\n",
       "                   title                 text             keywords  \\\n",
       "0    House investigat...  Washington (CNN)...  [mueller, trump,...   \n",
       "1    The latest on th...  Mark Makela/Gett...  [trump, set, twe...   \n",
       "2    Trump attacks an...  (CNN) President ...  [witness, trump,...   \n",
       "3    Tweets can be us...  Chat with us in ...  [world, court, m...   \n",
       "4    House Republican...  (CNN) House Repu...  [republican, ukr...   \n",
       "5    Supreme Court st...  Washington (CNN)...  [documents, trum...   \n",
       "6    Why you just can...  (CNN) President ...  [physical, donal...   \n",
       "7    Chick-fil-A will...  New York (CNN Bu...  [salvation, orga...   \n",
       "8    Gunman kills at ...  (CNN) At least t...  [lot, walmart, s...   \n",
       "9    Ryan Costello: M...  (CNN) Ryan Coste...  [organization, m...   \n",
       "10   Lindsey Graham f...  New York (CNN Bu...  [failure, media,...   \n",
       "11   10 people were s...  (CNN) A group of...  [reid, shooting,...   \n",
       "12   Hunter in China ...  Hong Kong (CNN) ...  [person, infecte...   \n",
       "13   WeWork braces fo...  New York (CNN Bu...  [according, brac...   \n",
       "14   Box truck crash ...  A close call was...  [struck, skidded...   \n",
       "15   How to help Saug...  (CNN) While noth...  [school, shootin...   \n",
       "16   Kylie Jenner sel...  New York (CNN Bu...  [cosmetics, mill...   \n",
       "17   Will.i.am: Qanta...  (CNN) — Qantas h...  [member, doesnt,...   \n",
       "18   Kathie Lee Giffo...  (CNN) Multiple w...  [lee, thrilled, ...   \n",
       "19   'Jeopardy! The G...  (CNN) The three ...  [receive, trebek...   \n",
       "20   Idris Elba is OK...  (CNN) In case yo...  [legend, alive, ...   \n",
       "21   'The Real Housew...  \"The Real Housew...  [coming, 2020, u...   \n",
       "22   Marie Kondo clea...  London (CNN Lond...  [london, product...   \n",
       "23   Ford takes aim a...  Los Angeles (CNN...  [model, ford, pe...   \n",
       "24   Elon Musk tweets...  (CNN) Ford unvei...  [elon, ford, mod...   \n",
       "25   Prince Andrew ac...  London (CNN) Pri...  [standard, andre...   \n",
       "26   What a shot! 27 ...                       [shot, photos, a...   \n",
       "27   The flat-Earth c...  (CNN) \"I don't w...  [does, earth, wo...   \n",
       "28   FedEx CEO challe...  New York (CNN Bu...  [challenges, fed...   \n",
       "29   Andrew Yang: As ...  Andrew Yang is f...  [kids, media, an...   \n",
       "..                   ...                  ...                  ...   \n",
       "414  UPDATE 2-UK lawm...  (Adds Standard C...  [2uk, stanchart,...   \n",
       "415  NVIDIA Corporati...  A week ago, NVID...  [past, eps, esti...   \n",
       "416  Edited Transcrip...  Q3 2019 Hornbeck...  [guidance, offsh...   \n",
       "417  Trump backs down...  The New York Tim...  [nyt, trump, zac...   \n",
       "418  UPDATE 5-Shootin...  (Adds details on...  [lot, walmart, 5...   \n",
       "419  8 Stocks Donald ...  The president an...  [guru, continues...   \n",
       "420  Millennials 'are...  The declining he...  [health, decline...   \n",
       "421  How Does Investi...  If you're intere...  [does, understan...   \n",
       "422  Why Norfolk Sout...  Many investors a...  [high, roe, nyse...   \n",
       "423  Fuel for Rockets...  (Bloomberg) -- I...  [green, producti...   \n",
       "424  HP Doesn’t Need ...  (Bloomberg Opini...  [hp, xerox, gene...   \n",
       "425  FTC chief says h...  WASHINGTON (Reut...  [say, chief, pla...   \n",
       "426  FAA may overhaul...  The Federal Avia...  [keenan, faa, ex...   \n",
       "427  Market Morning: ...  China Edges Into...  [morning, heinz,...   \n",
       "428  Canadian Nationa...  Canadian Nationa...  [laying, cns, vo...   \n",
       "429  Warren Calls Out...  (Bloomberg) -- E...  [street, blackst...   \n",
       "430  Did You Manage T...  The main aim of ...  [nokia, growth, ...   \n",
       "431  Are Kinross Gold...  Today we'll look...  [high, roce, ret...   \n",
       "432  Kylie Jenner Jus...  Kylie Jenner jus...  [companys, cosme...   \n",
       "433  10 Most Profitab...  Have you ever wo...  [profitable, wor...   \n",
       "434  Food prices push...  By Alexis Akwagy...  [high, push, 17m...   \n",
       "435  Wall Street nudg...  © Reuters. A tra...  [n, corp, street...   \n",
       "436  Canadian ministe...  © Reuters. Canad...  [planned, strike...   \n",
       "437  California to st...  © Reuters. FILE ...  [trump, toyota, ...   \n",
       "438  Pompeo expected ...  © Reuters. U.S. ...  [pompeo, expecte...   \n",
       "439  Brexit Bulletin:...  Days to Brexit d...  [does, johnson, ...   \n",
       "440  Dollar Stifled b...  © Reuters. Dolla...  [yearend, worlds...   \n",
       "441  FCC chairman Pai...  © Reuters. Ajit ...  [commission, pub...   \n",
       "442  U.S. grants Huaw...  © Reuters. FILE ...  [security, comme...   \n",
       "443  Lawmakers urge U...  WASHINGTON (Reut...  [export, tighten...   \n",
       "\n",
       "                   image              summary          clean_title  \\\n",
       "0    https://cdn.cnn....  Washington (CNN)...  house investigat...   \n",
       "1    https://cdn.cnn....  Mark Makela/Gett...  latest trump imp...   \n",
       "2    https://cdn.cnn....  But Sondland is ...  trump attacks an...   \n",
       "3    https://cdn.cnn....  Chat with us in ...  tweets used cour...   \n",
       "4    https://cdn.cnn....  (CNN) House Repu...  house republican...   \n",
       "5    https://cdn.cnn....  Washington (CNN)...  supreme court st...   \n",
       "6    https://cdn.cnn....  (CNN) President ...  cant trust white...   \n",
       "7    https://cdn.cnn....  New York (CNN Bu...  chickfila longer...   \n",
       "8    https://cdn.cnn....  (CNN) At least t...  gunman kills lea...   \n",
       "9    https://cdn.cnn....  (CNN) Ryan Coste...  ryan costello mi...   \n",
       "10   https://cdn.cnn....  He then repeated...  lindsey graham f...   \n",
       "11   https://cdn.cnn....  (CNN) A group of...   people shot  fa...   \n",
       "12   https://cdn.cnn....  Hong Kong (CNN) ...  hunter china cat...   \n",
       "13   https://cdn.cnn....  On Monday, WeWor...  wework braces ma...   \n",
       "14   https://cdn.cnn....  A close call was...  box truck crash ...   \n",
       "15   https://cdn.cnn....  (CNN) While noth...  help saugus high...   \n",
       "16   https://cdn.cnn....  New York (CNN Bu...  kylie jenner sel...   \n",
       "17   https://dynaimag...  He said on Twitt...  william qantas s...   \n",
       "18   https://cdn.cnn....  (CNN) Multiple w...  kathie lee giffo...   \n",
       "19   https://cdn.cnn....  (CNN) The three ...  jeopardy greates...   \n",
       "20   https://cdn.cnn....  (CNN) In case yo...  idris elba ok de...   \n",
       "21   https://cdn.cnn....  \"The Real Housew...  the real housewi...   \n",
       "22   https://cdn.cnn....  London (CNN Lond...  marie kondo clea...   \n",
       "23   https://cdn.cnn....  Los Angeles (CNN...  ford takes aim t...   \n",
       "24   https://cdn.cnn....  (CNN) Ford unvei...  elon musk tweets...   \n",
       "25   https://cdn.cnn....  London (CNN) Pri...  prince andrew ac...   \n",
       "26   https://cdn.cnn....                       shot  amazing sp...   \n",
       "27   https://cdn.cnn....  But remarkably, ...  flatearth conspi...   \n",
       "28   https://cdn.cnn....  New York (CNN Bu...  fedex ceo challe...   \n",
       "29   https://cdn.cnn....  Andrew Yang is f...  andrew yang pres...   \n",
       "..                   ...                  ...                  ...   \n",
       "414  https://s.yimg.c...  HSBC said it ful...  update uk lawmak...   \n",
       "415  https://s.yimg.c...  NVIDIA beat earn...  nvidia corporati...   \n",
       "416  https://s.yimg.c...  Now I'd like to ...  edited transcrip...   \n",
       "417  https://s.yimg.c...  The New York Tim...  trump backs flav...   \n",
       "418  https://s.yimg.c...  Video of the sce...  update shooting ...   \n",
       "419  https://s.yimg.c...  The president an...   stocks donald s...   \n",
       "420  https://s.yimg.c...  The declining he...  millennials are ...   \n",
       "421  https://s.yimg.c...  If you're intere...  investing netgea...   \n",
       "422  https://s.yimg.c...  We'll use ROE to...  norfolk southern...   \n",
       "423  https://s.yimg.c...  When it’s driven...  fuel rockets zep...   \n",
       "424  https://s.yimg.c...  The problem for ...  hp doesnt need x...   \n",
       "425  https://s.yimg.c...  WASHINGTON (Reut...  ftc chief says m...   \n",
       "426  https://s.yimg.c...  The Federal Avia...  faa may overhaul...   \n",
       "427  https://s.yimg.c...  China Edges Into...  market morning c...   \n",
       "428  https://s.yimg.c...  Canadian Nationa...  canadian nationa...   \n",
       "429  https://s.yimg.c...  (Bloomberg) -- E...  warren calls bla...   \n",
       "430  https://s.yimg.c...  At this point so...  manage avoid nok...   \n",
       "431  https://s.yimg.c...  Today we'll look...  kinross gold cor...   \n",
       "432  https://s.yimg.c...  Kylie Jenner jus...  kylie jenner got...   \n",
       "433  https://s.yimg.c...  Have you ever wo...   profitable fran...   \n",
       "434  https://i-invdn-...  By Alexis Akwagy...  food prices push...   \n",
       "435  https://i-invdn-...  But also on inve...  wall street nudg...   \n",
       "436  https://i-invdn-...  The threatened s...  canadian ministe...   \n",
       "437  https://i-invdn-...  Between 2016 and...  california stop ...   \n",
       "438  https://i-invdn-...  U.S. Secretary o...  pompeo expected ...   \n",
       "439  https://i-invdn-...  Days to Brexit d...  brexit bulletin ...   \n",
       "440  https://i-invdn-...  Dollar Stifled b...  dollar stifled g...   \n",
       "441  https://i-invdn-...  Ajit Pai, Chairm...  fcc chairman pai...   \n",
       "442  https://i-invdn-...  Reuters on Sunda...  us grants huawei...   \n",
       "443  https://i-invdn-...  The letter, seen...  lawmakers urge u...   \n",
       "\n",
       "              clean_text  \n",
       "0    washington  hous...  \n",
       "1    mark makelagetty...  \n",
       "2     president donal...  \n",
       "3    chat us facebook...  \n",
       "4     house republica...  \n",
       "5    washington  pres...  \n",
       "6     president donal...  \n",
       "7    new york  busine...  \n",
       "8     least two victi...  \n",
       "9     ryan costello p...  \n",
       "10   new york  busine...  \n",
       "11    group family fr...  \n",
       "12   hong kong  twent...  \n",
       "13   new york  busine...  \n",
       "14   close call caugh...  \n",
       "15    nothing bring b...  \n",
       "16   new york  busine...  \n",
       "17     qantas vowed h...  \n",
       "18    multiple weddin...  \n",
       "19    three top winne...  \n",
       "20    case wondering ...  \n",
       "21   the real housewi...  \n",
       "22   london  london m...  \n",
       "23   los angeles  bus...  \n",
       "24    ford unveiled a...  \n",
       "25   london  prince a...  \n",
       "26                        \n",
       "27    i want flat ear...  \n",
       "28   new york  busine...  \n",
       "29   andrew yang foun...  \n",
       "..                   ...  \n",
       "414  adds standard ch...  \n",
       "415  week ago nvidia ...  \n",
       "416  q  hornbeck offs...  \n",
       "417  new york times r...  \n",
       "418  adds details vic...  \n",
       "419  president cochie...  \n",
       "420  declining health...  \n",
       "421  interested netge...  \n",
       "422  many investors s...  \n",
       "423  bloomberg  its n...  \n",
       "424  bloomberg opinio...  \n",
       "425  washington reute...  \n",
       "426  federal aviation...  \n",
       "427  china edges hong...  \n",
       "428  canadian nationa...  \n",
       "429  bloomberg  eliza...  \n",
       "430  main aim stock p...  \n",
       "431  today well look ...  \n",
       "432  kylie jenner got...  \n",
       "433  ever wondered pr...  \n",
       "434  alexis akwagyira...  \n",
       "435   reuters trader ...  \n",
       "436   reuters canadas...  \n",
       "437   reuters file ph...  \n",
       "438   reuters us secr...  \n",
       "439  days brexit dead...  \n",
       "440   reuters dollar ...  \n",
       "441   reuters ajit pa...  \n",
       "442   reuters file ph...  \n",
       "443  washington reute...  \n",
       "\n",
       "[444 rows x 11 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of english stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Creating a dictionary for removing the names of the source websites\n",
    "sources_list = (list(source_dict.values()))\n",
    "for i in sources_list:\n",
    "    sources_set = set(i)\n",
    "sources_to_replace = dict.fromkeys(sources_set, \"\") # replace every source with \"\" nothing\n",
    "\n",
    "# Cleaning the dataframe\n",
    "#news_df_daily = news_df[news_df.published_date == today] # filter by todays date\n",
    "news_df_daily = news_df.reset_index(drop=True) # reseting the index\n",
    "\n",
    "news_df_daily[\"clean_title\"] = news_df_daily[\"title\"].str.lower()\n",
    "news_df_daily[\"clean_text\"] = news_df_daily[\"text\"].str.lower()\n",
    "\n",
    "# Filter out the stopwords\n",
    "news_df_daily['clean_title'] = news_df_daily['clean_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "news_df_daily['clean_text'] = news_df_daily['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "news_df_daily[\"clean_title\"] = ((news_df_daily[\"clean_title\"].str.replace('[^\\w\\s]','')) # remove punctuation from titles\n",
    "                                .str.replace('\\d+', '')) # remove numbers from titles\n",
    "\n",
    "news_df_daily[\"clean_text\"] = (((news_df_daily[\"clean_text\"].str.replace('[^\\w\\s]','')) #remove punctuation from texts\n",
    "                                .str.replace('\\d+', '')) # remove numbers from texts\n",
    "                               .replace(sources_to_replace, regex=True)) # remove source website names in text\n",
    "\n",
    "\n",
    "\n",
    "# you can't use none here\n",
    "pd.set_option('display.max_colwidth', 20)\n",
    "news_df_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing the dataset\n",
    "\n",
    "In this step, we apply several different analysis methods, in order to define which articles out of those we scraped are **most relevant** for portfolio trading customers and **cover trending financial topics**.\n",
    "\n",
    "### 2.1. Cosine similarity\n",
    "\n",
    "Cosine similarity is a metric for measuring the similarity between two sentences. It creates numbered vectors out of sentences and measures the **cosine of the angle between them**.\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d\" alt=\"Cosine similarity formula\" title=\"Cosine similarity formula\" />\n",
    "\n",
    "where\n",
    "* A ........... vector A\n",
    "* A • B ..... dot product between vector A and B\n",
    "* | A | ....... length of vector A\n",
    "\n",
    "\n",
    "We apply this measure for both the title and the texts.\n",
    "\n",
    "#### 2.1.A. Cosine similarity: titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0.272166</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125988</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125988</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    3         4         5         6    7    8    \\\n",
       "0  1.000000  0.204124  0.136083  0.0  0.154303  0.272166  0.154303  0.0  0.0   \n",
       "1  0.204124  1.000000  0.333333  0.0  0.000000  0.166667  0.000000  0.0  0.0   \n",
       "2  0.136083  0.333333  1.000000  0.0  0.000000  0.111111  0.000000  0.0  0.0   \n",
       "3  0.000000  0.000000  0.000000  1.0  0.000000  0.149071  0.000000  0.0  0.0   \n",
       "4  0.154303  0.000000  0.000000  0.0  1.000000  0.125988  0.142857  0.0  0.0   \n",
       "\n",
       "        9    ...  434  435  436  437  438  439  440  441       442  443  \n",
       "0  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  \n",
       "1  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  \n",
       "2  0.111111  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.125988  0.0  \n",
       "3  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  \n",
       "4  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  \n",
       "\n",
       "[5 rows x 444 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer #for creating count vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity #cosine similarity calculator\n",
    "\n",
    "# for analysis, we need a list of all the titles\n",
    "clean_titles_list = list(news_df_daily['clean_title'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix_title = count_vectorizer.fit_transform(clean_titles_list) # creates the count vector\n",
    "count_matrix_title = count_matrix_title.todense() # creates numpy matrix out from all count vectors\n",
    "count_matrix_title = pd.DataFrame(count_matrix_title, columns=count_vectorizer.get_feature_names()) # creates df from count vectors\n",
    "\n",
    "# apply consine smilarity on count vector dataframe\n",
    "df_cosim_title = pd.DataFrame(cosine_similarity(count_matrix_title, count_matrix_title))\n",
    "df_cosim_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.B. Cosine similarity: texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393918</td>\n",
       "      <td>0.221237</td>\n",
       "      <td>0.361961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176696</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.057833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155287</td>\n",
       "      <td>0.016945</td>\n",
       "      <td>0.047378</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.024376</td>\n",
       "      <td>0.046458</td>\n",
       "      <td>0.045340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.393918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348979</td>\n",
       "      <td>0.317417</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>0.262719</td>\n",
       "      <td>0.172090</td>\n",
       "      <td>0.153299</td>\n",
       "      <td>0.148967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237521</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>0.071282</td>\n",
       "      <td>0.019480</td>\n",
       "      <td>0.109172</td>\n",
       "      <td>0.062931</td>\n",
       "      <td>0.038391</td>\n",
       "      <td>0.071882</td>\n",
       "      <td>0.102331</td>\n",
       "      <td>0.097625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.221237</td>\n",
       "      <td>0.348979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101844</td>\n",
       "      <td>0.092324</td>\n",
       "      <td>0.049174</td>\n",
       "      <td>0.046668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110623</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.054615</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.032172</td>\n",
       "      <td>0.030693</td>\n",
       "      <td>0.019018</td>\n",
       "      <td>0.046832</td>\n",
       "      <td>0.037488</td>\n",
       "      <td>0.046459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.361961</td>\n",
       "      <td>0.317417</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195096</td>\n",
       "      <td>0.060739</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125047</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>0.047565</td>\n",
       "      <td>0.017193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027639</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.061517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055661</td>\n",
       "      <td>0.025747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.000000  0.393918  0.221237  0.361961  0.000000  0.000000  0.176696   \n",
       "1  0.393918  1.000000  0.348979  0.317417  0.016165  0.016165  0.262719   \n",
       "2  0.221237  0.348979  1.000000  0.298039  0.000000  0.000000  0.101844   \n",
       "3  0.361961  0.317417  0.298039  1.000000  0.000000  0.000000  0.195096   \n",
       "4  0.000000  0.016165  0.000000  0.000000  1.000000  1.000000  0.027639   \n",
       "\n",
       "         7         8         9   ...        42        43        44        45  \\\n",
       "0  0.065258  0.054845  0.057833  ...  0.155287  0.016945  0.047378  0.035183   \n",
       "1  0.172090  0.153299  0.148967  ...  0.237521  0.013676  0.071282  0.019480   \n",
       "2  0.092324  0.049174  0.046668  ...  0.110623  0.004883  0.054615  0.004056   \n",
       "3  0.060739  0.020797  0.000000  ...  0.125047  0.008778  0.000000  0.000000   \n",
       "4  0.015776  0.032410  0.061517  ...  0.018559  0.000000  0.000000  0.000000   \n",
       "\n",
       "         46        47        48        49        50        51  \n",
       "0  0.059305  0.021785  0.027497  0.024376  0.046458  0.045340  \n",
       "1  0.109172  0.062931  0.038391  0.071882  0.102331  0.097625  \n",
       "2  0.032172  0.030693  0.019018  0.046832  0.037488  0.046459  \n",
       "3  0.011906  0.008261  0.000000  0.013865  0.047565  0.017193  \n",
       "4  0.055661  0.025747  0.000000  0.000000  0.000000  0.053586  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for analysis, we need a list of all the texts\n",
    "clean_texts_list = list(news_df_daily['clean_text'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix_text = count_vectorizer.fit_transform(clean_texts_list) # creates the count vector\n",
    "count_matrix_text = count_matrix_text.todense() # creates numpy matrix out from all count vectors\n",
    "#count_matrix_text.shape\n",
    "\n",
    "count_matrix_text = pd.DataFrame(count_matrix_text, columns=count_vectorizer.get_feature_names()) # creates df from count vectors\n",
    "\n",
    "# apply consine smilarity on count vector dataframe\n",
    "df_cosim_texts = pd.DataFrame(cosine_similarity(count_matrix_text, count_matrix_text))\n",
    "df_cosim_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Soft cosine similarity measure\n",
    "\n",
    "Metric for measuring the similarity between two sentences, but gives **higher scores for words with similar meaning**. For Example, ‘President’ vs ‘Prime minister’, ‘Food’ vs ‘Dish’, ‘Hi’ vs ‘Hello’ are considered similar. \n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9743aceb346ccb501ceaef15a46570d1ba8a6a1b\" alt=\"Soft cosine formula\" title=\"Soft cosine formula\" />\n",
    "\n",
    "where\n",
    "* sij .... similarity (feature i, feature j)\n",
    "\n",
    "**Difference to cosine similarity**: the traditional cosine similarity considers the vector space model (VSM i.e. features, unique words) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which help generalize the concept of cosine (and soft cosine) as well as the idea of (soft) similarity. https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "This implies that we need some vector defining the similarity between words i.e. vectors of words that are similar. \n",
    "In our case we are going to use the pretrained `fasttext-wiki-news-subwords-300` vector dataset containing 1 million word embeddings trained on Wikipedia 2017. More info here: https://github.com/RaRe-Technologies/gensim-data/releases/tag/fasttext-wiki-news-subwords-300\n",
    "\n",
    "_**Side note:** other pre-trained models to be found here: https://github.com/RaRe-Technologies/gensim-data/releases_\n",
    "\n",
    "**Word embeddings**: position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be used with pre-trained models applying transfer learning.\n",
    "\n",
    "#### 2.2.A. Soft cosine measure: titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elandman\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "### ! ### this will download a file to your harddrive ### ! ###\n",
    "\n",
    "# first we need to download the FastText model - about 250MB\n",
    "\n",
    "glove_wiki = api.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nows', 0.5313489437103271),\n",
       " ('ivana', 0.5286872982978821),\n",
       " ('ivanka', 0.5049400925636292),\n",
       " ('knauss', 0.4901247024536133),\n",
       " ('melania', 0.46927106380462646),\n",
       " ('casino', 0.46679919958114624),\n",
       " ('developer', 0.4634384512901306),\n",
       " ('trumps', 0.4476448595523834),\n",
       " ('condo', 0.4284646511077881),\n",
       " ('hilton', 0.4261905550956726)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary, a map of word to unique id from the title list\n",
    "dictionary_titles = corpora.Dictionary([simple_preprocess(word) for word in clean_titles_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a similarity sparse matrix from the words in the dictionary\n",
    "# this process takes a bit due to calculation time\n",
    "similarity_matrix_titles = glove_wiki.similarity_matrix(dictionary_titles, \n",
    "                                                        tfidf=None, \n",
    "                                                        threshold=0.0, \n",
    "                                                        exponent=2.0, \n",
    "                                                        nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the titles into bag-of-words vectors through function\n",
    "# appends the bag-of-words from all sentences into the sent list\n",
    "def convert_bow(sentences):\n",
    "    global sent_bow\n",
    "    sent_bow = []\n",
    "    for i in sentences:\n",
    "        bow = dictionary_titles.doc2bow(simple_preprocess(i))\n",
    "        sent_bow.append(bow)\n",
    "        \n",
    "convert_bow(clean_titles_list) \n",
    "\n",
    "#create soft cosine measure matrix thourgh function \n",
    "\"\"\" creates a matrix with the results of soft cosine measure calculation.\n",
    "Takes into account the previously created similarity sparse matrix was created from the similar word meanings \n",
    "(we extracted from the FastText model) from the unique words that were in our unique dictionary.\"\"\"\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array) # creates a grid with dimensions (nr of articles x nr of articles)\n",
    "    soft_cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix_titles) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return soft_cossim_mat\n",
    "\n",
    "soft_cossim_mat_titles = create_soft_cossim_matrix(sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9    ...   434  \\\n",
       "0  1.00  0.34  0.29  0.10  0.24  0.33  0.23  0.07  0.09  0.03  ...  0.00   \n",
       "1  0.34  1.00  0.43  0.03  0.05  0.24  0.09  0.00  0.00  0.05  ...  0.06   \n",
       "2  0.29  0.43  1.00  0.05  0.08  0.26  0.14  0.00  0.11  0.23  ...  0.10   \n",
       "3  0.10  0.03  0.05  1.00  0.00  0.25  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "4  0.24  0.05  0.08  0.00  1.00  0.18  0.21  0.02  0.06  0.03  ...  0.02   \n",
       "\n",
       "    435   436   437   438   439   440   441   442   443  \n",
       "0  0.02  0.00  0.00  0.00  0.09  0.07  0.00  0.04  0.06  \n",
       "1  0.00  0.00  0.00  0.11  0.00  0.00  0.00  0.05  0.06  \n",
       "2  0.06  0.06  0.02  0.09  0.00  0.14  0.00  0.26  0.15  \n",
       "3  0.00  0.00  0.00  0.00  0.00  0.05  0.00  0.00  0.00  \n",
       "4  0.02  0.03  0.00  0.00  0.09  0.05  0.02  0.03  0.14  \n",
       "\n",
       "[5 rows x 444 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_cossim_mat_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.B. Soft cosine measure: texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**! Be aware !** \n",
    "\n",
    "When you run the cell below - even when having only around 50 articles - the creation of a unique word dictionary and especially the corresponding similarity matrix for article texts takes at least 2 to 5min. \n",
    "\n",
    "This waiting time cannot be skipped for text soft cosine measure similarity comparison, since it just takes a lot of ressources to compute. If you want to time how long it exacly takes, look below for paragraph _X. Other stuff that could be helpful in the future_ - there is a code for timing the run time of a code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary, a map of word to unique id from the text list\n",
    "dictionary_texts = corpora.Dictionary([simple_preprocess(word) for word in clean_texts_list])\n",
    "\n",
    "# generate a similarity sparse matrix from the words in the dictionary\n",
    "# this process takes a bit due to calculation time\n",
    "similarity_matrix_texts = fasttext_model300.similarity_matrix(dictionary_texts, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the texts into bag-of-words vectors through function\n",
    "# appends the bag-of-words from all sentences into the sent list\n",
    "def convert_bow(sentences):\n",
    "    global sent_bow\n",
    "    sent_bow = []\n",
    "    for i in sentences:\n",
    "        bow = dictionary_texts.doc2bow(simple_preprocess(i))\n",
    "        sent_bow.append(bow)\n",
    "        \n",
    "convert_bow(clean_texts_list) \n",
    "\n",
    "#create soft cosine measure matrix thourgh function \n",
    "\"\"\" creates a matrix with the results of soft cosine measure calculation.\n",
    "Takes into account the previously created similarity sparse matrix was created from the similar word meanings \n",
    "(we extracted from the FastText model) from the unique words that were in our unique dictionary.\"\"\"\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array) # creates a grid with dimensions (nr of articles x nr of articles)\n",
    "    soft_cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix_texts) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return soft_cossim_mat\n",
    "\n",
    "soft_cossim_mat_texts = create_soft_cossim_matrix(sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9   ...    57  \\\n",
       "0  1.00  0.41  0.22  0.08  0.41  0.18  0.40  0.24  0.22  0.08  ...  0.31   \n",
       "1  0.41  1.00  0.22  0.05  0.27  0.09  0.15  0.20  0.19  0.05  ...  0.11   \n",
       "2  0.22  0.22  1.00  0.03  0.27  0.12  0.24  0.42  0.16  0.03  ...  0.19   \n",
       "3  0.08  0.05  0.03  1.00  0.07  0.15  0.15  0.11  0.05  1.00  ...  0.06   \n",
       "4  0.41  0.27  0.27  0.07  1.00  0.20  0.44  0.47  0.28  0.07  ...  0.32   \n",
       "\n",
       "     58    59    60    61    62    63    64    65    66  \n",
       "0  0.22  0.18  0.27  0.25  0.12  0.20  0.17  0.29  0.28  \n",
       "1  0.09  0.03  0.04  0.07  0.02  0.08  0.03  0.05  0.04  \n",
       "2  0.16  0.10  0.12  0.13  0.06  0.10  0.07  0.15  0.14  \n",
       "3  0.06  0.06  0.07  0.10  0.03  0.09  0.04  0.09  0.13  \n",
       "4  0.25  0.18  0.21  0.21  0.13  0.19  0.15  0.27  0.25  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_cossim_mat_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results: extracting most similar articles\n",
    "\n",
    "After finding some results for the similarity in our scraped articles, we have to **filter the similar articles out of our initial** `news_df_daily` **dataframe**, in order to find out the title and article text.\n",
    "\n",
    "We want to extract only articles that have some predefined minimum value for similarity f.e. we only want **articles that have a similarity of at least 0.7** (this number could vary depending on our choice). Since the row indexes and the column numbers in the `soft_cossim_mat` matrix are equal to the indexes of the articles in our initial `news_df_daily` dataframe, we need to filter `news_df_daily` by exactly these indexes which contain the minimum similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function to find the row and column index in a dataframe for a specific value\n",
    "def get_indexes(dataframe, value):\n",
    "    pos_list = list()\n",
    "    for i in value:\n",
    "        result = dataframe.isin([value]) # crete bool dataframe with True at positions where the given value exists\n",
    "        series = result.any()\n",
    "        column_names = list(series[series == True].index) # create list of columns that contain the value\n",
    "        for col in column_names: # iterate over list of columns and fetch the rows indexes where value exists\n",
    "            rows = list(result[col][result[col] == True].index)\n",
    "            for row in rows:\n",
    "                if row != col: # since matrix diagonal is always == 1, we exclude these results here\n",
    "                    pos_list.append((row, col)) #creates a list of row, col position\n",
    "        return pos_list # Return a list of tuples indicating the positions of value in the dataframe\n",
    "\n",
    "# function for creating a list of the row indexes\n",
    "def find_indexes(dict_pos, index_list):\n",
    "    for key, value in dict_pos.items():\n",
    "    #print(key, ' : ', value) # this prints the similarity values and its corresponding row and col indexes in the df\n",
    "        for num in value:\n",
    "            for firstnum in num:\n",
    "                index_list.append(firstnum)\n",
    "                \n",
    "# choosing the range of similarity values for which the sentences should be filtered\n",
    "simval = np.arange(0.9, 1.01, 0.01) # choose similarity values between first number and 1.0, by steps of 0.01\n",
    "simval = np.around(simval, decimals=2)\n",
    "simval = (simval.astype(str))\n",
    " \n",
    "# use dict comprehension and 'get_indexes' function to get index positions of elements in df with predefined similarity values\n",
    "dict_pos_titles = {elem: get_indexes(soft_cossim_mat_titles, elem) for elem in simval}\n",
    "#dict_pos_texts = {elem: get_indexes(soft_cossim_mat_texts, elem) for elem in simval}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Most similar articles: by similarity of article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>image</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:14:24 UTC</td>\n",
       "      <td>WeWork braces fo...</td>\n",
       "      <td>New York (CNN Bu...</td>\n",
       "      <td>[according, brac...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>On Monday, WeWor...</td>\n",
       "      <td>wework braces ma...</td>\n",
       "      <td>new york  busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:18:26 UTC</td>\n",
       "      <td>Prince Andrew ac...</td>\n",
       "      <td>London (CNN) Pri...</td>\n",
       "      <td>[standard, andre...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>London (CNN) Pri...</td>\n",
       "      <td>prince andrew ac...</td>\n",
       "      <td>london  prince a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cnn</td>\n",
       "      <td>http://rss.cnn.c...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:28:42 UTC</td>\n",
       "      <td>Jennifer Arcuri ...</td>\n",
       "      <td>London (CNN) Jen...</td>\n",
       "      <td>[johnson, fed, l...</td>\n",
       "      <td>https://cdn.cnn....</td>\n",
       "      <td>Arcuri made the ...</td>\n",
       "      <td>jennifer arcuri ...</td>\n",
       "      <td>london  jennifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>bs_top</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:17:56</td>\n",
       "      <td>SC stays tributa...</td>\n",
       "      <td>The on Monday st...</td>\n",
       "      <td>[crore, held, or...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>On January 10, 2...</td>\n",
       "      <td>sc stays tributa...</td>\n",
       "      <td>monday stayed or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>bs_top</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>15:43:39</td>\n",
       "      <td>Gayatri Projects...</td>\n",
       "      <td>Shares of plunge...</td>\n",
       "      <td>[tanks, projects...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Shares of plunge...</td>\n",
       "      <td>gayatri projects...</td>\n",
       "      <td>shares plunged  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>bs_top</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>13:29:41</td>\n",
       "      <td>Pokarna tanks 20...</td>\n",
       "      <td>Shares of Limite...</td>\n",
       "      <td>[tanks, determin...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>PESL, wholly-own...</td>\n",
       "      <td>pokarna tanks  u...</td>\n",
       "      <td>shares limited l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>bs_top</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:40:38</td>\n",
       "      <td>PSB Q2 review: N...</td>\n",
       "      <td>Over 100 per cen...</td>\n",
       "      <td>[analysts, psb, ...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Mid-and small-si...</td>\n",
       "      <td>psb q review nii...</td>\n",
       "      <td>per cent yearon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:08:00</td>\n",
       "      <td>Parag Milk Foods...</td>\n",
       "      <td>Shares of climbe...</td>\n",
       "      <td>[market, crore, ...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Shares of climbe...</td>\n",
       "      <td>parag milk foods...</td>\n",
       "      <td>shares climbed  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>11:53:00</td>\n",
       "      <td>Morgan Stanley p...</td>\n",
       "      <td>After cutting th...</td>\n",
       "      <td>[2020, ems, pegs...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>They, however, s...</td>\n",
       "      <td>morgan stanley p...</td>\n",
       "      <td>cutting growth p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>10:55:00</td>\n",
       "      <td>Bharti Airtel hi...</td>\n",
       "      <td>Shares of Bharti...</td>\n",
       "      <td>[21month, high, ...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>On Friday, Novem...</td>\n",
       "      <td>bharti airtel hi...</td>\n",
       "      <td>shares bharti ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>10:44:00</td>\n",
       "      <td>Essar Steel case...</td>\n",
       "      <td>Public Sector Ba...</td>\n",
       "      <td>[sbi, big, banks...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Besides, the gov...</td>\n",
       "      <td>essar steel case...</td>\n",
       "      <td>public sector ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>10:38:00</td>\n",
       "      <td>Brokerages turn ...</td>\n",
       "      <td>The slump in key...</td>\n",
       "      <td>[remain, given, ...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>The S&amp;P BSE Sens...</td>\n",
       "      <td>brokerages turn ...</td>\n",
       "      <td>slump key econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>10:03:00</td>\n",
       "      <td>Glenmark Pharma ...</td>\n",
       "      <td>Extending their ...</td>\n",
       "      <td>[crore, glenmark...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Extending their ...</td>\n",
       "      <td>glenmark pharma ...</td>\n",
       "      <td>extending gains ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>09:55:00</td>\n",
       "      <td>BPCL gains 4% on...</td>\n",
       "      <td>Shares of Bharat...</td>\n",
       "      <td>[2020, govt, sal...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Finance Minister...</td>\n",
       "      <td>bpcl gains  repo...</td>\n",
       "      <td>shares bharat pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>bs_market</td>\n",
       "      <td>https://www.busi...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>07:41:00</td>\n",
       "      <td>MARKET WRAP: Sen...</td>\n",
       "      <td>Type address sep...</td>\n",
       "      <td>[market, commasy...</td>\n",
       "      <td>https://bsmedia....</td>\n",
       "      <td>Type address sep...</td>\n",
       "      <td>market wrap sens...</td>\n",
       "      <td>type address sep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>theguardian</td>\n",
       "      <td>https://www.theg...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:22:16 UTC</td>\n",
       "      <td>House investigat...</td>\n",
       "      <td>House tells fede...</td>\n",
       "      <td>[responses, muel...</td>\n",
       "      <td>https://i.guim.c...</td>\n",
       "      <td>House tells fede...</td>\n",
       "      <td>house investigat...</td>\n",
       "      <td>house tells fede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>nytimes_world</td>\n",
       "      <td>https://www.nyti...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>17:56:27 UTC</td>\n",
       "      <td>Trump’s Made-for...</td>\n",
       "      <td>Without a set de...</td>\n",
       "      <td>[entertained, wa...</td>\n",
       "      <td>https://static01...</td>\n",
       "      <td>Without a set de...</td>\n",
       "      <td>trumps madefortv...</td>\n",
       "      <td>without set dead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>nytimes_world</td>\n",
       "      <td>https://www.nyti...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:54:19 UTC</td>\n",
       "      <td>SoftBank and Lin...</td>\n",
       "      <td>TOKYO — Two week...</td>\n",
       "      <td>[japanese, seek,...</td>\n",
       "      <td>https://static01...</td>\n",
       "      <td>TOKYO — Two week...</td>\n",
       "      <td>softbank line co...</td>\n",
       "      <td>tokyo  two weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>nytimes_world</td>\n",
       "      <td>https://www.nyti...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>08:00:09 UTC</td>\n",
       "      <td>TikTok’s Chief I...</td>\n",
       "      <td>Like almost ever...</td>\n",
       "      <td>[media, chief, b...</td>\n",
       "      <td>https://static01...</td>\n",
       "      <td>Like almost ever...</td>\n",
       "      <td>tiktoks chief mi...</td>\n",
       "      <td>like almost ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>nytimes_business</td>\n",
       "      <td>https://www.nyti...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>19:24:57 UTC</td>\n",
       "      <td>Trump Meets With...</td>\n",
       "      <td>WASHINGTON — The...</td>\n",
       "      <td>[fed, trump, pow...</td>\n",
       "      <td>https://static01...</td>\n",
       "      <td>WASHINGTON — The...</td>\n",
       "      <td>trump meets fed ...</td>\n",
       "      <td>washington  chai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:34:38</td>\n",
       "      <td>US cancels civil...</td>\n",
       "      <td>Secretary of Sta...</td>\n",
       "      <td>[civil, secretar...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Secretary of Sta...</td>\n",
       "      <td>us cancels civil...</td>\n",
       "      <td>secretary state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>14:08:17</td>\n",
       "      <td>UN expert: 100,0...</td>\n",
       "      <td>GENEVA (AP) — An...</td>\n",
       "      <td>[detention, kids...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>GENEVA (AP) — An...</td>\n",
       "      <td>un expert  kids ...</td>\n",
       "      <td>geneva ap  indep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:02:36</td>\n",
       "      <td>Libyan officials...</td>\n",
       "      <td>A man injured in...</td>\n",
       "      <td>[airstrike, figh...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>A man injured in...</td>\n",
       "      <td>libyan officials...</td>\n",
       "      <td>man injured airs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>09:38:02</td>\n",
       "      <td>The West Needs t...</td>\n",
       "      <td>(Bloomberg Opini...</td>\n",
       "      <td>[russian, states...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>This doesn’t mea...</td>\n",
       "      <td>west needs measu...</td>\n",
       "      <td>bloomberg opinio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>05:06:35</td>\n",
       "      <td>Japan hosts firs...</td>\n",
       "      <td>By Tim Kelly\\n\\n...</td>\n",
       "      <td>[hosts, world, j...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>Worried by incre...</td>\n",
       "      <td>japan hosts firs...</td>\n",
       "      <td>tim kelly tokyo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>04:36:29</td>\n",
       "      <td>UPDATE 1-Japan h...</td>\n",
       "      <td>(Adds name of ar...</td>\n",
       "      <td>[hosts, world, j...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>\"Innovation is h...</td>\n",
       "      <td>update japan hos...</td>\n",
       "      <td>adds name arms s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>https://news.yah...</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>03:55:45</td>\n",
       "      <td>North Korea says...</td>\n",
       "      <td>In this undated ...</td>\n",
       "      <td>[wont, north, tr...</td>\n",
       "      <td>https://s.yimg.c...</td>\n",
       "      <td>In this undated ...</td>\n",
       "      <td>north korea says...</td>\n",
       "      <td>undated photo pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                 link published_date published_time  \\\n",
       "13                cnn  http://rss.cnn.c...     2019-11-18   19:14:24 UTC   \n",
       "25                cnn  http://rss.cnn.c...     2019-11-18   17:18:26 UTC   \n",
       "33                cnn  http://rss.cnn.c...     2019-11-18   17:28:42 UTC   \n",
       "92             bs_top  https://www.busi...     2019-11-18      19:17:56    \n",
       "102            bs_top  https://www.busi...     2019-11-18      15:43:39    \n",
       "104            bs_top  https://www.busi...     2019-11-18      13:29:41    \n",
       "105            bs_top  https://www.busi...     2019-11-18      12:40:38    \n",
       "128         bs_market  https://www.busi...     2019-11-18      12:08:00    \n",
       "129         bs_market  https://www.busi...     2019-11-18      11:53:00    \n",
       "130         bs_market  https://www.busi...     2019-11-18      10:55:00    \n",
       "131         bs_market  https://www.busi...     2019-11-18      10:44:00    \n",
       "132         bs_market  https://www.busi...     2019-11-18      10:38:00    \n",
       "133         bs_market  https://www.busi...     2019-11-18      10:03:00    \n",
       "134         bs_market  https://www.busi...     2019-11-18      09:55:00    \n",
       "137         bs_market  https://www.busi...     2019-11-18      07:41:00    \n",
       "146       theguardian  https://www.theg...     2019-11-18   19:22:16 UTC   \n",
       "322     nytimes_world  https://www.nyti...     2019-11-18   17:56:27 UTC   \n",
       "324     nytimes_world  https://www.nyti...     2019-11-18   12:54:19 UTC   \n",
       "326     nytimes_world  https://www.nyti...     2019-11-18   08:00:09 UTC   \n",
       "337  nytimes_business  https://www.nyti...     2019-11-18   19:24:57 UTC   \n",
       "344         yahoonews  https://news.yah...     2019-11-18      14:34:38    \n",
       "346         yahoonews  https://news.yah...     2019-11-18      14:08:17    \n",
       "354         yahoonews  https://news.yah...     2019-11-18      12:02:36    \n",
       "364         yahoonews  https://news.yah...     2019-11-18      09:38:02    \n",
       "385         yahoonews  https://news.yah...     2019-11-18      05:06:35    \n",
       "388         yahoonews  https://news.yah...     2019-11-18      04:36:29    \n",
       "393         yahoonews  https://news.yah...     2019-11-18      03:55:45    \n",
       "\n",
       "                   title                 text             keywords  \\\n",
       "13   WeWork braces fo...  New York (CNN Bu...  [according, brac...   \n",
       "25   Prince Andrew ac...  London (CNN) Pri...  [standard, andre...   \n",
       "33   Jennifer Arcuri ...  London (CNN) Jen...  [johnson, fed, l...   \n",
       "92   SC stays tributa...  The on Monday st...  [crore, held, or...   \n",
       "102  Gayatri Projects...  Shares of plunge...  [tanks, projects...   \n",
       "104  Pokarna tanks 20...  Shares of Limite...  [tanks, determin...   \n",
       "105  PSB Q2 review: N...  Over 100 per cen...  [analysts, psb, ...   \n",
       "128  Parag Milk Foods...  Shares of climbe...  [market, crore, ...   \n",
       "129  Morgan Stanley p...  After cutting th...  [2020, ems, pegs...   \n",
       "130  Bharti Airtel hi...  Shares of Bharti...  [21month, high, ...   \n",
       "131  Essar Steel case...  Public Sector Ba...  [sbi, big, banks...   \n",
       "132  Brokerages turn ...  The slump in key...  [remain, given, ...   \n",
       "133  Glenmark Pharma ...  Extending their ...  [crore, glenmark...   \n",
       "134  BPCL gains 4% on...  Shares of Bharat...  [2020, govt, sal...   \n",
       "137  MARKET WRAP: Sen...  Type address sep...  [market, commasy...   \n",
       "146  House investigat...  House tells fede...  [responses, muel...   \n",
       "322  Trump’s Made-for...  Without a set de...  [entertained, wa...   \n",
       "324  SoftBank and Lin...  TOKYO — Two week...  [japanese, seek,...   \n",
       "326  TikTok’s Chief I...  Like almost ever...  [media, chief, b...   \n",
       "337  Trump Meets With...  WASHINGTON — The...  [fed, trump, pow...   \n",
       "344  US cancels civil...  Secretary of Sta...  [civil, secretar...   \n",
       "346  UN expert: 100,0...  GENEVA (AP) — An...  [detention, kids...   \n",
       "354  Libyan officials...  A man injured in...  [airstrike, figh...   \n",
       "364  The West Needs t...  (Bloomberg Opini...  [russian, states...   \n",
       "385  Japan hosts firs...  By Tim Kelly\\n\\n...  [hosts, world, j...   \n",
       "388  UPDATE 1-Japan h...  (Adds name of ar...  [hosts, world, j...   \n",
       "393  North Korea says...  In this undated ...  [wont, north, tr...   \n",
       "\n",
       "                   image              summary          clean_title  \\\n",
       "13   https://cdn.cnn....  On Monday, WeWor...  wework braces ma...   \n",
       "25   https://cdn.cnn....  London (CNN) Pri...  prince andrew ac...   \n",
       "33   https://cdn.cnn....  Arcuri made the ...  jennifer arcuri ...   \n",
       "92   https://bsmedia....  On January 10, 2...  sc stays tributa...   \n",
       "102  https://bsmedia....  Shares of plunge...  gayatri projects...   \n",
       "104  https://bsmedia....  PESL, wholly-own...  pokarna tanks  u...   \n",
       "105  https://bsmedia....  Mid-and small-si...  psb q review nii...   \n",
       "128  https://bsmedia....  Shares of climbe...  parag milk foods...   \n",
       "129  https://bsmedia....  They, however, s...  morgan stanley p...   \n",
       "130  https://bsmedia....  On Friday, Novem...  bharti airtel hi...   \n",
       "131  https://bsmedia....  Besides, the gov...  essar steel case...   \n",
       "132  https://bsmedia....  The S&P BSE Sens...  brokerages turn ...   \n",
       "133  https://bsmedia....  Extending their ...  glenmark pharma ...   \n",
       "134  https://bsmedia....  Finance Minister...  bpcl gains  repo...   \n",
       "137  https://bsmedia....  Type address sep...  market wrap sens...   \n",
       "146  https://i.guim.c...  House tells fede...  house investigat...   \n",
       "322  https://static01...  Without a set de...  trumps madefortv...   \n",
       "324  https://static01...  TOKYO — Two week...  softbank line co...   \n",
       "326  https://static01...  Like almost ever...  tiktoks chief mi...   \n",
       "337  https://static01...  WASHINGTON — The...  trump meets fed ...   \n",
       "344  https://s.yimg.c...  Secretary of Sta...  us cancels civil...   \n",
       "346  https://s.yimg.c...  GENEVA (AP) — An...  un expert  kids ...   \n",
       "354  https://s.yimg.c...  A man injured in...  libyan officials...   \n",
       "364  https://s.yimg.c...  This doesn’t mea...  west needs measu...   \n",
       "385  https://s.yimg.c...  Worried by incre...  japan hosts firs...   \n",
       "388  https://s.yimg.c...  \"Innovation is h...  update japan hos...   \n",
       "393  https://s.yimg.c...  In this undated ...  north korea says...   \n",
       "\n",
       "              clean_text  \n",
       "13   new york  busine...  \n",
       "25   london  prince a...  \n",
       "33   london  jennifer...  \n",
       "92   monday stayed or...  \n",
       "102  shares plunged  ...  \n",
       "104  shares limited l...  \n",
       "105   per cent yearon...  \n",
       "128  shares climbed  ...  \n",
       "129  cutting growth p...  \n",
       "130  shares bharti ai...  \n",
       "131  public sector ba...  \n",
       "132  slump key econom...  \n",
       "133  extending gains ...  \n",
       "134  shares bharat pe...  \n",
       "137  type address sep...  \n",
       "146  house tells fede...  \n",
       "322  without set dead...  \n",
       "324  tokyo  two weeks...  \n",
       "326  like almost ever...  \n",
       "337  washington  chai...  \n",
       "344  secretary state ...  \n",
       "346  geneva ap  indep...  \n",
       "354  man injured airs...  \n",
       "364  bloomberg opinio...  \n",
       "385  tim kelly tokyo ...  \n",
       "388  adds name arms s...  \n",
       "393  undated photo pr...  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list_titles = []\n",
    "find_indexes(dict_pos_titles, index_list_titles)\n",
    "index_list_titles = list(set(index_list_titles))\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_list_titles, :]).drop_duplicates((\"title\"))).sort_index()\n",
    "select_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_articles1[\"keywords\"]= select_articles1[\"keywords\"].astype(str) \n",
    "\n",
    "listtest = [\"model\",\"sales\",\"technology\",\"stocks\",\"stockmarket\",\"finance\",\"model\",\"2020\"]\n",
    "\n",
    "#select_articles1[select_articles1['keywords'].str.contains(\"tesla\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Most similar articles: by similarity of article texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_texts = []\n",
    "find_indexes(dict_pos_texts, index_list_texts)\n",
    "index_list_texts = list(set(index_list_texts))\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_list_texts, :]).drop_duplicates()).sort_index()\n",
    "select_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Most similar articles: by similarity of article titles and articles\n",
    "\n",
    "This filters the sentences by the `simval` defined before and keeps only the titles and the texts that BOTH match the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_intersection = (set(index_list_titles).intersection(set(index_list_texts)))\n",
    "index_intersection = list(index_intersection)\n",
    "\n",
    "select_articles = ((news_df_daily.iloc[index_intersection, :]).drop_duplicates()).sort_index()\n",
    "select_articles1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generating newsletter in HTML\n",
    "\n",
    "After already having designed the HTML body for the newsletter, we need to prepare the extracted article titles and texts for automatically entering intp the HTML body.\n",
    "\n",
    "## 4.1 Importing extracted titles and content into Newsletter\n",
    "\n",
    "Just for testing, we will randomly chose which articles to include in our newsletter body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC stays tributal order that held Sebi had no powers to bar auditors \n",
      " The on Monday stayed an order of the Securities Appellate Tribunal (SAT) which had held that markets' watchdog does not have the power to bar\n",
      "\n",
      "A bench comprising Justices Arun Mishra and Indira Banerjee also issued notice on the appeal filed by the S... \n",
      " bs_top \n",
      " https://www.business-standard.com/article/pti-stories/sc-stays-sat-s-order-holding-that-sebi-lacks-power-to-bar-auditors-119111801286_1.html\n"
     ]
    }
   ],
   "source": [
    "# creating separate lists of the columns and info we want to include\n",
    "similar_sources_list = list(select_articles1['source'])\n",
    "similar_links_list = list(select_articles1['link'])\n",
    "similar_titles_list = list(select_articles1['title'])\n",
    "similar_texts_list = list(select_articles1['text'])\n",
    "\n",
    "# randomly select articles to include\n",
    "nr_of_art = (list(random_select.shape))[0] # finding max number of rows of the df of the most similar articles\n",
    "\n",
    "random_art_nr = np.random.choice(nr_of_art, 8, replace=False) # randomly chose 7 articles out of the max possible\n",
    "random_art_nr_list = list(random_art_nr)\n",
    "\n",
    "# function to extract the articles by their random number in the index, limits the characters of text by 'max_chars' and adds '...' to the end \n",
    "def rand_info(nr_of_art, max_chars):\n",
    "    global rand_text, rand_source, rand_link, rand_title\n",
    "    rand_text, rand_source, rand_link, rand_title = [], [], [], []\n",
    "    random_art_nr = np.random.choice(nr_of_art, 8, replace=False)  # chosen randomly  \n",
    "    for nr in random_art_nr:\n",
    "        (rand_text.append((similar_texts_list[nr])[:max_chars]))\n",
    "        (rand_source.append(similar_sources_list[nr]))\n",
    "        (rand_link.append(similar_links_list[nr]))\n",
    "        (rand_title.append(similar_titles_list[nr]))\n",
    "    rand_text = [item + '...' for item in rand_text]\n",
    "\n",
    "random_select = select_articles.reset_index(drop=True) # resetting the index of the df\n",
    "rand_info(random_art_nr_list, 250) # selecting the articles randomly and maximizing texts by 250 chars\n",
    "\n",
    "# now every time the code is excuted, a new randomly chosen article appears\n",
    "print(rand_title[0],\"\\n\" , rand_text[0], \"\\n\" , rand_source[0], \"\\n\", rand_link[0])\n",
    "\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SC stays tributal order that held Sebi had no powers to bar auditors',\n",
       " 'US cancels civil nuclear cooperation waiver for Iran',\n",
       " 'BPCL gains 4% on report that govt wants to wrap up stake sale by March 2020',\n",
       " \"Bharti Airtel hits 21-month high, surges 20% from Friday's low\",\n",
       " 'UN expert: 100,000 kids in migration-related detention in US',\n",
       " 'Morgan Stanley pegs 2020 global growth at 3.2%; EMs to outperform',\n",
       " 'Brokerages turn cautious on markets amid slowing growth, rich valuation',\n",
       " 'Highway Patrol: Three People Killed in Shooting at Oklahoma Walmart']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting issues\n",
    "# ! are more unsupported characters ! to be edited and added over time\n",
    "\n",
    "# function to replace the wrongly formatted characters (obersed by looking at the html output)\n",
    "def replace_char(list_of_str):\n",
    "    for i in range(len(list_of_str)):\n",
    "        list_of_str[i] = list_of_str[i].replace(\"’\",\"`\")\n",
    "        list_of_str[i] = list_of_str[i].replace(\":\",\":\")\n",
    "        list_of_str[i] = list_of_str[i].replace(\"–\",\"-\")\n",
    "        #print(data)\n",
    "\n",
    "replace_char(rand_text)\n",
    "replace_char(rand_title)\n",
    "\n",
    "rand_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SC stays tributal order that held Sebi had no powers to bar auditors',\n",
       " 'US cancels civil nuclear cooperation waiver for Iran',\n",
       " 'BPCL gains 4% on report that govt wants to wrap up stake sale by March 2020',\n",
       " \"Bharti Airtel hits 21-month high, surges 20% from Friday's low\",\n",
       " 'UN expert: 100,000 kids in migration-related detention in US',\n",
       " 'Morgan Stanley pegs 2020 global growth at 3.2%; EMs to outperform',\n",
       " 'Brokerages turn cautious on markets amid slowing growth, rich valuation',\n",
       " 'Highway Patrol: Three People Killed in Shooting at Oklahoma Walmart']"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code is way easier to edit in Notepad ++\n",
    "\n",
    "print ()\n",
    "f = open('HTML_with VARS_V1.html','w')\n",
    " \n",
    "message = \"\"\"\n",
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
    "<html xmlns=\"http://www.w3.org/1999/xhtml\"><head>\n",
    "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
    "<title>Demystifying Email Design</title>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<link href=\"NewsletterTemplate_files/css.css\" rel=\"stylesheet\">    \n",
    " \n",
    "</head>\n",
    "<body style=\"margin: 0; padding: 0;\">\n",
    "    <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\"> \n",
    "        <tbody><tr>\n",
    "            <td style=\"padding: 10px 0 10px 0;\">\n",
    "                <table style=\"border: 1px solid #cccccc; border-collapse: collapse;\" width=\"1000\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\" align=\"center\">\n",
    "                    <tbody><tr>\n",
    "                        <td style=\"padding: 20px\" height=\"204\" bgcolor=\"#fbf315\" align=\"top\">\n",
    "                            <img alt=\"Creating Email Magic\" style=\"display: block;\" src=\"NewsletterTemplate_files/Logo-Raiffeisen-Bank-2017.png\" width=\"304\" height=\"304\">\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 20px 30px 40px 30px;\" bgcolor=\"#ffffff\">\n",
    "                            <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                <tbody><tr>\n",
    "                                    <td style=\"color: #153643; \n",
    "    font-family: 'Archivo Black', sans-serif; font-size: 40px;\">\n",
    "                                        <b>Daily Finance Update\n",
    "</b>\n",
    "                                    </td>\n",
    "                                \n",
    "                                        \n",
    "                                    </tr><tr>\n",
    "                                    <td style=\"color: #153643; \n",
    "    font-family: 'Archivo Black', sans-serif; font-size: 20px; padding: 10px 0px 10px 0px;\">\n",
    "                                        <b>Stocks\n",
    "</b>\n",
    "                                    </td>\n",
    "                                \n",
    "                                        \n",
    "                                    </tr>\n",
    "                                \n",
    "                                <tr>\n",
    "                                    <td>\n",
    "                                        <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                            <tbody><tr>\n",
    "                                                <td style=\"box-shadow: 1px 2px 4px rgba(0, 0, 0, .5);\" width=\"160\" valign=\"top\">\n",
    "                                                    <div style=\"padding: 20px 10px 5px 10px; font-family: 'Archivo Black', sans-serif; font-size: 22px\">\n",
    "  <b>TECH</b>\n",
    "</div><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Tech | {rand_source[0]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[0]}\">{rand_title[0]}</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[0]}\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Tech | {rand_source[1]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[1]}\">{rand_title[1]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[1]}\n",
    "                                                   \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Tech | {rand_source[2]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[2]}\">{rand_title[2]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[2]}\n",
    "</div></tdbody><table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                                                        </table>\n",
    "                                                </td><td style=\"font-size: 0; line-height: 0;\" width=\"20\">\n",
    "                                                    &nbsp;\n",
    "                                                </td><td style=\"box-shadow: 1px 2px 4px rgba(0, 0, 0, .5);\" width=\"160\" valign=\"top\">\n",
    "                                                    <div style=\"padding: 20px 10px 5px 10px; font-family: 'Archivo Black', sans-serif; font-size: 22px\">\n",
    "  <b>DEALS AND IPOs\n",
    "</b>\n",
    "</div><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Deals | {rand_source[3]} \n",
    "</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[3]}\">{rand_title[3]}\n",
    "Day record of more than $30 billion in sales and climbing</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">\n",
    "                                                            \n",
    "{rand_text[3]}\n",
    "\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Markets | {rand_source[4]}\n",
    "</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[4]}\">{rand_title[4]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">\n",
    "                                                            \n",
    "{rand_text[4]}\n",
    "\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                                        \n",
    "</table>\n",
    "                                                </td>\n",
    "                                                <td style=\"font-size: 0; line-height: 0;\" width=\"20\">\n",
    "                                                    &nbsp;\n",
    "                                                </td>\n",
    "                                                <td style=\"box-shadow: 1px 2px 4px rgba(0, 0, 0, .5);\" width=\"160\" valign=\"top\">\n",
    "                                                    <div style=\"padding: 20px 10px 5px 10px; font-family: 'Archivo Black', sans-serif; font-size: 22px\">\n",
    "  <b>BANKS\n",
    "</b>\n",
    "</div><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Trading | {rand_source[5]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[5]}\">{rand_title[5]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">\n",
    "                                                            \n",
    "{rand_text[5]}\n",
    "\n",
    "\n",
    "                                                            \n",
    "                                                        \n",
    "                                                    </div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">Earnings | {rand_source[6]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[6]}\">{rand_title[6]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[6]}\n",
    "</div></tdbody><tdbody>\n",
    "                                                        <div class=\"row margin-top\" style=\"padding: 10px 10px 5px 10px\">\n",
    "  <div style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> \n",
    "       <span class=\"item-Label\">JPMorgan | {rand_source[7]}</span>\n",
    "   </div>\n",
    "</div><div style=\"padding: 5px 10px 0 10px\" font-family:=\"\" font-size:=\"\"><b style=\"color: #153643; font-family: Roboto, sans-serif; font-size: 18px;\"> <a href=\"{rand_link[7]}\">{rand_title[7]}\n",
    "</a></b></div><div class=\"row margin-top\" style=\"padding: 5px 10px 15px 10px; font-family:'Raleway', sans-serif; font-size: 14px\">{rand_text[7]}\n",
    "</div></tdbody><table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                                                        </table>\n",
    "                                                </td>\n",
    "                                            </tr>\n",
    "                                        </tbody></table>\n",
    "                                    </td>\n",
    "                                </tr>\n",
    "                            </tbody></table>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 30px 30px 30px 30px;\" bgcolor=\"#666666\">\n",
    "                            <table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                <tbody><tr>\n",
    "                                    <td style=\"color: #ffffff; font-family: Arial, sans-serif; font-size: 14px;\" width=\"75%\">\n",
    "                                        ® Someone, somewhere 2019<br>\n",
    "                                        <a href=\"#\" style=\"color: #ffffff;\"><font color=\"#ffffff\">Unsubscribe</font></a> to this newsletter instantly\n",
    "                                    </td>\n",
    "                                    <td width=\"25%\" align=\"right\">\n",
    "                                        <table cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "                                            <tbody><tr>\n",
    "                                                <td style=\"font-family: Arial, sans-serif; font-size: 12px; font-weight: bold;\">\n",
    "                                                    <a href=\"https://twitter.com/raiffeisen_at\" style=\"color: #666666;\">\n",
    "                                                        <img src=\"NewsletterTemplate_files/logo.png\" alt=\"Twitter\" style=\"display: block;\" width=\"38\" height=\"38\" border=\"0\">\n",
    "                                                    </a>\n",
    "                                                </td>\n",
    "                                                <td style=\"font-size: 0; line-height: 0;\" width=\"20\">&nbsp;</td>\n",
    "                                                <td style=\"font-family: Arial, sans-serif; font-size: 12px; font-weight: bold;\">\n",
    "                                                    <a href=\"http://www.facebook.com/raiffeisen/\" style=\"color: #666666;\">\n",
    "                                                        <img alt=\"Facebook\" style=\"display: block;\" src=\"NewsletterTemplate_files/facebook-2.svg\" width=\"38\" height=\"38\" border=\"0\">\n",
    "                                                    </a>\n",
    "                                                </td>\n",
    "                                            </tr>\n",
    "                                        </tbody></table>\n",
    "                                    </td>\n",
    "                                </tr>\n",
    "                            </tbody></table>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </tbody></table>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody></table>\n",
    "\n",
    "\n",
    "\n",
    "</body></html>\n",
    "\n",
    "\"\"\".format(**locals()) #########\n",
    " \n",
    "f.write(message)\n",
    "f.close()\n",
    "\n",
    "#Change path to reflect file location\n",
    "filename = 'file:///'+os.getcwd()+'/' + 'HTML_with VARS_V1.html'\n",
    "webbrowser.open_new_tab(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Other stuff that could be helpful in the future\n",
    "\n",
    "## Time how long a code takes to execute\n",
    "\n",
    "Could be used for speed comparison of two similarity methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2019-11-18\n",
      "Today's date: 2019-11-18\n",
      "Today's date: 2019-11-18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWantReadError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1821\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1822\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1621\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_READ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWantReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWantReadError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-3fb9d28cb3e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \"\"\"\n\u001b[1;32m--> 117\u001b[1;33m \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_to_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[0;32m    230\u001b[0m            number=default_number, globals=None):\n\u001b[0;32m    231\u001b[0m     \u001b[1;34m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m def repeat(stmt=\"pass\", setup=\"pass\", timer=default_timer,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\newspaper\\article.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, input_html, title, recursion_counter)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput_html\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_html_2XX_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAILED_RESPONSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\newspaper\\network.py\u001b[0m in \u001b[0;36mget_html_2XX_only\u001b[1;34m(url, config, response)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     response = requests.get(\n\u001b[1;32m---> 63\u001b[1;33m         url=url, **get_request_kwargs(timeout, useragent, proxies, headers))\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_html_from_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    667\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m                 decoded = self._decode(chunk, decode_content=decode_content,\n\u001b[0;32m    671\u001b[0m                                        flush_decoder=False)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# amt > self.chunk_left\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mreturned_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Toss the CRLF at the end of the chunk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreturned_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The read operation timed out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[1;34m(sock, timeout)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreadable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwait_for_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mselect_wait_for_socket\u001b[1;34m(sock, read, write, timeout)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# thing.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mrready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mxready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36m_retry_on_intr\u001b[1;34m(fn, timeout)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Modern Python, that retries syscalls by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Old and broken Pythons.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "code_to_test = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "elapsed_time = timeit.timeit(code_to_test, number=100)/100\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google word meaning vector, pre-trained\n",
    "\n",
    "Maybe useful, some time?\n",
    "\n",
    "Other pre-trained models to be found here: https://github.com/RaRe-Technologies/gensim-data/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\") #1.6GB to download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting each word in title/text in pandas df to a separate column\n",
    "\n",
    "Maybe useful, some time?\n",
    "\n",
    "Code was hard to find via google haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = news_df_daily.str.split(expand=True)\n",
    "title_splitted = pd.DataFrame(split)\n",
    "title_splitted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
